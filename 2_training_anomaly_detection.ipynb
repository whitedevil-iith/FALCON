{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# === JVGAN COMPONENTS ===\n",
    "class JVGANEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(JVGANEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, 128, batch_first=True)\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        h = h_n.squeeze(0)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        z = mu + std * torch.randn_like(std)\n",
    "        return z, mu, logvar\n",
    "\n",
    "class JVGANDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, input_dim, sequence_length=65):\n",
    "        super(JVGANDecoder, self).__init__()\n",
    "        self.latent_to_hidden = nn.Linear(latent_dim, 128)\n",
    "        self.lstm = nn.LSTM(128, 128, batch_first=True)\n",
    "        self.output_layer = nn.Linear(128, input_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def forward(self, z):\n",
    "        h0 = self.latent_to_hidden(z).unsqueeze(1).repeat(1, self.sequence_length, 1)\n",
    "        lstm_out, _ = self.lstm(h0)\n",
    "        return self.output_layer(lstm_out)\n",
    "\n",
    "class JVGANDiscriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(JVGANDiscriminator, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, 128, batch_first=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        return self.classifier(h_n.squeeze(0))\n",
    "\n",
    "class JVGAN(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=32, sequence_length=65):\n",
    "        super(JVGAN, self).__init__()\n",
    "        self.encoder = JVGANEncoder(input_dim, latent_dim)\n",
    "        self.decoder = JVGANDecoder(latent_dim, input_dim, sequence_length)\n",
    "        self.discriminator = JVGANDiscriminator(input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        d_real = self.discriminator(x)\n",
    "        d_fake = self.discriminator(x_recon.detach())\n",
    "        return x_recon, d_real, d_fake, mu, logvar\n",
    "\n",
    "\n",
    "def detect_anomaly(x, model, threshold):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_recon, _, _, _, _ = model(x)\n",
    "        last_real = x[:, -1, :]\n",
    "        last_recon = x_recon[:, -1, :]\n",
    "        error = F.mse_loss(last_recon, last_real, reduction='none').mean(dim=1)\n",
    "        return error > threshold, error\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_jvgan(dataset, input_dim, num_epochs=30, batch_size=64, k_folds=5, latent_dim=32,\n",
    "                threshold=0.05, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", save_dir=\"jvgan_models\"):\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "        print(f\"\\n=== Fold {fold + 1}/{k_folds} ===\")\n",
    "        \n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = JVGAN(input_dim=input_dim, latent_dim=latent_dim).to(device)\n",
    "\n",
    "        # Define separate optimizers\n",
    "        optimizer_encoder = torch.optim.Adam(model.encoder.parameters(), lr=1e-3)\n",
    "        optimizer_decoder = torch.optim.Adam(model.decoder.parameters(), lr=1e-3)\n",
    "        optimizer_discriminator = torch.optim.Adam(model.discriminator.parameters(), lr=1e-4)\n",
    "\n",
    "        # Learning rate schedulers\n",
    "        scheduler_encoder = torch.optim.lr_scheduler.StepLR(optimizer_encoder, step_size=10, gamma=0.5)\n",
    "        scheduler_decoder = torch.optim.lr_scheduler.StepLR(optimizer_decoder, step_size=10, gamma=0.5)\n",
    "        scheduler_discriminator = torch.optim.lr_scheduler.StepLR(optimizer_discriminator, step_size=10, gamma=0.5)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_x, in train_loader:\n",
    "                batch_x = batch_x.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                x_recon, d_real, d_fake, mu, logvar = model(batch_x)\n",
    "                last_real = batch_x[:, -1, :]\n",
    "                last_recon = x_recon[:, -1, :]\n",
    "\n",
    "                # Losses\n",
    "                recon_loss = F.mse_loss(last_recon, last_real)\n",
    "                kld = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "                real_labels = torch.ones_like(d_real)\n",
    "                fake_labels = torch.zeros_like(d_fake)\n",
    "                d_loss = F.binary_cross_entropy(d_real, real_labels) + F.binary_cross_entropy(d_fake, fake_labels)\n",
    "\n",
    "                # Total loss\n",
    "                total = recon_loss + kld + d_loss\n",
    "\n",
    "                # Backpropagation with multiple optimizers\n",
    "                optimizer_encoder.zero_grad()\n",
    "                optimizer_decoder.zero_grad()\n",
    "                optimizer_discriminator.zero_grad()\n",
    "                total.backward()\n",
    "                optimizer_encoder.step()\n",
    "                optimizer_decoder.step()\n",
    "                optimizer_discriminator.step()\n",
    "\n",
    "                total_loss += total.item()\n",
    "\n",
    "            # Scheduler steps\n",
    "            scheduler_encoder.step()\n",
    "            scheduler_decoder.step()\n",
    "            scheduler_discriminator.step()\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs} - Loss: {total_loss:.4f}\")\n",
    "\n",
    "        # Save model\n",
    "        model_path = os.path.join(save_dir, f\"jvgan_fold{fold + 1}.pt\")\n",
    "        torch.save(model, model_path)\n",
    "        print(f\"Saved model for fold {fold + 1} at {model_path}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_data = torch.stack([x[0] for x in val_subset]).to(device)\n",
    "        print(\"size of val_data:\", val_data.shape)\n",
    "        print(\"size of val_subset:\", len(val_subset))\n",
    "        is_anomaly, recon_errors = detect_anomaly(val_data, model, threshold)\n",
    "        fold_results.append((is_anomaly.cpu(), recon_errors.cpu()))\n",
    "\n",
    "    return fold_results\n",
    "\n",
    "\n",
    "\n",
    "def create_sequences(df, look_back, look_forward, CU, DU):\n",
    "    \"\"\"\n",
    "    Create LSTM-ready sequences from a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "        look_back (int): Number of timesteps to look back.\n",
    "        look_forward (int): Number of timesteps to predict forward.\n",
    "\n",
    "    Returns:\n",
    "        X (np.ndarray): Shape (samples, look_back, features)\n",
    "        y_df (pd.DataFrame): Shape (samples, features), from last step of forecast horizon\n",
    "        column_names (list): List of column names used\n",
    "    \"\"\"\n",
    "    # print(df.columns)\n",
    "    data = df.drop(columns=[f\"srscu{CU}_stepStress\", f\"srscu{CU}_stressType\", f\"srsdu{DU}_stepStress\", f\"srsdu{DU}_stressType\"]).values\n",
    "    targets = df[[f\"srscu{CU}_stepStress\", f\"srscu{CU}_stressType\", f\"srsdu{DU}_stepStress\", f\"srsdu{DU}_stressType\"]].values\n",
    "\n",
    "    pca_features_input, pca_features_output, input_targets, output_targets  = [], [], [], []\n",
    "\n",
    "    for i in range(len(data) - look_back - look_forward):\n",
    "        pca_features_input.append(data[i:(i + look_back + look_forward)])\n",
    "        pca_features_output.append(data[i + look_back + look_forward])  # last step of forecast\n",
    "\n",
    "        input_targets.append(targets[i:(i + look_back + look_forward)])\n",
    "        output_targets.append(targets[i + look_back + look_forward])  # last step of forecast\n",
    "\n",
    "    return pca_features_input, pca_features_output, input_targets, output_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser(description=\"Run script with optional CU and DU values.\")\n",
    "# parser.add_argument(\"--cu\", type=int, default=0, help=\"CU value (default: 0)\")\n",
    "# parser.add_argument(\"--du\", type=int, default=0, help=\"DU value (default: 0)\")\n",
    "# parser.add_argument(\"--lb\", type=int, default=60, help=\"lookback value (default: 60)\")\n",
    "# parser.add_argument(\"--lf\", type=int, default=1, help=\"lookforward value (default: 5)\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# CU, DU = args.cu, args.du\n",
    "# LOOK_BACK, LOOK_FORWARD = args.lb, args.lf\n",
    "\n",
    "CU = 0\n",
    "DU = 0\n",
    "\n",
    "LOOK_BACK, LOOK_FORWARD = 65, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = pd.read_csv(f'dataset_srscu{CU}_srsdu{DU}.csv')\n",
    "# dataset = dataset[:int(0.01*len(dataset))]\n",
    "\n",
    "dataset.index = dataset['Timestamp']\n",
    "dataset = dataset.drop(columns=['Timestamp'])\n",
    "\n",
    "# dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dataset.drop(columns=[f\"srscu{CU}_stepStress\", f\"srscu{CU}_stressType\", f\"srsdu{DU}_stepStress\", f\"srsdu{DU}_stressType\"])\n",
    "targets = dataset[[f\"srscu{CU}_stepStress\", f\"srscu{CU}_stressType\", f\"srsdu{DU}_stepStress\", f\"srsdu{DU}_stressType\"]]\n",
    "\n",
    "# **Data Preprocessing**\n",
    "# Handle missing values\n",
    "features = features.apply(lambda x: x.fillna(0) if x.isna().all() else x)\n",
    "\n",
    "\n",
    "threshold = 0.6 * len(features)\n",
    "features = features.loc[:, ~features.columns.duplicated()]  # Remove duplicates\n",
    "\n",
    "for col in features.columns:\n",
    "    nan_count = features[col].isna().sum()\n",
    "    if int(nan_count) > threshold:  # Explicit scalar conversion\n",
    "        mode_value = features[col].mode().iloc[0] if not features[col].mode().empty else 0\n",
    "        features[col].fillna(mode_value, inplace=True)\n",
    "\n",
    "numeric_cols = features.select_dtypes(include=[np.number]).columns\n",
    "features[numeric_cols] = features[numeric_cols].fillna(features[numeric_cols].mean())\n",
    "\n",
    "# Scale features\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "scaler = joblib.load(\"minmax_scaler.pkl\")\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Drop the unwanted columns from the original dataset\n",
    "drop_cols = [f\"srscu{CU}_stepStress\", f\"srscu{CU}_stressType\", f\"srsdu{DU}_stepStress\", f\"srsdu{DU}_stressType\"]\n",
    "columns_to_keep = [col for col in dataset.columns if col not in drop_cols]\n",
    "\n",
    "# Create the DataFrame with scaled features\n",
    "features_scaled = pd.DataFrame(features_scaled, columns=columns_to_keep, index=dataset.index)\n",
    "\n",
    "# Concatenate with targets\n",
    "new_dataset = pd.concat([features_scaled, targets], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1434, 65, 290) (1434, 290)\n",
      "(1434, 65, 4) (1434, 4)\n"
     ]
    }
   ],
   "source": [
    "features_input , features_output, input_targets, output_targets = create_sequences(new_dataset, LOOK_BACK, LOOK_FORWARD, CU, DU)\n",
    "\n",
    "\n",
    "print(np.array(features_input).shape, np.array(features_output).shape)\n",
    "\n",
    "print(np.array(input_targets).shape, np.array(output_targets).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1434, 65, 290])\n",
      "\n",
      "=== Fold 1/2 ===\n",
      "Epoch 1/2 - Loss: 25.3031\n",
      "Epoch 2/2 - Loss: 18.9685\n",
      "Saved model for fold 1 at jvgan_models/jvgan_fold1.pt\n",
      "size of val_data: torch.Size([717, 65, 290])\n",
      "size of val_subset: 717\n",
      "\n",
      "=== Fold 2/2 ===\n",
      "Epoch 1/2 - Loss: 25.2167\n",
      "Epoch 2/2 - Loss: 18.6295\n",
      "Saved model for fold 2 at jvgan_models/jvgan_fold2.pt\n",
      "size of val_data: torch.Size([717, 65, 290])\n",
      "size of val_subset: 717\n"
     ]
    }
   ],
   "source": [
    "tensor_data = torch.tensor(features_input, dtype=torch.float32)\n",
    "\n",
    "print(tensor_data.shape)\n",
    "\n",
    "k_folds = 2\n",
    "results = train_jvgan(TensorDataset(tensor_data), input_dim=np.array(features_input).shape[2], num_epochs=2, batch_size=64, k_folds=k_folds, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14644/4030787430.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  JVGAN_model = torch.load(model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 0.0010 | F1: 0.9037 | Precision: 0.8243 | Recall: 1.0000\n",
      "Saved threshold metrics for Fold 1 at jvgan_thresholds/threshold_metrics_fold1.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14644/4030787430.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  JVGAN_model = torch.load(model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 0.0010 | F1: 0.9037 | Precision: 0.8243 | Recall: 1.0000\n",
      "Saved threshold metrics for Fold 2 at jvgan_thresholds/threshold_metrics_fold2.pkl\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import pickle  # or use joblib if preferred\n",
    "\n",
    "\n",
    "# features_input = pd.DataFrame(features_input, columns=dataset.drop(columns=[f\"srscu{CU}_stepStress\", f\"srscu{CU}_stressType\", f\"srsdu{DU}_stepStress\", f\"srsdu{DU}_stressType\"]).columns, index=dataset.index)\n",
    "# input_targets = pd.DataFrame(input_targets, columns=[f\"srscu{CU}_stepStress\", f\"srscu{CU}_stressType\", f\"srsdu{DU}_stepStress\", f\"srsdu{DU}_stressType\"], index = dataset.index)\n",
    "\n",
    "def find_optimal_threshold(model, dataset, true_labels, thresholds=None, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model: Trained JVGAN model.\n",
    "        dataset: Tensor of shape (N, 65, features).\n",
    "        true_labels: Ground truth binary anomaly labels for each sequence (0: normal, 1: anomaly).\n",
    "        thresholds: List or numpy array of threshold values to try.\n",
    "        device: Device to run inference on.\n",
    "        \n",
    "    Returns:\n",
    "        best_threshold: Threshold giving best F1-score.\n",
    "        metrics_dict: Dictionary of threshold to (F1, precision, recall).\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0.001, 0.5, 100)\n",
    "\n",
    "    model.eval()\n",
    "    dataset = torch.tensor(dataset, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Get reconstruction errors\n",
    "    with torch.no_grad():\n",
    "        x_recon, _, _, _, _ = model(dataset)\n",
    "        last_real = dataset[:, -1, :]\n",
    "        last_recon = x_recon[:, -1, :]\n",
    "        errors = F.mse_loss(last_recon, last_real, reduction='none').mean(dim=1).cpu().numpy()\n",
    "\n",
    "    true_labels = np.array(true_labels)\n",
    "    best_f1 = -1\n",
    "    best_threshold = None\n",
    "    metrics_dict = {}\n",
    "\n",
    "    for thresh in thresholds:\n",
    "        preds = (errors > thresh).astype(int)\n",
    "        f1 = f1_score(true_labels, preds)\n",
    "        precision = precision_score(true_labels, preds, zero_division=0)\n",
    "        recall = recall_score(true_labels, preds, zero_division=0)\n",
    "\n",
    "        metrics_dict[thresh] = (f1, precision, recall)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = thresh\n",
    "\n",
    "    print(f\"Best Threshold: {best_threshold:.4f} | F1: {best_f1:.4f} | Precision: {metrics_dict[best_threshold][1]:.4f} | Recall: {metrics_dict[best_threshold][2]:.4f}\")\n",
    "    return best_threshold, metrics_dict\n",
    "\n",
    "\n",
    "save_metrics_dir = \"jvgan_thresholds\"\n",
    "os.makedirs(save_metrics_dir, exist_ok=True)\n",
    "\n",
    "for i in range(1, k_folds + 1):\n",
    "    # Load model using torch.load\n",
    "    model_path = f\"jvgan_models/jvgan_fold{i}.pt\"\n",
    "    JVGAN_model = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Get true labels from input_targets\n",
    "    true_labels = []\n",
    "    for sample in input_targets:  # i-1 because Python is 0-indexed\n",
    "        last_sample_target = sample[-1][:]\n",
    "        last_sample_target = last_sample_target.reshape(1, 4)\n",
    "        last_sample_target = pd.DataFrame(last_sample_target, columns=[f\"srscu{CU}_stepStress\", f\"srscu{CU}_stressType\", f\"srsdu{DU}_stepStress\", f\"srsdu{DU}_stressType\"])\n",
    "        if (last_sample_target[f'srscu{CU}_stressType'].item() != 0 or last_sample_target[f'srsdu{DU}_stressType'].item() != 0):\n",
    "            true_labels.append(1)\n",
    "        else:\n",
    "            true_labels.append(0)\n",
    "    \n",
    "    # Compute best threshold\n",
    "    best_threshold, metrics_dict = find_optimal_threshold(\n",
    "        JVGAN_model,\n",
    "        dataset=features_input,  # Assuming features_input is a list of tensors per fold\n",
    "        true_labels=true_labels,\n",
    "        thresholds=np.linspace(0.001, 0.5, 100),\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Save best threshold and metrics_dict to a file\n",
    "    results = {\n",
    "        \"fold\": i,\n",
    "        \"best_threshold\": best_threshold,\n",
    "        \"metrics_dict\": metrics_dict\n",
    "    }\n",
    "\n",
    "    with open(Path(save_metrics_dir) / f\"threshold_metrics_fold{i}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(results, f)\n",
    "    with open(Path(save_metrics_dir) / f\"threshold_metrics_fold{i}.txt\", \"w\") as f:\n",
    "        f.write(str(results))\n",
    "\n",
    "    print(f\"Saved threshold metrics for Fold {i} at {save_metrics_dir}/threshold_metrics_fold{i}.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
