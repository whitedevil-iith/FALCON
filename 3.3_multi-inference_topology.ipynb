{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Set device (GPU if available)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*weights_only=False.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Define LSTM Model**\n",
    "class LSTMForecaster(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTMForecaster, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
    "        c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
    "        out, _ = self.lstm(x, (h_0, c_0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "def autoregressive_forecast(model, initial_sequence, steps_ahead=5, device='cpu'):\n",
    "    # Ensure input is a torch tensor of shape (1, 60, M)\n",
    "    if isinstance(initial_sequence, np.ndarray):\n",
    "        initial_sequence = torch.tensor(initial_sequence, dtype=torch.float32)\n",
    "    \n",
    "    if len(initial_sequence.shape) == 2:\n",
    "        initial_sequence = initial_sequence.unsqueeze(0)\n",
    "\n",
    "    initial_sequence = initial_sequence.to(device)\n",
    "    \n",
    "    generated_sequence = []\n",
    "\n",
    "    current_sequence = initial_sequence.clone()\n",
    "\n",
    "    for _ in range(steps_ahead):\n",
    "        with torch.no_grad():\n",
    "            # Predict next step\n",
    "            next_step = model(current_sequence)  # (1, M)\n",
    "        \n",
    "        generated_sequence.append(next_step.squeeze(0))  # (M,)\n",
    "        \n",
    "        # Prepare input for next step by appending prediction and removing oldest timestep\n",
    "        next_step_expanded = next_step.unsqueeze(1)  # (1, 1, M)\n",
    "        current_sequence = torch.cat((current_sequence[:, 1:, :], next_step_expanded), dim=1)  # (1, 60, M)\n",
    "\n",
    "    return torch.stack(generated_sequence)  # (steps_ahead, M)\n",
    "\n",
    "# === JVGAN COMPONENTS ===\n",
    "class JVGANEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(JVGANEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, 128, batch_first=True)\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        h = h_n.squeeze(0)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        z = mu + std * torch.randn_like(std)\n",
    "        return z, mu, logvar\n",
    "\n",
    "class JVGANDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, input_dim, sequence_length=65):\n",
    "        super(JVGANDecoder, self).__init__()\n",
    "        self.latent_to_hidden = nn.Linear(latent_dim, 128)\n",
    "        self.lstm = nn.LSTM(128, 128, batch_first=True)\n",
    "        self.output_layer = nn.Linear(128, input_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def forward(self, z):\n",
    "        h0 = self.latent_to_hidden(z).unsqueeze(1).repeat(1, self.sequence_length, 1)\n",
    "        lstm_out, _ = self.lstm(h0)\n",
    "        return self.output_layer(lstm_out)\n",
    "\n",
    "class JVGANDiscriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(JVGANDiscriminator, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, 128, batch_first=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        return self.classifier(h_n.squeeze(0))\n",
    "\n",
    "class JVGAN(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=32, sequence_length=65):\n",
    "        super(JVGAN, self).__init__()\n",
    "        self.encoder = JVGANEncoder(input_dim, latent_dim)\n",
    "        self.decoder = JVGANDecoder(latent_dim, input_dim, sequence_length)\n",
    "        self.discriminator = JVGANDiscriminator(input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        d_real = self.discriminator(x)\n",
    "        d_fake = self.discriminator(x_recon.detach())\n",
    "        return x_recon, d_real, d_fake, mu, logvar\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def detect_anomaly(x, model, threshold):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_recon, _, _, _, _ = model(x)\n",
    "        last_real = x[:, -1, :]\n",
    "        last_recon = x_recon[:, -1, :]\n",
    "        error = F.mse_loss(last_recon, last_real, reduction='none').mean(dim=1)\n",
    "        return error > threshold, error\n",
    "\n",
    "def get_result(target_df, CU, DU):\n",
    "    \"\"\"\n",
    "    Compare target DataFrame with original dataset to calculate accuracy.\n",
    "    Args:\n",
    "        target_df (pd.DataFrame): DataFrame with CU/DU targets.\n",
    "        CU (int): CU index.\n",
    "        DU (int): DU index.\n",
    "    Returns:\n",
    "        accuracy (float): Accuracy of the CU/DU targets against the original dataset.\n",
    "    \"\"\"\n",
    "    original_dataframe = pd.read_csv(f'dataset_srscu{CU}_srsdu{DU}.csv')\n",
    "\n",
    "    score = 0\n",
    "\n",
    "    for df in target_df.itertuples():\n",
    "        timestamp = df.Timestamp\n",
    "        srscu_stepStress = df.srscu_stepStress\n",
    "        srsdu_stepStress = df.srsdu_stepStress\n",
    "        \n",
    "        # Find the corresponding row in the original DataFrame\n",
    "        original_row = original_dataframe[original_dataframe['Timestamp'] == timestamp]\n",
    "        \n",
    "        # compare the values of original srscu_stressType and srsdu_stressType with the new values\n",
    "        if not original_row.empty:\n",
    "            original_srscu_stressType = original_row['srscu_stressType'].values[0]\n",
    "            original_srsdu_stressType = original_row['srsdu_stressType'].values[0]\n",
    "            \n",
    "            # if one match score 1 if both 2 and calculare accuracy accordingly\n",
    "            if(srscu_stepStress==original_srscu_stressType):\n",
    "                score += 1\n",
    "            if(srsdu_stepStress==original_srsdu_stressType):\n",
    "                score += 1\n",
    "    # Calculate accuracy# === Step 1: Load the full causal graph ===\n",
    "    with open(\"causal_graph.gpickle\", \"rb\") as f:\n",
    "        full_graph = pickle.load(f)\n",
    "    accuracy = score / (2 * len(target_df))  # Each anomaly can contribute up to 2 points\n",
    "    print(f\"Accuracy of CU/DU targets against original dataset: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Score = {score/2}, Total Anomalies = {len(target_df)}\")\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# **Data Preprocessing**\n",
    "# Handle missing values\n",
    "def preprocessing(features):\n",
    "    \"\"\"\n",
    "    Preprocess the features DataFrame by handling missing values and duplicates.\n",
    "    \"\"\"\n",
    "    # Handle missing values\n",
    "    features_Timestamp = {'Timestamp': features['Timestamp'].copy()}  # Preserve Timestamp for later\n",
    "    features = features.drop(columns=['Timestamp'])\n",
    "\n",
    "    features = features.apply(lambda x: x.fillna(0) if x.isna().all() else x)\n",
    "    # column_names = features.columns\n",
    "\n",
    "    threshold = 0.6 * len(features)\n",
    "    features = features.loc[:, ~features.columns.duplicated()]  # Remove duplicates\n",
    "\n",
    "    for col in features.columns:\n",
    "        nan_count = features[col].isna().sum()\n",
    "        if int(nan_count) > threshold:  # Explicit scalar conversion\n",
    "            mode_value = features[col].mode().iloc[0] if not features[col].mode().empty else 0\n",
    "            features[col].fillna(mode_value, inplace=True)\n",
    "\n",
    "    numeric_cols = features.select_dtypes(include=[np.number]).columns\n",
    "    features[numeric_cols] = features[numeric_cols].fillna(features[numeric_cols].mean())\n",
    "    \n",
    "    # return pd.DataFrame(features, columns=column_names, index=features_Timestamp)\n",
    "    return features, features_Timestamp\n",
    "\n",
    "def LSTM_inference(LSTM_model, pca_features_input, pca_features_output, steps_ahead=5):\n",
    "    full_sequences_with_predictions = []\n",
    "\n",
    "    for i, sequence in enumerate(pca_features_input):\n",
    "        temp_columns = sequence.columns.difference(['Timestamp'])\n",
    "        sequence = sequence.drop(columns=['Timestamp']).values  # Ensure sequence is only numeric data\n",
    "        predicted_future = autoregressive_forecast(LSTM_model, sequence, steps_ahead=steps_ahead)  # (5, M)\n",
    "        \n",
    "        # Convert input sequence and prediction to tensors if not already\n",
    "        if isinstance(sequence, np.ndarray):\n",
    "            sequence_tensor = torch.tensor(sequence, dtype=torch.float32)\n",
    "        else:\n",
    "            sequence_tensor = sequence\n",
    "\n",
    "        # Concatenate original 60 input samples with 5 predicted ones → (65, M)\n",
    "        full_sequence = torch.cat([sequence_tensor, predicted_future.cpu()], dim=0)\n",
    "        full_sequence = pd.DataFrame(full_sequence.numpy(), columns=temp_columns, index = pd.Index(list(pca_features_input[i].index) + list(pca_features_output[i].index)))  # Convert to DataFrame for consistency\n",
    "        full_sequences_with_predictions.append(full_sequence)\n",
    "        # print(full_sequence.numpy())\n",
    "\n",
    "    # Final shape: (num_sequences, 65, M)\n",
    "    full_sequences_with_predictions = np.array([df.to_numpy() for df in full_sequences_with_predictions])\n",
    "\n",
    "    print(\"Shape of sequences with input + predictions:\", full_sequences_with_predictions.shape)\n",
    "\n",
    "    return full_sequences_with_predictions\n",
    "\n",
    "\n",
    "\n",
    "# === Step 2: Define wrapper model returning last timestep ===\n",
    "class LastStepDecoder(nn.Module):\n",
    "    def __init__(self, jvgan_model):\n",
    "        super().__init__()\n",
    "        self.encoder = jvgan_model.encoder\n",
    "        self.decoder = jvgan_model.decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, _, _ = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon[:, -1, :]  # (batch_size, M)\n",
    "\n",
    "\n",
    "def RCA(is_anomaly, full_sequences_with_predictions, JVGAN_model, original_feature_names, CU, DU, device='cpu'):\n",
    "    \"\"\"\n",
    "    Perform Integrated Gradients (IG) for anomaly detection.\n",
    "    Args:\n",
    "        is_anomaly (pd.DataFrame): DataFrame with anomaly detection results.\n",
    "        full_sequences_with_predictions (np.ndarray): Full sequences with predictions.\n",
    "        model (JVGAN): The JVGAN model used for reconstruction.\n",
    "        original_feature_names (list): List of original feature names.\n",
    "        device (str): Device to run the computations on ('cpu' or 'cuda').\n",
    "    \"\"\"\n",
    "    # Ensure model is in evaluation mode\n",
    "    # === Step 3: Select Anomalous Samples ===\n",
    "    anomaly_indices = torch.nonzero(torch.tensor(is_anomaly['Anomaly Detected'])).squeeze().cpu().numpy()\n",
    "    if anomaly_indices.ndim == 0:\n",
    "        anomaly_indices = anomaly_indices.reshape(1)\n",
    "\n",
    "    # Get corresponding DataFrame index values\n",
    "    anomaly_df_indices = is_anomaly.index[anomaly_indices]\n",
    "\n",
    "    max_explain = min(100, len(anomaly_indices))\n",
    "    samples_to_explain = torch.tensor(full_sequences_with_predictions[anomaly_indices[:max_explain]],\n",
    "                                    dtype=torch.float32).to(device)\n",
    "\n",
    "    # === Step 4: Setup model and IG explainer ===\n",
    "    wrapped_decoder = LastStepDecoder(JVGAN_model).to(device).eval()\n",
    "    ig = IntegratedGradients(wrapped_decoder)\n",
    "    baseline = torch.zeros_like(samples_to_explain)\n",
    "\n",
    "    # === Step 5: Compute attributions ===\n",
    "    all_attributions = []\n",
    "\n",
    "    for i in range(samples_to_explain.size(0)):\n",
    "        input_sample = samples_to_explain[i].unsqueeze(0)         # (1, 65, M)\n",
    "        baseline_sample = baseline[i].unsqueeze(0)                # (1, 65, M)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            recon = wrapped_decoder(input_sample)                 # (1, M)\n",
    "            original = input_sample[:, -1, :]                     # (1, M)\n",
    "            recon_error = torch.abs(recon - original)             # (1, M)\n",
    "            target_feature = torch.argmax(recon_error).item()     # int\n",
    "\n",
    "        # Compute IG for the selected output index\n",
    "        attr, _ = ig.attribute(input_sample, baselines=baseline_sample,\n",
    "                            target=target_feature,\n",
    "                            return_convergence_delta=True)\n",
    "\n",
    "        # Attribution shape: (1, 65, M), extract last step\n",
    "        attr_last_step = attr.squeeze(0)[-1].detach().cpu().numpy()\n",
    "        all_attributions.append(attr_last_step)\n",
    "\n",
    "        # === Step 6: Print Top 10 Features ===\n",
    "        top_10_idx = np.argsort(np.abs(attr_last_step))[-10:][::-1]\n",
    "        # print(f\"\\nTop 10 impacting features using Integrated Gradients for Anomalous Sample #{i+1}:\")\n",
    "        # for rank, idx in enumerate(top_10_idx, 1):\n",
    "        #     print(f\"{rank}. {original_feature_names[idx]} | IG Value: {attr_last_step[idx]:.4f}\")\n",
    "\n",
    "    # === Step 7: Save IG attributions to CSV ===\n",
    "    ig_df = pd.DataFrame(all_attributions, columns=original_feature_names, index=anomaly_df_indices)\n",
    "    ig_df.to_csv(f'integrated_gradients_srscu{CU}_srsdu{DU}.csv', index=True)\n",
    "    return ig_df\n",
    "\n",
    "\n",
    "def create_sequences(features_pca, targets, look_back, look_forward, CU, DU):\n",
    "    \"\"\"\n",
    "    Create LSTM-ready sequences from a DataFrame.\n",
    "    \"\"\"\n",
    "    pca_features_input, pca_features_output, input_targets, output_targets = [], [], [], []\n",
    "\n",
    "    for i in range(len(features_pca) - look_back - look_forward + 1):\n",
    "        pca_features_input.append(features_pca.iloc[i:(i + look_back)])\n",
    "        pca_features_output.append(features_pca.iloc[(i + look_back):i + look_back + look_forward ])\n",
    "\n",
    "        input_targets.append(targets.iloc[i:(i + look_back)].values)\n",
    "        output_targets.append(targets.iloc[(i + look_back):i + look_back + look_forward ])  # assuming this is a scalar/class label\n",
    "\n",
    "    return pca_features_input, pca_features_output, input_targets, output_targets\n",
    "\n",
    "\n",
    "def inverse_pca(full_sequences_with_predictions, pca, look_back, look_forward):\n",
    "    # Flatten from (num_sequences, 65, reduced_dim) → (num_sequences * 65, reduced_dim)\n",
    "    full_sequences_with_predictions = full_sequences_with_predictions.reshape(-1, full_sequences_with_predictions.shape[-1])\n",
    "    print(\"Shape before inverse PCA:\", full_sequences_with_predictions.shape)\n",
    "\n",
    "    # Inverse PCA transformation → (num_sequences * 65, original_feature_dim)\n",
    "    full_sequences_with_predictions = pca.inverse_transform(full_sequences_with_predictions)\n",
    "    print(\"Shape after inverse PCA:\", full_sequences_with_predictions.shape)\n",
    "\n",
    "    # Reshape back to (num_sequences, 65, original_feature_dim)\n",
    "    original_feature_dim = pca.inverse_transform(np.zeros((1, pca.n_components_))).shape[1]\n",
    "\n",
    "    full_sequences_with_predictions = full_sequences_with_predictions.reshape(\n",
    "        -1, look_back + look_forward, original_feature_dim\n",
    "    )\n",
    "    print(\"Final shape after inverse PCA:\", full_sequences_with_predictions.shape)\n",
    "    \n",
    "    return full_sequences_with_predictions\n",
    "\n",
    "def JVGAN_anomaly_detection(model, full_sequences_with_predictions, output_targets, threshold):\n",
    "    is_anomaly, recon_errors = detect_anomaly(torch.tensor(full_sequences_with_predictions, dtype=torch.float32), model, threshold)\n",
    "    anomaly_indices = [df.iloc[-1]['Timestamp'] for df in output_targets]\n",
    "    is_anomaly = pd.DataFrame(is_anomaly.numpy(), columns=['Anomaly Detected'], index=anomaly_indices)\n",
    "\n",
    "\n",
    "    true_labels = [1 if (val.iloc[-1][f'srscu_stressType'] != 0 or val.iloc[-1][f'srsdu_stressType'] != 0) else 0 for val in output_targets]\n",
    "    # Calculate accuracy\n",
    "    # accuracy = np.mean(is_anomaly.numpy() == true_labels)\n",
    "    predicted_labels = []\n",
    "    for val in np.array(is_anomaly['Anomaly Detected']):\n",
    "        if(val==False):\n",
    "            predicted_labels.append(0)\n",
    "        else:\n",
    "            predicted_labels.append(1)\n",
    "    accuracy = np.mean(predicted_labels == true_labels)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(true_labels, np.array(is_anomaly['Anomaly Detected']))\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    # Print classification report\n",
    "    print(classification_report(true_labels, np.array(is_anomaly['Anomaly Detected']), target_names=['Normal', 'Anomalous']))\n",
    "    # Print confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, np.array(is_anomaly['Anomaly Detected']))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    # Save the results to a CSV file\n",
    "    results_df = pd.DataFrame({\n",
    "        'True Label': true_labels,\n",
    "        'Timestamp': anomaly_indices,\n",
    "        'Predicted Label': np.array(is_anomaly['Anomaly Detected']),\n",
    "        'Reconstruction Error': recon_errors.numpy()\n",
    "    })\n",
    "    results_df.to_csv(f'results_srscu{CU}_srsdu{DU}.csv', index=False)\n",
    "\n",
    "    return is_anomaly, recon_errors, accuracy, f1, results_df\n",
    "\n",
    "# === Step 2: Define function to get subgraph and root causes ===\n",
    "def get_subgraph_and_roots(graph, top_features):\n",
    "    sub_G = graph.subgraph(top_features).copy()\n",
    "    root_nodes = [n for n in sub_G.nodes if sub_G.in_degree(n) == 0]\n",
    "    return sub_G, root_nodes\n",
    "\n",
    "def get_root_features(ig_df, original_feature_names, full_graph, CU, DU, save_dir=\"anomaly_subgraphs\", save_subgraphs=True):\n",
    "    \"\"\"\n",
    "    Analyze Integrated Gradients (IG) and causal graph to find root causes for each anomaly.\n",
    "    Args:\n",
    "        ig_df (pd.DataFrame): DataFrame with IG values.\n",
    "        original_feature_names (list): List of original feature names.\n",
    "        full_graph (nx.Graph): Full causal graph.\n",
    "        CU (int): CU index.\n",
    "        DU (int): DU index.\n",
    "    Returns:\n",
    "        anomaly_subgraphs (dict): Dictionary of subgraphs for each anomaly.\n",
    "        root_cause_records (list): List of root cause records.\n",
    "    \"\"\"\n",
    "    # === Step 3: Analyze IG and causal graph for each anomaly ===\n",
    "    anomaly_subgraphs = {}\n",
    "    root_cause_records = []\n",
    "\n",
    "    for i, row in ig_df.iterrows():\n",
    "        anomaly_id = f\"anomaly_{i}\"\n",
    "        \n",
    "        # Step 3.1: Top 10 features by IG magnitude\n",
    "        abs_values = np.abs(row.values)\n",
    "        top_10_idx = np.argsort(abs_values)[-10:][::-1]\n",
    "        top_features = [original_feature_names[idx] for idx in top_10_idx]\n",
    "\n",
    "        # Step 3.2: Build causal subgraph and get root causes\n",
    "        subgraph, roots = get_subgraph_and_roots(full_graph, top_features)\n",
    "        anomaly_subgraphs[anomaly_id] = subgraph\n",
    "\n",
    "        if roots:\n",
    "            for rank, r in enumerate(roots, 1):\n",
    "                root_cause_records.append({\n",
    "                    \"anomaly_id\": anomaly_id,\n",
    "                    \"Timestamp\": row.name,  # Use the index as Timestamp\n",
    "                    \"root_cause\": r,\n",
    "                    \"rank\": rank\n",
    "                })\n",
    "        else:\n",
    "            cu_count, du_count,app_count = 0, 0, 0\n",
    "            for r in top_features:\n",
    "                if \"srscu\" in r:\n",
    "                    cu_count += 1\n",
    "                elif \"srsdu\" in r:\n",
    "                    du_count += 1\n",
    "                elif \"Application\" in r:\n",
    "                    app_count += 1\n",
    "            if cu_count > du_count and cu_count > app_count:\n",
    "                root_cause_records.append({\n",
    "                    \"anomaly_id\": anomaly_id,\n",
    "                    \"Timestamp\": row.name,  # Use the index as Timestamp\n",
    "                    \"root_cause\": \"srscu\",\n",
    "                    \"rank\": 1\n",
    "                })\n",
    "            elif du_count > cu_count and du_count > app_count:\n",
    "                root_cause_records.append({\n",
    "                    \"anomaly_id\": anomaly_id,\n",
    "                    \"Timestamp\": row.name,  # Use the index as Timestamp\n",
    "                    \"root_cause\": \"srsdu\",\n",
    "                    \"rank\": 1\n",
    "                })\n",
    "            elif app_count > cu_count and app_count > du_count:\n",
    "                root_cause_records.append({\n",
    "                    \"anomaly_id\": anomaly_id,\n",
    "                    \"Timestamp\": row.name,  # Use the index as Timestamp\n",
    "                    \"root_cause\": \"Application\",\n",
    "                    \"rank\": 1\n",
    "                })\n",
    "            elif(cu_count == du_count):\n",
    "                root_cause_records.append({\n",
    "                    \"anomaly_id\": anomaly_id,\n",
    "                    \"Timestamp\": row.name,  # Use the index as Timestamp\n",
    "                    \"root_cause\": \"srscu\",\n",
    "                    \"rank\": 1\n",
    "                })\n",
    "                root_cause_records.append({\n",
    "                    \"anomaly_id\": anomaly_id,\n",
    "                    \"Timestamp\": row.name,  # Use the index as Timestamp\n",
    "                    \"root_cause\": \"srsdu\",\n",
    "                    \"rank\": 2\n",
    "                })\n",
    "            else:\n",
    "                root_cause_records.append({\n",
    "                    \"anomaly_id\": anomaly_id,\n",
    "                    \"Timestamp\": row.name,  # Use the index as Timestamp\n",
    "                    \"root_cause\": \"node\",\n",
    "                    \"rank\": 1\n",
    "                })\n",
    "\n",
    "            # root_cause_records.append({\n",
    "            #     \"anomaly_id\": anomaly_id,\n",
    "            #     \"root_cause\": [if \"srscu\" in r for r in top_features],\n",
    "            #     \"rank\": 1\n",
    "            # })\n",
    "\n",
    "    # === Step 4: Save root causes and optionally subgraphs ===\n",
    "    root_df = pd.DataFrame(root_cause_records)\n",
    "    root_df.to_csv(f'root_causes_top10_srscu{CU}_srsdu{DU}.csv', index=False)\n",
    "    print(f\"Saved root causes per anomaly to: root_causes_top10_srscu{CU}_srsdu{DU}.csv\")\n",
    "\n",
    "    if(save_subgraphs==True):\n",
    "        # Optional: Save each subgraph individually as a .gpickle\n",
    "        save_dir = \"anomaly_subgraphs\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        for anomaly_id, sub_G in anomaly_subgraphs.items():\n",
    "            with open(os.path.join(save_dir, f\"{anomaly_id}_subgraph.gpickle\"), \"wb\") as f:\n",
    "                pickle.dump(sub_G, f)\n",
    "\n",
    "    return anomaly_subgraphs, root_df\n",
    "\n",
    "\n",
    "def generate_labels(root_df, CU, DU):\n",
    "    \"\"\"\n",
    "    Generate binary targets based on root cause counts.\n",
    "    Args:\n",
    "        root_df (pd.DataFrame): DataFrame with root causes.\n",
    "        CU (int): CU index.\n",
    "        DU (int): DU index.\n",
    "    Returns:\n",
    "        target_df (pd.DataFrame): DataFrame with binary targets.\n",
    "    \"\"\"\n",
    "    # === Step 5: Generate binary targets from root cause counts ===\n",
    "    grouped = root_df.groupby('anomaly_id')['root_cause'].apply(list).reset_index()\n",
    "\n",
    "    target_records = []\n",
    "\n",
    "    for _, row in grouped.iterrows():\n",
    "        anomaly_id = row['anomaly_id']\n",
    "        causes = row['root_cause']\n",
    "\n",
    "        srscu_count = sum(1 for c in causes if 'srscu' in c)\n",
    "        srsdu_count = sum(1 for c in causes if 'srsdu' in c)\n",
    "        \n",
    "        app_count = sum(1 for c in causes if 'Application' in c)\n",
    "\n",
    "        app_count += sum(1 for c in causes if 'node' in c)\n",
    "\n",
    "\n",
    "        srscu_count += app_count  # Treat Application as srscu for target generation\n",
    "        srsdu_count += app_count  # Treat Application as srsdu for target generation\n",
    "\n",
    "        if srscu_count > srsdu_count:\n",
    "            target_srscu, target_srsdu = 1, 0\n",
    "            \n",
    "        elif srsdu_count > srscu_count:\n",
    "            target_srscu, target_srsdu = 0, 1\n",
    "        else:  # equal counts or both 0\n",
    "            target_srscu, target_srsdu = 1, 1\n",
    "\n",
    "        target_records.append({\n",
    "            'anomaly_id': anomaly_id,\n",
    "            \"Timestamp\": row.name,  # Use the index as Timestamp\n",
    "            'srscu_stepStress': target_srscu,\n",
    "            'srsdu_stepStress': target_srsdu\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    target_df = pd.DataFrame(target_records)\n",
    "    target_df.to_csv(f'cu_du_targets_srscu{CU}_srsdu{DU}.csv', index=False)\n",
    "    print(f\"Saved CU/DU targets to: cu_du_targets_srscu{CU}_srsdu{DU}.csv\")\n",
    "\n",
    "    return target_df\n",
    "\n",
    "def apply_pca(features, scaler, pca, features_Timestamp):\n",
    "    \"\"\"\n",
    "    Apply MinMax scaling and PCA transformation to the features DataFrame.\n",
    "    \"\"\"\n",
    "        # Scale the new features using the same scaler\n",
    "    features_scaled = scaler.transform(features)\n",
    "\n",
    "\n",
    "    # Apply PCA transformation using the loaded PCA model\n",
    "    features_pca = pca.transform(features_scaled)\n",
    "\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    features_pca = pd.DataFrame(features_pca, columns=[f'Reduced-{i+1}' for i in range(pca.n_components_)])\n",
    "    features_pca['Timestamp'] = features_Timestamp['Timestamp'].values  # Add Timestamp back\n",
    "    features_pca.index = features.index\n",
    "    \n",
    "    return features_pca \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JVGAN(\n",
       "  (encoder): JVGANEncoder(\n",
       "    (lstm): LSTM(344, 128, batch_first=True)\n",
       "    (fc_mu): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (fc_logvar): Linear(in_features=128, out_features=32, bias=True)\n",
       "  )\n",
       "  (decoder): JVGANDecoder(\n",
       "    (latent_to_hidden): Linear(in_features=32, out_features=128, bias=True)\n",
       "    (lstm): LSTM(128, 128, batch_first=True)\n",
       "    (output_layer): Linear(in_features=128, out_features=344, bias=True)\n",
       "  )\n",
       "  (discriminator): JVGANDiscriminator(\n",
       "    (lstm): LSTM(344, 128, batch_first=True)\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.2)\n",
       "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "      (3): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load models\n",
    "scaler = joblib.load('minmax_scaler.pkl')\n",
    "pca = joblib.load('pca_model.pkl')\n",
    "\n",
    "\n",
    "# Load LSTM the model\n",
    "LSTM_model_path = 'lstm_models/lstm_fold1_RMSprop.pt'\n",
    "LSTM_model = torch.load(LSTM_model_path, map_location=device)\n",
    "LSTM_model.eval()\n",
    "\n",
    "\n",
    "\n",
    "# === Step 1: Load the full causal graph ===\n",
    "with open(\"causal_graph.gpickle\", \"rb\") as f:\n",
    "    full_graph = pickle.load(f)\n",
    "\n",
    "\n",
    "# load the JVGAN model\n",
    "JVGAN_model_path = 'jvgan_models/jvgan_fold1.pt'\n",
    "# testing of jvgan model\n",
    "JVGAN_model = torch.load(JVGAN_model_path, map_location=device)\n",
    "# Validation\n",
    "JVGAN_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topology is as follows: \n",
      "{'srscu0': ['srsdu0'], 'srscu1': ['srsdu1'], 'srscu2': ['srsdu2'], 'srscu3': ['srsdu3']}\n",
      "\n",
      "Application Features: {'pusch_ta_ns', 'bsr', 'ul_nof_nok', 'pucch_ta_ns', 'dl_bs', 'dl_nof_nok', 'ul_nof_ok', 'ri', 'ul_brate', 'cqi', 'srs_ta_ns', 'pusch_snr_db', 'dl_brate', 'dl_nof_ok', 'ta_ns', 'dl_mcs', 'ul_mcs', 'pucch_snr_db'}\n"
     ]
    }
   ],
   "source": [
    "NoOfCUs = 4\n",
    "NoOfDUs = 4\n",
    "\n",
    "# Creating Topology\n",
    "topology = {}\n",
    "\n",
    "# Form the graph where srscu0 connects to srsdu0, srscu1 to srsdu1, and so on\n",
    "for i in range(min(NoOfCUs, NoOfDUs)):  # Prevent index errors\n",
    "    topology[f\"srscu{i}\"] = [f\"srsdu{i}\"]\n",
    "\n",
    "UEsOfDUs = {}\n",
    "\n",
    "for i in range(NoOfDUs):\n",
    "    UEsOfDUs[f\"srsdu{i}\"] = []\n",
    "\n",
    "\n",
    "# Display the graph\n",
    "print(f'Topology is as follows: \\n{topology}', end=\"\\n\\n\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = pd.read_csv('small_sample.csv')\n",
    "# dataset = dataset[:int(0.01*len(dataset))]\n",
    "\n",
    "dataset.index = dataset['Timestamp']\n",
    "dataset = dataset.drop(columns=['Timestamp'])\n",
    "\n",
    "# for column in dataset.columns:\n",
    "#     if 'PCI' in column:\n",
    "#         print(column)\n",
    "\n",
    "# Dictionary to store for each PCI:\n",
    "# a list of RNTIs and a list of metric types\n",
    "pci_info_map = defaultdict(lambda: {'rntis': set(), 'metrics': set()})\n",
    "\n",
    "\n",
    "# Process each column\n",
    "for column in dataset.columns:\n",
    "    match = re.match(r'PCI-(\\d+)_RNTI-(\\d+)_([a-zA-Z0-9_]+)', column)\n",
    "    if match:\n",
    "        pci = match.group(1)\n",
    "        rnti = match.group(2)\n",
    "        metric = match.group(3)\n",
    "        pci_info_map[pci]['rntis'].add(rnti)\n",
    "        pci_info_map[pci]['metrics'].add(metric)\n",
    "\n",
    "# # Print results\n",
    "# for pci, info in pci_info_map.items():\n",
    "#     print(f\"PCI-{pci}:\")\n",
    "#     print(f\"  RNTIs: {sorted(info['rntis'])}\")\n",
    "#     print(f\"  Metrics: {sorted(info['metrics'])}\\n\\t   Length: {len(info['metrics'])}\")\n",
    "\n",
    "\n",
    "application_features = pci_info_map['1'][ 'metrics']\n",
    "print(f\"Application Features: {application_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>((node_memory_MemTotal_bytes{instance=\"node-exporter:9100\", job=\"node\"} - node_memory_MemFree_bytes{instance=\"node-exporter:9100\", job=\"node\"}) / node_memory_MemTotal_bytes{instance=\"node-exporter:9100\", job=\"node\"}) * 100</th>\n",
       "      <th>((node_memory_SwapTotal_bytes{instance=\"node-exporter:9100\",job=\"node\"} - node_memory_SwapFree_bytes{instance=\"node-exporter:9100\",job=\"node\"}) / (node_memory_SwapTotal_bytes{instance=\"node-exporter:9100\",job=\"node\"})) * 100</th>\n",
       "      <th>((sum(container_memory_usage_bytes{name=\"srscu\",instance=\"cadvisor:8080\"}) by (instance) - sum(container_memory_cache{name=\"srscu\",instance=\"cadvisor:8080\"}) by (instance)) / sum(machine_memory_bytes{instance=\"cadvisor:8080\"}) by (instance)) * 100</th>\n",
       "      <th>((sum(container_memory_usage_bytes{name=\"srsdu\",instance=\"cadvisor:8080\"}) by (instance) - sum(container_memory_cache{name=\"srsdu\",instance=\"cadvisor:8080\"}) by (instance)) / sum(machine_memory_bytes{instance=\"cadvisor:8080\"}) by (instance)) * 100</th>\n",
       "      <th>(1 - (node_memory_MemAvailable_bytes{instance=\"node-exporter:9100\", job=\"node\"} / node_memory_MemTotal_bytes{instance=\"node-exporter:9100\", job=\"node\"})) * 100</th>\n",
       "      <th>(node_memory_SwapTotal_bytes{instance=\"node-exporter:9100\",job=\"node\"} - node_memory_SwapFree_bytes{instance=\"node-exporter:9100\",job=\"node\"})</th>\n",
       "      <th>100 * (1 - avg(rate(node_cpu_seconds_total{mode=\"idle\", instance=\"node-exporter:9100\"}[10s])))</th>\n",
       "      <th>100 - ((node_filesystem_avail_bytes{instance=\"node-exporter:9100\",job=\"node\",device!~'rootfs'} * 100) / node_filesystem_size_bytes{instance=\"node-exporter:9100\",job=\"node\",device!~'rootfs'})</th>\n",
       "      <th>100 - ((node_filesystem_avail_bytes{instance=\"node-exporter:9100\",job=\"node\",mountpoint=\"/\",fstype!=\"rootfs\"} * 100) / node_filesystem_size_bytes{instance=\"node-exporter:9100\",job=\"node\",mountpoint=\"/\",fstype!=\"rootfs\"})</th>\n",
       "      <th>...</th>\n",
       "      <th>PCI-4_Average_pusch_ta_ns</th>\n",
       "      <th>PCI-4_Average_pucch_ta_ns</th>\n",
       "      <th>PCI-4_Summation_ul_nof_ok</th>\n",
       "      <th>PCI-4_Summation_ul_nof_nok</th>\n",
       "      <th>PCI-4_Summation_dl_nof_nok</th>\n",
       "      <th>PCI-4_Summation_ul_brate</th>\n",
       "      <th>PCI-4_Summation_dl_bs</th>\n",
       "      <th>PCI-4_Summation_bsr</th>\n",
       "      <th>PCI-4_Summation_dl_brate</th>\n",
       "      <th>PCI-4_Summation_dl_nof_ok</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-05-23 12:58:38.769</th>\n",
       "      <td>2025-05-23 12:58:38.769</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-23 12:58:39.769</th>\n",
       "      <td>2025-05-23 12:58:39.769</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-23 12:58:40.770</th>\n",
       "      <td>2025-05-23 12:58:40.770</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-23 12:58:41.819</th>\n",
       "      <td>2025-05-23 12:58:41.819</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.408751</td>\n",
       "      <td>0.454471</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.108545</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-23 12:58:42.820</th>\n",
       "      <td>2025-05-23 12:58:42.820</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.408947</td>\n",
       "      <td>0.454973</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.533914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 349 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Timestamp  \\\n",
       "Timestamp                                          \n",
       "2025-05-23 12:58:38.769  2025-05-23 12:58:38.769   \n",
       "2025-05-23 12:58:39.769  2025-05-23 12:58:39.769   \n",
       "2025-05-23 12:58:40.770  2025-05-23 12:58:40.770   \n",
       "2025-05-23 12:58:41.819  2025-05-23 12:58:41.819   \n",
       "2025-05-23 12:58:42.820  2025-05-23 12:58:42.820   \n",
       "\n",
       "                         ((node_memory_MemTotal_bytes{instance=\"node-exporter:9100\", job=\"node\"} - node_memory_MemFree_bytes{instance=\"node-exporter:9100\", job=\"node\"}) / node_memory_MemTotal_bytes{instance=\"node-exporter:9100\", job=\"node\"}) * 100  \\\n",
       "Timestamp                                                                                                                                                                                                                                                 \n",
       "2025-05-23 12:58:38.769                                                NaN                                                                                                                                                                                \n",
       "2025-05-23 12:58:39.769                                                NaN                                                                                                                                                                                \n",
       "2025-05-23 12:58:40.770                                                NaN                                                                                                                                                                                \n",
       "2025-05-23 12:58:41.819                                                NaN                                                                                                                                                                                \n",
       "2025-05-23 12:58:42.820                                                NaN                                                                                                                                                                                \n",
       "\n",
       "                         ((node_memory_SwapTotal_bytes{instance=\"node-exporter:9100\",job=\"node\"} - node_memory_SwapFree_bytes{instance=\"node-exporter:9100\",job=\"node\"}) / (node_memory_SwapTotal_bytes{instance=\"node-exporter:9100\",job=\"node\"})) * 100  \\\n",
       "Timestamp                                                                                                                                                                                                                                                   \n",
       "2025-05-23 12:58:38.769                                                NaN                                                                                                                                                                                  \n",
       "2025-05-23 12:58:39.769                                                NaN                                                                                                                                                                                  \n",
       "2025-05-23 12:58:40.770                                                NaN                                                                                                                                                                                  \n",
       "2025-05-23 12:58:41.819                                                NaN                                                                                                                                                                                  \n",
       "2025-05-23 12:58:42.820                                                NaN                                                                                                                                                                                  \n",
       "\n",
       "                         ((sum(container_memory_usage_bytes{name=\"srscu\",instance=\"cadvisor:8080\"}) by (instance) - sum(container_memory_cache{name=\"srscu\",instance=\"cadvisor:8080\"}) by (instance)) / sum(machine_memory_bytes{instance=\"cadvisor:8080\"}) by (instance)) * 100  \\\n",
       "Timestamp                                                                                                                                                                                                                                                                          \n",
       "2025-05-23 12:58:38.769                                                NaN                                                                                                                                                                                                         \n",
       "2025-05-23 12:58:39.769                                                NaN                                                                                                                                                                                                         \n",
       "2025-05-23 12:58:40.770                                                NaN                                                                                                                                                                                                         \n",
       "2025-05-23 12:58:41.819                                           0.408751                                                                                                                                                                                                         \n",
       "2025-05-23 12:58:42.820                                           0.408947                                                                                                                                                                                                         \n",
       "\n",
       "                         ((sum(container_memory_usage_bytes{name=\"srsdu\",instance=\"cadvisor:8080\"}) by (instance) - sum(container_memory_cache{name=\"srsdu\",instance=\"cadvisor:8080\"}) by (instance)) / sum(machine_memory_bytes{instance=\"cadvisor:8080\"}) by (instance)) * 100  \\\n",
       "Timestamp                                                                                                                                                                                                                                                                          \n",
       "2025-05-23 12:58:38.769                                                NaN                                                                                                                                                                                                         \n",
       "2025-05-23 12:58:39.769                                                NaN                                                                                                                                                                                                         \n",
       "2025-05-23 12:58:40.770                                                NaN                                                                                                                                                                                                         \n",
       "2025-05-23 12:58:41.819                                           0.454471                                                                                                                                                                                                         \n",
       "2025-05-23 12:58:42.820                                           0.454973                                                                                                                                                                                                         \n",
       "\n",
       "                         (1 - (node_memory_MemAvailable_bytes{instance=\"node-exporter:9100\", job=\"node\"} / node_memory_MemTotal_bytes{instance=\"node-exporter:9100\", job=\"node\"})) * 100  \\\n",
       "Timestamp                                                                                                                                                                                  \n",
       "2025-05-23 12:58:38.769                                                NaN                                                                                                                 \n",
       "2025-05-23 12:58:39.769                                                NaN                                                                                                                 \n",
       "2025-05-23 12:58:40.770                                                NaN                                                                                                                 \n",
       "2025-05-23 12:58:41.819                                                NaN                                                                                                                 \n",
       "2025-05-23 12:58:42.820                                                NaN                                                                                                                 \n",
       "\n",
       "                         (node_memory_SwapTotal_bytes{instance=\"node-exporter:9100\",job=\"node\"} - node_memory_SwapFree_bytes{instance=\"node-exporter:9100\",job=\"node\"})  \\\n",
       "Timestamp                                                                                                                                                                 \n",
       "2025-05-23 12:58:38.769                                                NaN                                                                                                \n",
       "2025-05-23 12:58:39.769                                                NaN                                                                                                \n",
       "2025-05-23 12:58:40.770                                                NaN                                                                                                \n",
       "2025-05-23 12:58:41.819                                                NaN                                                                                                \n",
       "2025-05-23 12:58:42.820                                                NaN                                                                                                \n",
       "\n",
       "                         100 * (1 - avg(rate(node_cpu_seconds_total{mode=\"idle\", instance=\"node-exporter:9100\"}[10s])))  \\\n",
       "Timestamp                                                                                                                 \n",
       "2025-05-23 12:58:38.769                                                NaN                                                \n",
       "2025-05-23 12:58:39.769                                                NaN                                                \n",
       "2025-05-23 12:58:40.770                                                NaN                                                \n",
       "2025-05-23 12:58:41.819                                          84.108545                                                \n",
       "2025-05-23 12:58:42.820                                          75.533914                                                \n",
       "\n",
       "                         100 - ((node_filesystem_avail_bytes{instance=\"node-exporter:9100\",job=\"node\",device!~'rootfs'} * 100) / node_filesystem_size_bytes{instance=\"node-exporter:9100\",job=\"node\",device!~'rootfs'})  \\\n",
       "Timestamp                                                                                                                                                                                                                 \n",
       "2025-05-23 12:58:38.769                                                NaN                                                                                                                                                \n",
       "2025-05-23 12:58:39.769                                                NaN                                                                                                                                                \n",
       "2025-05-23 12:58:40.770                                                NaN                                                                                                                                                \n",
       "2025-05-23 12:58:41.819                                                NaN                                                                                                                                                \n",
       "2025-05-23 12:58:42.820                                                NaN                                                                                                                                                \n",
       "\n",
       "                         100 - ((node_filesystem_avail_bytes{instance=\"node-exporter:9100\",job=\"node\",mountpoint=\"/\",fstype!=\"rootfs\"} * 100) / node_filesystem_size_bytes{instance=\"node-exporter:9100\",job=\"node\",mountpoint=\"/\",fstype!=\"rootfs\"})  \\\n",
       "Timestamp                                                                                                                                                                                                                                               \n",
       "2025-05-23 12:58:38.769                                                NaN                                                                                                                                                                              \n",
       "2025-05-23 12:58:39.769                                                NaN                                                                                                                                                                              \n",
       "2025-05-23 12:58:40.770                                                NaN                                                                                                                                                                              \n",
       "2025-05-23 12:58:41.819                                                NaN                                                                                                                                                                              \n",
       "2025-05-23 12:58:42.820                                                NaN                                                                                                                                                                              \n",
       "\n",
       "                         ...  PCI-4_Average_pusch_ta_ns  \\\n",
       "Timestamp                ...                              \n",
       "2025-05-23 12:58:38.769  ...                        0.0   \n",
       "2025-05-23 12:58:39.769  ...                        0.0   \n",
       "2025-05-23 12:58:40.770  ...                        0.0   \n",
       "2025-05-23 12:58:41.819  ...                        0.0   \n",
       "2025-05-23 12:58:42.820  ...                        0.0   \n",
       "\n",
       "                         PCI-4_Average_pucch_ta_ns  PCI-4_Summation_ul_nof_ok  \\\n",
       "Timestamp                                                                       \n",
       "2025-05-23 12:58:38.769                        0.0                        0.0   \n",
       "2025-05-23 12:58:39.769                        0.0                        0.0   \n",
       "2025-05-23 12:58:40.770                        0.0                        0.0   \n",
       "2025-05-23 12:58:41.819                        0.0                        0.0   \n",
       "2025-05-23 12:58:42.820                        0.0                        0.0   \n",
       "\n",
       "                         PCI-4_Summation_ul_nof_nok  \\\n",
       "Timestamp                                             \n",
       "2025-05-23 12:58:38.769                         0.0   \n",
       "2025-05-23 12:58:39.769                         0.0   \n",
       "2025-05-23 12:58:40.770                         0.0   \n",
       "2025-05-23 12:58:41.819                         0.0   \n",
       "2025-05-23 12:58:42.820                         0.0   \n",
       "\n",
       "                         PCI-4_Summation_dl_nof_nok  PCI-4_Summation_ul_brate  \\\n",
       "Timestamp                                                                       \n",
       "2025-05-23 12:58:38.769                         0.0                       0.0   \n",
       "2025-05-23 12:58:39.769                         0.0                       0.0   \n",
       "2025-05-23 12:58:40.770                         0.0                       0.0   \n",
       "2025-05-23 12:58:41.819                         0.0                       0.0   \n",
       "2025-05-23 12:58:42.820                         0.0                       0.0   \n",
       "\n",
       "                         PCI-4_Summation_dl_bs  PCI-4_Summation_bsr  \\\n",
       "Timestamp                                                             \n",
       "2025-05-23 12:58:38.769                    0.0                  0.0   \n",
       "2025-05-23 12:58:39.769                    0.0                  0.0   \n",
       "2025-05-23 12:58:40.770                    0.0                  0.0   \n",
       "2025-05-23 12:58:41.819                    0.0                  0.0   \n",
       "2025-05-23 12:58:42.820                    0.0                  0.0   \n",
       "\n",
       "                         PCI-4_Summation_dl_brate  PCI-4_Summation_dl_nof_ok  \n",
       "Timestamp                                                                     \n",
       "2025-05-23 12:58:38.769                       0.0                        0.0  \n",
       "2025-05-23 12:58:39.769                       0.0                        0.0  \n",
       "2025-05-23 12:58:40.770                       0.0                        0.0  \n",
       "2025-05-23 12:58:41.819                       0.0                        0.0  \n",
       "2025-05-23 12:58:42.820                       0.0                        0.0  \n",
       "\n",
       "[5 rows x 349 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import argparse\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser(description=\"Run script with optional CU and DU values.\")\n",
    "# parser.add_argument(\"--cu\", type=int, default=0, help=\"CU value (default: 0)\")\n",
    "# parser.add_argument(\"--du\", type=int, default=0, help=\"DU value (default: 0)\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# CU, DU = args.cu, args.du\n",
    "\n",
    "CU = 0\n",
    "DU = 0\n",
    "\n",
    "# Load dataset\n",
    "dataset = pd.read_csv(f'dataset_srscu{CU}_srsdu{DU}.csv')\n",
    "# dataset = dataset[:int(0.01*len(dataset))]\n",
    "\n",
    "dataset.index = dataset['Timestamp']\n",
    "# dataset = dataset.drop(columns=['Timestamp'])\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         srscu_stepStress  srscu_stressType  srsdu_stepStress  \\\n",
      "Timestamp                                                                       \n",
      "2025-05-23 12:58:38.769                 0                 0                 0   \n",
      "2025-05-23 12:58:39.769                 0                 0                 0   \n",
      "2025-05-23 12:58:40.770                 0                 0                 0   \n",
      "2025-05-23 12:58:41.819                 0                 0                 0   \n",
      "2025-05-23 12:58:42.820                 0                 0                 0   \n",
      "\n",
      "                         srsdu_stressType                Timestamp  \n",
      "Timestamp                                                           \n",
      "2025-05-23 12:58:38.769                 0  2025-05-23 12:58:38.769  \n",
      "2025-05-23 12:58:39.769                 0  2025-05-23 12:58:39.769  \n",
      "2025-05-23 12:58:40.770                 0  2025-05-23 12:58:40.770  \n",
      "2025-05-23 12:58:41.819                 0  2025-05-23 12:58:41.819  \n",
      "2025-05-23 12:58:42.820                 0  2025-05-23 12:58:42.820  \n"
     ]
    }
   ],
   "source": [
    "features = dataset.drop(columns=[f\"srscu_stepStress\", f\"srscu_stressType\", f\"srsdu_stepStress\", f\"srsdu_stressType\"])\n",
    "original_feature_names = features.columns\n",
    "targets = dataset[[f\"srscu_stepStress\", f\"srscu_stressType\", f\"srsdu_stepStress\", f\"srsdu_stressType\", \"Timestamp\"]]\n",
    "\n",
    "print(targets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = targets[[f\"srscu_stressType\", f\"srsdu_stressType\"]].values\n",
    "output = [] \n",
    "for val in true_labels:\n",
    "    if val[0] != 0 or val[1] != 0:\n",
    "        output.append(1)\n",
    "    else:\n",
    "        output.append(0)\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import classification_report\n",
    "# import numpy as np\n",
    "\n",
    "# index = int(0.8*len(features))\n",
    "# # Reassign train/test sets properly for classifier training\n",
    "# X_train, X_test = features[:index], features[index:]  # features was re-assigned above\n",
    "# y_train, y_test = output[:index], output[index:]    # output was re-assigned above\n",
    "\n",
    "# # Initialize and train the Random Forest classifier\n",
    "# rf = RandomForestClassifier(n_estimators=100, random_state=42,  class_weight='balanced')\n",
    "# rf.fit(X_train.drop(columns=['Timestamp']), y_train)\n",
    "\n",
    "# # Predict on the test set\n",
    "\n",
    "# y_pred = rf.predict(X_test.drop(columns=['Timestamp']))\n",
    "\n",
    "# # Print classification report\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "# # Print confusion matrix\n",
    "# conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, features_Timestamp = preprocessing(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/miniconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but PCA was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "features_pca = apply_pca(features, scaler, pca, features_Timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1435, 60, 6)\n",
      "Output shape: (1435, 5, 5)\n",
      "                         srscu_stepStress  srscu_stressType  srsdu_stepStress  \\\n",
      "Timestamp                                                                       \n",
      "2025-05-23 12:59:49.257                 0                 0                 0   \n",
      "2025-05-23 12:59:50.591                 0                 0                 0   \n",
      "2025-05-23 12:59:51.923                 0                 0                 0   \n",
      "2025-05-23 12:59:53.154                 0                 0                 0   \n",
      "2025-05-23 12:59:54.297                 0                 0                 0   \n",
      "\n",
      "                         srsdu_stressType                Timestamp  \n",
      "Timestamp                                                           \n",
      "2025-05-23 12:59:49.257                 0  2025-05-23 12:59:49.257  \n",
      "2025-05-23 12:59:50.591                 0  2025-05-23 12:59:50.591  \n",
      "2025-05-23 12:59:51.923                 0  2025-05-23 12:59:51.923  \n",
      "2025-05-23 12:59:53.154                 0  2025-05-23 12:59:53.154  \n",
      "2025-05-23 12:59:54.297                 0  2025-05-23 12:59:54.297  \n"
     ]
    }
   ],
   "source": [
    "LOOK_BACK, LOOK_FORWARD = 60, 5\n",
    "\n",
    "pca_features_input , pca_features_output, input_targets, output_targets = create_sequences(features_pca, targets, LOOK_BACK, LOOK_FORWARD, CU, DU)\n",
    "\n",
    "print(f\"Input shape: {np.array(pca_features_input).shape}\")\n",
    "print(f\"Output shape: {np.array(output_targets).shape}\")\n",
    "\n",
    "print(output_targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of sequences with input + predictions: (1435, 65, 5)\n"
     ]
    }
   ],
   "source": [
    "full_sequences_with_predictions = LSTM_inference(LSTM_model, pca_features_input, pca_features_output, steps_ahead=LOOK_FORWARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before inverse PCA: (93275, 5)\n",
      "Shape after inverse PCA: (93275, 344)\n",
      "Final shape after inverse PCA: (1435, 65, 344)\n"
     ]
    }
   ],
   "source": [
    "full_sequences_with_predictions = inverse_pca(full_sequences_with_predictions, pca, look_back=LOOK_BACK, look_forward=LOOK_FORWARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.00%\n",
      "F1 Score: 0.11\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.17      0.93      0.29       252\n",
      "   Anomalous       0.80      0.06      0.11      1183\n",
      "\n",
      "    accuracy                           0.21      1435\n",
      "   macro avg       0.49      0.50      0.20      1435\n",
      "weighted avg       0.69      0.21      0.14      1435\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 235   17]\n",
      " [1113   70]]\n"
     ]
    }
   ],
   "source": [
    "# Define the threshold for anomaly detection\n",
    "threshold = 0.05  # Adjust based on your dataset and requirements\n",
    "\n",
    "is_anomaly, recon_errors, accuracy, f1, results_df = JVGAN_anomaly_detection(JVGAN_model, full_sequences_with_predictions, output_targets, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_feature_names = features.columns.tolist()  # after inverse_transform\n",
    "\n",
    "# import shap\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import torch.nn as nn\n",
    "\n",
    "# # === Step 1: Wrap Decoder for SHAP ===\n",
    "# class LastStepDecoder(nn.Module):\n",
    "#     def __init__(self, jvgan_model):\n",
    "#         super().__init__()\n",
    "#         self.encoder = jvgan_model.encoder\n",
    "#         self.decoder = jvgan_model.decoder\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         z, _, _ = self.encoder(x)\n",
    "#         x_recon = self.decoder(z)\n",
    "#         return x_recon[:, -1, :]  # Only last timestep reconstruction\n",
    "\n",
    "# # === Step 2: Select Anomalous Samples ===\n",
    "# anomaly_indices = torch.nonzero(is_anomaly).squeeze().cpu().numpy()\n",
    "# if anomaly_indices.ndim == 0:\n",
    "#     anomaly_indices = anomaly_indices.reshape(1)\n",
    "\n",
    "# # Limit to a small number for SHAP (e.g., first 5–10 anomalies)\n",
    "# max_explain = min(10, len(anomaly_indices))\n",
    "# samples_to_explain = torch.tensor(full_sequences_with_predictions[anomaly_indices[:max_explain]], dtype=torch.float32).to(device)\n",
    "# print(\"Initialization started\")\n",
    "\n",
    "# # === Step 3: Wrap Model & Initialize SHAP Explainer ===\n",
    "# wrapped_decoder = LastStepDecoder(model).to(device)\n",
    "\n",
    "# explainer = shap.GradientExplainer(wrapped_decoder, samples_to_explain)\n",
    "# # Time taken for initialization\n",
    "# print(\"Initialization done. SHAP Explainer ready to compute values.\")\n",
    "\n",
    "# # === Step 4: Compute SHAP Values ===\n",
    "# shap_values = explainer.shap_values(samples_to_explain)  # List with one element: (N, M)\n",
    "# print(\"computation of SHAP values done.\")\n",
    "\n",
    "# # === Step 5: Display Top 10 Features ===\n",
    "# for i, shap_val in enumerate(shap_values[0]):\n",
    "#     top_10_idx = np.argsort(np.abs(shap_val))[-10:][::-1]\n",
    "#     print(f\"\\nTop 10 impacting features for Anomalous Sample #{i+1}:\")\n",
    "#     for rank, idx in enumerate(top_10_idx, start=1):\n",
    "#         print(f\"{rank}. {original_feature_names[idx]} | SHAP value: {shap_val[idx]:.4f}\")\n",
    "# # === Step 6: Visualize SHAP Values ===\n",
    "# shap.summary_plot(shap_values[0], samples_to_explain.cpu().numpy(), feature_names=original_feature_names, max_display=10)\n",
    "# # Save the SHAP values to a CSV file\n",
    "# shap_df = pd.DataFrame(shap_values[0], columns=original_feature_names)\n",
    "# shap_df.to_csv(f'shap_values_srscu{CU}_srsdu{DU}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14237/3398716809.py:238: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  anomaly_indices = torch.nonzero(torch.tensor(is_anomaly['Anomaly Detected'])).squeeze().cpu().numpy()\n"
     ]
    }
   ],
   "source": [
    "# === Step 1: Extract original feature names ===\n",
    "original_feature_names = features.columns.tolist()\n",
    "\n",
    "# Run the RCA function\n",
    "ig_df = RCA(is_anomaly, full_sequences_with_predictions, JVGAN_model, original_feature_names, CU, DU, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "# import os\n",
    "# import pickle\n",
    "\n",
    "# # === Step 8: Load causal graph ===\n",
    "# causal_graph_path = \"causal_graph.gpickle\"\n",
    "# if not os.path.exists(causal_graph_path):\n",
    "#     raise FileNotFoundError(\"Causal graph not found! Please run the Granger causality code first.\")\n",
    "\n",
    "# with open(\"causal_graph.gpickle\", \"rb\") as f:\n",
    "#     G = pickle.load(f)\n",
    "\n",
    "\n",
    "# # === Step 9: Root cause tracing function ===\n",
    "# def find_root_causes(graph, feature):\n",
    "#     \"\"\"\n",
    "#     For a given feature, walk backward in the graph to find root causes\n",
    "#     (nodes with no incoming edges that can reach this node).\n",
    "#     \"\"\"\n",
    "#     root_causes = set()\n",
    "\n",
    "#     def dfs(node, visited):\n",
    "#         if node in visited:\n",
    "#             return\n",
    "#         visited.add(node)\n",
    "#         preds = list(graph.predecessors(node))\n",
    "#         if not preds:\n",
    "#             root_causes.add(node)\n",
    "#         else:\n",
    "#             for pred in preds:\n",
    "#                 dfs(pred, visited)\n",
    "\n",
    "#     dfs(feature, set())\n",
    "#     return list(root_causes)\n",
    "\n",
    "# # === Step 10: Extract top IG features and rank root causes ===\n",
    "# ranked_root_cause_rows = []\n",
    "\n",
    "# for i, row in ig_df.iterrows():\n",
    "#     anomaly_id = f\"anomaly_{i+1:03d}\"\n",
    "#     abs_values = np.abs(row.values)\n",
    "#     top_10_idx = np.argsort(abs_values)[-10:][::-1]\n",
    "#     top_features = [original_feature_names[idx] for idx in top_10_idx]\n",
    "\n",
    "#     root_cause_counter = {}\n",
    "\n",
    "#     for f in top_features:\n",
    "#         if f in G.nodes:\n",
    "#             root_causes = find_root_causes(G, f)\n",
    "#             for root in root_causes:\n",
    "#                 root_cause_counter[root] = root_cause_counter.get(root, 0) + 1\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "#     if not root_cause_counter:\n",
    "#         ranked_root_cause_rows.append({\n",
    "#             \"anomaly_id\": anomaly_id,\n",
    "#             \"root_cause\": \"None\",\n",
    "#             \"rank\": 1,\n",
    "#             \"count\": 0\n",
    "#         })\n",
    "#         continue\n",
    "\n",
    "#     # Rank by frequency (descending)\n",
    "#     sorted_roots = sorted(root_cause_counter.items(), key=lambda x: -x[1])\n",
    "#     for rank, (rc, count) in enumerate(sorted_roots, 1):\n",
    "#         ranked_root_cause_rows.append({\n",
    "#             \"anomaly_id\": anomaly_id,\n",
    "#             \"root_cause\": rc,\n",
    "#             \"rank\": rank,\n",
    "#             \"count\": count\n",
    "#         })\n",
    "\n",
    "# # === Step 11: Save ranked root causes ===\n",
    "# ranked_df = pd.DataFrame(ranked_root_cause_rows)\n",
    "# ranked_df.to_csv(f'ranked_root_causes_srscu{CU}_srsdu{DU}.csv', index=False)\n",
    "# print(f\"\\nSaved ranked root causes to: ranked_root_causes_srscu{CU}_srsdu{DU}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved root causes per anomaly to: root_causes_top10_srscu0_srsdu0.csv\n"
     ]
    }
   ],
   "source": [
    "# Run the root cause analysis\n",
    "anomaly_subgraphs, root_df = get_root_features(ig_df, original_feature_names, full_graph, CU, DU, save_dir=\"anomaly_subgraphs\", save_subgraphs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CU/DU targets to: cu_du_targets_srscu0_srsdu0.csv\n"
     ]
    }
   ],
   "source": [
    "target_df = generate_labels(root_df, CU, DU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of CU/DU targets against original dataset: 0.00%\n",
      "Score = 0.0, Total Anomalies = 87\n"
     ]
    }
   ],
   "source": [
    "accuracy = get_result(target_df, CU, DU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topology is as follows: \n",
      "{'srscu0': ['srsdu0'], 'srscu1': ['srsdu1'], 'srscu2': ['srsdu2'], 'srscu3': ['srsdu3']}\n",
      "\n",
      "Application Features: {'pusch_ta_ns', 'bsr', 'ul_nof_nok', 'pucch_ta_ns', 'dl_bs', 'dl_nof_nok', 'ul_nof_ok', 'ri', 'ul_brate', 'cqi', 'srs_ta_ns', 'pusch_snr_db', 'dl_brate', 'dl_nof_ok', 'ta_ns', 'dl_mcs', 'ul_mcs', 'pucch_snr_db'}\n"
     ]
    }
   ],
   "source": [
    "NoOfCUs = 4\n",
    "NoOfDUs = 4\n",
    "\n",
    "# Creating Topology\n",
    "topology = {}\n",
    "\n",
    "# Form the graph where srscu0 connects to srsdu0, srscu1 to srsdu1, and so on\n",
    "for i in range(min(NoOfCUs, NoOfDUs)):  # Prevent index errors\n",
    "    topology[f\"srscu{i}\"] = [f\"srsdu{i}\"]\n",
    "\n",
    "UEsOfDUs = {}\n",
    "\n",
    "for i in range(NoOfDUs):\n",
    "    UEsOfDUs[f\"srsdu{i}\"] = []\n",
    "\n",
    "\n",
    "# Display the graph\n",
    "print(f'Topology is as follows: \\n{topology}', end=\"\\n\\n\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = pd.read_csv('small_sample.csv')\n",
    "# dataset = dataset[:int(0.01*len(dataset))]\n",
    "\n",
    "dataset.index = dataset['Timestamp']\n",
    "dataset = dataset.drop(columns=['Timestamp'])\n",
    "\n",
    "# for column in dataset.columns:\n",
    "#     if 'PCI' in column:\n",
    "#         print(column)\n",
    "\n",
    "# Dictionary to store for each PCI:\n",
    "# a list of RNTIs and a list of metric types\n",
    "pci_info_map = defaultdict(lambda: {'rntis': set(), 'metrics': set()})\n",
    "\n",
    "\n",
    "# Process each column\n",
    "for column in dataset.columns:\n",
    "    match = re.match(r'PCI-(\\d+)_RNTI-(\\d+)_([a-zA-Z0-9_]+)', column)\n",
    "    if match:\n",
    "        pci = match.group(1)\n",
    "        rnti = match.group(2)\n",
    "        metric = match.group(3)\n",
    "        pci_info_map[pci]['rntis'].add(rnti)\n",
    "        pci_info_map[pci]['metrics'].add(metric)\n",
    "\n",
    "# # Print results\n",
    "# for pci, info in pci_info_map.items():\n",
    "#     print(f\"PCI-{pci}:\")\n",
    "#     print(f\"  RNTIs: {sorted(info['rntis'])}\")\n",
    "#     print(f\"  Metrics: {sorted(info['metrics'])}\\n\\t   Length: {len(info['metrics'])}\")\n",
    "\n",
    "\n",
    "application_features = pci_info_map['1'][ 'metrics']\n",
    "print(f\"Application Features: {application_features}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
