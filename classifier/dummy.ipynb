{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, num_features, num_classes):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Adjust the first layer to match the input dimensions\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + num_classes, 512),  # Adjust this to match input dimensions\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_features),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z, labels):\n",
    "        # Concatenate latent vector and labels\n",
    "        inputs = torch.cat([z, labels], dim=1)\n",
    "        return self.model(inputs)\n",
    "\n",
    "# Define the Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(num_features + num_classes, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        # Concatenate features and labels\n",
    "        inputs = torch.cat([x, labels], dim=1)\n",
    "        return self.model(inputs)\n",
    "\n",
    "\n",
    "GLOBAL_SORTED_FEATURE_NAMES = None\n",
    "GLOBAL_SORTED_TARGET_NAMES = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                host    srscu0    srscu1    srscu2    srscu3    srsdu0  \\\n",
      "2025-04-01  0.496714 -1.415371  0.357787 -0.828995 -1.594428  0.926178   \n",
      "2025-04-02 -0.138264 -0.420645  0.560785 -0.560181 -0.599375  1.909417   \n",
      "2025-04-03  0.647689 -0.342715  1.083051  0.747294  0.005244 -1.398568   \n",
      "2025-04-04  1.523030 -0.802277  1.053802  0.610370  0.046981  0.562969   \n",
      "2025-04-05 -0.234153 -0.161286 -1.377669 -0.020902 -0.450065 -0.650643   \n",
      "\n",
      "              srsdu1    srsdu2    srsdu3  srscu0_stressType  \\\n",
      "2025-04-01  0.756989 -0.522723  0.938284                  1   \n",
      "2025-04-02 -0.922165  1.049009 -0.516045                  3   \n",
      "2025-04-03  0.869606 -0.704344  0.096121                  1   \n",
      "2025-04-04  1.355638 -1.408461 -0.462275                  4   \n",
      "2025-04-05  0.413435 -1.556629 -0.434496                  4   \n",
      "\n",
      "            srscu1_stressType  srscu2_stressType  srscu3_stressType  \\\n",
      "2025-04-01                  4                  4                  4   \n",
      "2025-04-02                  2                  2                  3   \n",
      "2025-04-03                  1                  3                  4   \n",
      "2025-04-04                  3                  1                  0   \n",
      "2025-04-05                  2                  1                  0   \n",
      "\n",
      "            srsdu0_stressType  srsdu1_stressType  srsdu2_stressType  \\\n",
      "2025-04-01                  0                  1                  3   \n",
      "2025-04-02                  4                  2                  3   \n",
      "2025-04-03                  0                  4                  0   \n",
      "2025-04-04                  3                  3                  0   \n",
      "2025-04-05                  1                  1                  4   \n",
      "\n",
      "            srsdu3_stressType  \n",
      "2025-04-01                  1  \n",
      "2025-04-02                  4  \n",
      "2025-04-03                  0  \n",
      "2025-04-04                  2  \n",
      "2025-04-05                  0  \n"
     ]
    }
   ],
   "source": [
    "# # Load dataset\n",
    "# dataset = pd.read_csv('dummy.csv')\n",
    "# dataset = dataset[:int(0.01*len(dataset))]\n",
    "\n",
    "# dataset.index = dataset['Timestamp']\n",
    "# dataset = dataset.drop(columns=['Timestamp'])\n",
    "# print(dataset.head())\n",
    "\n",
    "\n",
    "# # dataset.shape\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create dummy dataset with 4 features and 2 targets\n",
    "num_samples = 100\n",
    "data = {\n",
    "    'host': np.random.randn(num_samples),\n",
    "    'srscu0': np.random.randn(num_samples),\n",
    "    'srscu1': np.random.randn(num_samples),\n",
    "    'srscu2': np.random.randn(num_samples),\n",
    "    'srscu3': np.random.randn(num_samples),\n",
    "    'srsdu0': np.random.randn(num_samples),\n",
    "    'srsdu1': np.random.randn(num_samples),\n",
    "    'srsdu2': np.random.randn(num_samples),\n",
    "    'srsdu3': np.random.randn(num_samples),\n",
    "    'srscu0_stressType': np.random.randint(0, 5, num_samples),  # Binary target\n",
    "    'srscu1_stressType': np.random.randint(0, 5, num_samples),  # Binary target\n",
    "    'srscu2_stressType': np.random.randint(0, 5, num_samples),  # Binary target\n",
    "    'srscu3_stressType': np.random.randint(0, 5, num_samples),  # Binary target\n",
    "    'srsdu0_stressType': np.random.randint(0, 5, num_samples),  # Binary target\n",
    "    'srsdu1_stressType': np.random.randint(0, 5, num_samples),  # Binary target\n",
    "    'srsdu2_stressType': np.random.randint(0, 5, num_samples),  # Binary target\n",
    "    'srsdu3_stressType': np.random.randint(0, 5, num_samples),  # Binary target\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# Create a DataFrame\n",
    "dataset = pd.DataFrame(data)\n",
    "\n",
    "# Create a timestamp index (assuming the start date is '2025-04-01')\n",
    "dataset.index = pd.date_range(start='2025-04-01', periods=num_samples, freq='D')\n",
    "\n",
    "\n",
    "# Show the first few rows of the dataset\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, sample in dataset.iterrows():\n",
    "    if('key' in dataset.columns):\n",
    "        for itr in ['key']:  # Loop through the list of columns you want to modify\n",
    "            val = str(sample[itr])\n",
    "            \n",
    "            # Remove the substring 'bsr' from the value\n",
    "            val = val.replace('bsr', '')\n",
    "            \n",
    "            # Convert the modified value to an integer, if possible\n",
    "            try:\n",
    "                dataset.at[idx, itr] = int(val)\n",
    "            except ValueError:\n",
    "                # Handle the case where the value cannot be converted to an integer\n",
    "                dataset.at[idx, itr] = 0  # Or set it to some default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'srscu0': ['srsdu0'], 'srscu1': ['srsdu1'], 'srscu2': ['srsdu2'], 'srscu3': ['srsdu3']}\n"
     ]
    }
   ],
   "source": [
    "NoOfCUs = 4\n",
    "NoOfDUs = 4\n",
    "\n",
    "# Creating Topology\n",
    "topology = {}\n",
    "\n",
    "# Form the graph where srscu0 connects to srsdu0, srscu1 to srsdu1, and so on\n",
    "for i in range(min(NoOfCUs, NoOfDUs)):  # Prevent index errors\n",
    "    topology[f\"srscu{i}\"] = [f\"srsdu{i}\"]\n",
    "\n",
    "# Display the graph\n",
    "print(topology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_features = dataset.columns.tolist()\n",
    "container_specific_features = {}\n",
    "\n",
    "# Loop through and remove columns containing specific substrings\n",
    "for i in range(NoOfDUs+1):\n",
    "    common_features = [col for col in common_features if f\"srscu{i}\" not in col and f\"srsdu{i}\" not in col]\n",
    "\n",
    "# Store container-specific dataframes instead of lists\n",
    "for i in range(NoOfCUs+1):\n",
    "    container_specific_features[f'srscu{i}'] = dataset[[col for col in dataset.columns.tolist() if f\"srscu{i}\" in col]]\n",
    "\n",
    "for i in range(NoOfDUs+1):\n",
    "    container_specific_features[f'srsdu{i}'] = dataset[[col for col in dataset.columns.tolist() if f\"srsdu{i}\" in col]]\n",
    "\n",
    "# # Print the remaining features\n",
    "# print(len(common_features), common_features)\n",
    "\n",
    "# print(\"Before:\")\n",
    "\n",
    "# # Print container-specific features (as dataframes now)\n",
    "# for i in range(NoOfCUs):\n",
    "#     print(f\"srscu{i}:\")\n",
    "#     print(container_specific_features[f'srscu{i}'].shape)\n",
    "#     print(container_specific_features[f'srscu{i}'].head())\n",
    "\n",
    "# for i in range(NoOfDUs):\n",
    "#     print(f\"srsdu{i}:\")\n",
    "#     print(container_specific_features[f'srsdu{i}'].shape)\n",
    "#     print(container_specific_features[f'srsdu{i}'].head())\n",
    "\n",
    "# Filter out columns containing 'stepStress' from the container-specific dataframes\n",
    "for i in range(NoOfCUs):\n",
    "    container_specific_features[f'srscu{i}'] = container_specific_features[f'srscu{i}'].loc[:, ~container_specific_features[f'srscu{i}'].columns.str.contains('stepStress')]\n",
    "\n",
    "for i in range(NoOfDUs):\n",
    "    container_specific_features[f'srsdu{i}'] = container_specific_features[f'srsdu{i}'].loc[:, ~container_specific_features[f'srsdu{i}'].columns.str.contains('stepStress')]\n",
    "\n",
    "# print(\"After:\")\n",
    "\n",
    "# # Print container-specific features (after filtering)\n",
    "# for i in range(NoOfCUs):\n",
    "#     print(f\"srscu{i}:\")\n",
    "#     print(container_specific_features[f'srscu{i}'].shape)\n",
    "#     print(container_specific_features[f'srscu{i}'].head())\n",
    "\n",
    "# for i in range(NoOfDUs):\n",
    "#     print(f\"srsdu{i}:\")\n",
    "#     print(container_specific_features[f'srsdu{i}'].shape)\n",
    "#     print(container_specific_features[f'srsdu{i}'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('srscu0', 'srsdu0'): {'features': ['srscu0', 'srsdu0', 'host'], 'targets': ['srsdu0_stressType', 'srscu0_stressType']}, ('srscu1', 'srsdu1'): {'features': ['srscu1', 'host', 'srsdu1'], 'targets': ['srscu1_stressType', 'srsdu1_stressType']}, ('srscu2', 'srsdu2'): {'features': ['srsdu2', 'srscu2', 'host'], 'targets': ['srsdu2_stressType', 'srscu2_stressType']}, ('srscu3', 'srsdu3'): {'features': ['host', 'srscu3', 'srsdu3'], 'targets': ['srsdu3_stressType', 'srscu3_stressType']}}\n",
      "Host and CU: srscu0, DU: srsdu0 - Combined Features:\n",
      "Number of Features: 3\n",
      "Number of Targets: 2\n",
      "['srscu0', 'srsdu0', 'host']\n",
      "----------------------------------------\n",
      "Host and CU: srscu1, DU: srsdu1 - Combined Features:\n",
      "Number of Features: 3\n",
      "Number of Targets: 2\n",
      "['srscu1', 'host', 'srsdu1']\n",
      "----------------------------------------\n",
      "Host and CU: srscu2, DU: srsdu2 - Combined Features:\n",
      "Number of Features: 3\n",
      "Number of Targets: 2\n",
      "['srsdu2', 'srscu2', 'host']\n",
      "----------------------------------------\n",
      "Host and CU: srscu3, DU: srsdu3 - Combined Features:\n",
      "Number of Features: 3\n",
      "Number of Targets: 2\n",
      "['host', 'srscu3', 'srsdu3']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the topology and combine features\n",
    "combined_samples = {}\n",
    "\n",
    "for CU in topology.keys():\n",
    "    # The CU container-specific features\n",
    "    CU_features = container_specific_features[CU]\n",
    "    \n",
    "    # The connected DUs (from topology)\n",
    "    connected_DUs = topology[CU]\n",
    "    \n",
    "    # Add CU-specific features to the combined list\n",
    "    CU_features_list = CU_features.columns.tolist()\n",
    "    \n",
    "    # Extract the CU stress type column (if exists)\n",
    "    CU_stressType = f'{CU}_stressType' if f'{CU}_stressType' in CU_features.columns else None\n",
    "    \n",
    "    # Add DU-specific features to the combined list for each connected DU\n",
    "    for DU in connected_DUs:\n",
    "        # Ensure DU exists in container_specific_features\n",
    "        if DU in container_specific_features:\n",
    "            DU_features = container_specific_features[DU]\n",
    "            DU_features_list = DU_features.columns.tolist()\n",
    "\n",
    "            # Combine CU and DU features (remove the stress type columns from features)\n",
    "            combined_features = common_features.copy()  # Start with the common features\n",
    "            \n",
    "            # Modify these lines:\n",
    "            combined_features.extend(CU_features_list)  # Keep all CU features\n",
    "            combined_features.extend(DU_features_list)  # Keep all DU features\n",
    "\n",
    "            \n",
    "\n",
    "            # Extract targets and remove them from features\n",
    "            targets = [col for col in combined_features if '_stressType' in col]\n",
    "\n",
    "            # To keep stressType columns temporarily:\n",
    "            combined_samples[(CU, DU)] = {\n",
    "                'features': list(set(combined_features) - set(targets)),  # Include targets in features temporarily\n",
    "                'targets': list(set(targets))\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            print(f\"Error: {DU} not found in container_specific_features!\")\n",
    "            continue  # Skip this DU if not found in container_specific_features\n",
    "    \n",
    "print(combined_samples)\n",
    "# Print the results for each CU-DU pair and its combined features\n",
    "for (CU, DU), sample in combined_samples.items():\n",
    "    print(f\"Host and CU: {CU}, DU: {DU} - Combined Features:\")\n",
    "    print(f\"Number of Features: {len(sample['features'])}\")\n",
    "    print(f\"Number of Targets: {len(sample['targets'])}\")\n",
    "    print(sample['features'][:10])  # Print first 10 features as a preview\n",
    "    print(\"----\" * 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Before Ordering:\n",
      "['srscu0', 'srsdu0', 'host']\n",
      "['srsdu0_stressType', 'srscu0_stressType']\n",
      "\n",
      "\n",
      "After Ordering:\n",
      "['host', 'srscu0', 'srsdu0']\n",
      "['srscu0_stressType', 'srsdu0_stressType']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_sorted_feature_and_targets_names(combined_features, combined_targets):\n",
    "    # Sort the features and targets\n",
    "    return sorted(combined_features), sorted(combined_targets)  \n",
    "\n",
    "\n",
    "CU=\"srscu0\"\n",
    "DU=\"srsdu0\"\n",
    "# Extract the combined features for the specific CU-DU pair\n",
    "combined_features = combined_samples[(CU, DU)]['features']\n",
    "combined_targets = combined_samples[(CU, DU)]['targets']\n",
    "\n",
    "GLOBAL_SORTED_FEATURE_NAMES, GLOBAL_SORTED_TARGET_NAMES = get_sorted_feature_and_targets_names(combined_features, combined_targets)\n",
    "\n",
    "print(f\"\\n\\nBefore Ordering:\\n{combined_features}\\n{combined_targets}\")\n",
    "print(f\"\\n\\nAfter Ordering:\\n{GLOBAL_SORTED_FEATURE_NAMES}\\n{GLOBAL_SORTED_TARGET_NAMES}\")\n",
    "\n",
    "# print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on data of Host, srscu0 and srsdu0\n",
      "['host', 'srscu0', 'srsdu0']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate object of type '<class 'numpy.ndarray'>'; only Series and DataFrame objs are valid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 82\u001b[0m\n\u001b[1;32m     79\u001b[0m train_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.95\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(X))\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Concatenate X and Y into raw_data (pd.concat is used to join the features and targets)\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m raw_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m raw_data_training \u001b[38;5;241m=\u001b[39m raw_data[:train_idx]\n\u001b[1;32m     86\u001b[0m raw_data_testing \u001b[38;5;241m=\u001b[39m raw_data[train_idx:]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/reshape/concat.py:448\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clean_keys_and_objs(objs, keys)\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[0;32m--> 448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_ndims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m sample, objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sample_object(objs, ndims, keys, names, levels)\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# Standardize axis parameter to int\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/reshape/concat.py:489\u001b[0m, in \u001b[0;36m_Concatenator._get_ndims\u001b[0;34m(self, objs)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (ABCSeries, ABCDataFrame)):\n\u001b[1;32m    485\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    486\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot concatenate object of type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly Series and DataFrame objs are valid\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    488\u001b[0m         )\n\u001b[0;32m--> 489\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m    491\u001b[0m     ndims\u001b[38;5;241m.\u001b[39madd(obj\u001b[38;5;241m.\u001b[39mndim)\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ndims\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot concatenate object of type '<class 'numpy.ndarray'>'; only Series and DataFrame objs are valid"
     ]
    }
   ],
   "source": [
    "AUC_ROC = []\n",
    "PRECISION = []\n",
    "RECALL = []\n",
    "F1_SCORE = []\n",
    "\n",
    "for (CU, DU), sample in combined_samples.items():\n",
    "    print(f\"Training on data of Host, {CU} and {DU}\")\n",
    "\n",
    "\n",
    "    for i in range(len(GLOBAL_SORTED_FEATURE_NAMES)):\n",
    "        GLOBAL_SORTED_FEATURE_NAMES[i] = GLOBAL_SORTED_FEATURE_NAMES[i].replace('srscu0', CU)\n",
    "        GLOBAL_SORTED_FEATURE_NAMES[i] = GLOBAL_SORTED_FEATURE_NAMES[i].replace('srsdu0', DU)\n",
    "    print(GLOBAL_SORTED_FEATURE_NAMES)\n",
    "    # Load data with duplicate handling\n",
    "    raw_data = dataset[GLOBAL_SORTED_FEATURE_NAMES + sample['targets']].copy()\n",
    "    raw_data = raw_data.loc[:, ~raw_data.columns.duplicated()]  # KEY FIX\n",
    "\n",
    "    sorted_feature_names = sorted(raw_data.columns.tolist())\n",
    "    raw_data = raw_data[sorted_feature_names]\n",
    "\n",
    "    # Filter using targets\n",
    "    for target_col in sample['targets']:\n",
    "        raw_data = raw_data[raw_data[target_col].isin([0, 1, 2, 3])]\n",
    "\n",
    "\n",
    "    # **Data Preprocessing**\n",
    "    # Handle missing values\n",
    "    raw_data = raw_data.apply(lambda x: x.fillna(0) if x.isna().all() else x)\n",
    "\n",
    "\n",
    "    threshold = 0.6 * len(raw_data)\n",
    "    raw_data = raw_data.loc[:, ~raw_data.columns.duplicated()]  # Remove duplicates\n",
    "\n",
    "    for col in raw_data.columns:\n",
    "        nan_count = raw_data[col].isna().sum()\n",
    "        if int(nan_count) > threshold:  # Explicit scalar conversion\n",
    "            mode_value = raw_data[col].mode().iloc[0] if not raw_data[col].mode().empty else 0\n",
    "            raw_data[col].fillna(mode_value, inplace=True)\n",
    "\n",
    "    numeric_cols = raw_data.select_dtypes(include=[np.number]).columns\n",
    "    raw_data[numeric_cols] = raw_data[numeric_cols].fillna(raw_data[numeric_cols].mean())\n",
    "\n",
    "\n",
    "    # **Convert target columns to binary (0 or 1)**\n",
    "    # Instead of using a loop over rows, we can do it in a vectorized way\n",
    "    for target_col in sample['targets']:\n",
    "        raw_data[target_col] = raw_data[target_col].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "\n",
    "    # Create unified target column\n",
    "    raw_data['target'] = 0\n",
    "    for idx in raw_data.index:\n",
    "        if any(raw_data.loc[idx, sample['targets']] == 1):\n",
    "            raw_data.at[idx, 'target'] = 1\n",
    "\n",
    "    \n",
    "    X = raw_data.drop(columns=sample['targets']+['target'])\n",
    "    Y = raw_data['target']\n",
    "\n",
    "    # # To avoid division by zero:\n",
    "    # X = (X - X.mean()) / (X.std() + 1e-8)\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    import joblib\n",
    "\n",
    "    # Initialize the scaler (you can also use StandardScaler instead)\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Fit and transform the training features\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Save the scaler for future use\n",
    "    joblib.dump(scaler, 'scaler_model.pkl')\n",
    "\n",
    "    # Replace the original features with the scaled ones\n",
    "    X = X_scaled\n",
    "\n",
    "\n",
    "    train_idx = int(0.95 * len(X))\n",
    "\n",
    "    # Concatenate X and Y into raw_data (pd.concat is used to join the features and targets)\n",
    "    raw_data = pd.concat([X, Y], axis=1)\n",
    "\n",
    "\n",
    "    raw_data_training = raw_data[:train_idx]\n",
    "    raw_data_testing = raw_data[train_idx:]\n",
    "\n",
    "\n",
    "    # Convert all columns to float16\n",
    "    raw_data_training = raw_data_training.astype(np.float16)\n",
    "    raw_data_testing = raw_data_testing.astype(np.float16)\n",
    "\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    features = torch.FloatTensor(raw_data_training.drop(columns=['target']).values).to(device)\n",
    "    labels = torch.LongTensor(raw_data_training['target'].values).to(device)\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    trainingDataset = TensorDataset(features, labels)\n",
    "    trainingDataloader = DataLoader(trainingDataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "    # Hyperparameters\n",
    "    latent_dim = raw_data_training.shape[1] - 1\n",
    "    # latent_dim = 100\n",
    "    num_features = raw_data_training.shape[1] - 1\n",
    "    num_classes = 2\n",
    "    lr = 0.0002\n",
    "    num_epochs = 100\n",
    "\n",
    "    print(f'\\n\\nJVGAN parameters: Latent Dimension = {latent_dim}, num_features = {num_features}, lr={lr}', end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "    # # Check if the models exist\n",
    "    # generator_model_path = 'generator.pth'\n",
    "    # discriminator_model_path = 'discriminator.pth'\n",
    "\n",
    "    # # Initialize the models if they don't exist, otherwise load the saved models\n",
    "    # if os.path.exists(generator_model_path) and os.path.exists(discriminator_model_path):\n",
    "    #     # Load pre-trained models\n",
    "    #     generator = Generator(latent_dim, num_features, num_classes).to(device)\n",
    "    #     discriminator = Discriminator(num_features, num_classes).to(device)\n",
    "        \n",
    "    #     # Load the state dicts for both generator and discriminator\n",
    "    #     generator.load_state_dict(torch.load(generator_model_path))\n",
    "    #     discriminator.load_state_dict(torch.load(discriminator_model_path))\n",
    "        \n",
    "    #     print(\"Loaded pre-trained generator and discriminator models.\")\n",
    "    # else:\n",
    "    #     # Initialize models if they don't exist\n",
    "    #     generator = Generator(latent_dim, num_features, num_classes).to(device)\n",
    "    #     discriminator = Discriminator(num_features, num_classes).to(device)\n",
    "        \n",
    "    #     print(\"Initialized new generator and discriminator models.\")\n",
    "\n",
    "    # Initialize models\n",
    "    generator = Generator(latent_dim, num_features, num_classes).to(device)\n",
    "    discriminator = Discriminator(num_features, num_classes).to(device)\n",
    "\n",
    "    # Optimizers\n",
    "    g_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (real_data, real_labels) in enumerate(trainingDataloader):\n",
    "            batch_size = real_data.size(0)\n",
    "            \n",
    "            # Train Discriminator\n",
    "            d_optimizer.zero_grad()\n",
    "            \n",
    "            # Real data\n",
    "            real_labels_onehot = nn.functional.one_hot(real_labels, num_classes).float().to(device)\n",
    "            real_validity = discriminator(real_data, real_labels_onehot)\n",
    "            d_real_loss = criterion(real_validity, torch.ones_like(real_validity).to(device))\n",
    "            \n",
    "            # Fake data\n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            fake_labels = torch.randint(0, num_classes, (batch_size,)).to(device)\n",
    "            fake_labels_onehot = nn.functional.one_hot(fake_labels, num_classes).float().to(device)\n",
    "            fake_data = generator(z, fake_labels_onehot)\n",
    "            fake_validity = discriminator(fake_data.detach(), fake_labels_onehot)\n",
    "            d_fake_loss = criterion(fake_validity, torch.zeros_like(fake_validity).to(device))\n",
    "            \n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "            \n",
    "            # Train Generator\n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            fake_labels = torch.randint(0, num_classes, (batch_size,)).to(device)\n",
    "            fake_labels_onehot = nn.functional.one_hot(fake_labels, num_classes).float().to(device)\n",
    "            fake_data = generator(z, fake_labels_onehot)\n",
    "            fake_validity = discriminator(fake_data, fake_labels_onehot)\n",
    "            g_loss = criterion(fake_validity, torch.ones_like(fake_validity).to(device))\n",
    "            \n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch [{epoch}/{num_epochs}] Batch [{i}/{len(trainingDataloader)}] \"\n",
    "                    f\"D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "    # Testing\n",
    "    # Convert to PyTorch tensors\n",
    "    test_z = torch.FloatTensor(raw_data_testing.values[:, :-1]).to(device)\n",
    "    test_labels = torch.LongTensor(raw_data_testing['target'].values).to(device)\n",
    "    test_labels_onehot = nn.functional.one_hot(test_labels, num_classes).float().to(device)\n",
    "    test_data = generator(test_z, test_labels_onehot)\n",
    "\n",
    "    torch.save(generator.state_dict(), 'generator.pth')\n",
    "    torch.save(discriminator.state_dict(), 'discriminator.pth')\n",
    "\n",
    "    # Evaluate the generated data for anomaly detection\n",
    "    def evaluate_anomaly_detection(generator, real_data, labels, num_samples=1000):\n",
    "        # Generate synthetic data\n",
    "        z = torch.randn(num_samples, latent_dim).to(device)\n",
    "        synthetic_labels = torch.randint(0, num_classes, (num_samples,)).to(device)\n",
    "        synthetic_labels_onehot = nn.functional.one_hot(synthetic_labels, num_classes).float().to(device)\n",
    "        synthetic_data = generator(z, synthetic_labels_onehot)\n",
    "        \n",
    "        # Combine real and synthetic data\n",
    "        all_data = torch.cat([real_data, synthetic_data], dim=0)\n",
    "        all_labels = torch.cat([labels, synthetic_labels], dim=0)\n",
    "        \n",
    "        # Use discriminator to classify real vs synthetic\n",
    "        with torch.no_grad():\n",
    "            predictions = discriminator(all_data, nn.functional.one_hot(all_labels, num_classes).float().to(device))\n",
    "        \n",
    "        # Convert predictions to binary (0 for synthetic, 1 for real)\n",
    "        predictions = (predictions > 0.5).float()\n",
    "        \n",
    "        # Calculate anomaly detection metrics\n",
    "        real_labels = torch.ones(real_data.size(0)).to(device)\n",
    "        synthetic_labels = torch.zeros(synthetic_data.size(0)).to(device)\n",
    "        true_labels = torch.cat([real_labels, synthetic_labels], dim=0)\n",
    "        \n",
    "        auc_roc = roc_auc_score(true_labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(true_labels.cpu().numpy(), predictions.cpu().numpy(), average='binary')\n",
    "        \n",
    "        return auc_roc, precision, recall, f1\n",
    "\n",
    "\n",
    "    # Evaluate anomaly detection performance\n",
    "    auc_roc, precision, recall, f1 = evaluate_anomaly_detection(generator, features, labels)\n",
    "    \n",
    "    print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "\n",
    "    AUC_ROC.append(auc_roc)\n",
    "    PRECISION.append(precision)\n",
    "    RECALL.append(recall)\n",
    "    F1_SCORE.append(f1)\n",
    "\n",
    "print(f\"AUC-ROC: {AUC_ROC}\")\n",
    "print(f\"Precision: {PRECISION}\")\n",
    "print(f\"Recall: {RECALL}\")\n",
    "print(f\"F1-Score: {F1_SCORE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
