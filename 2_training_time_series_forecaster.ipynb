{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser(description=\"Run script with optional CU and DU values.\")\n",
    "# parser.add_argument(\"--cu\", type=int, default=0, help=\"CU value (default: 0)\")\n",
    "# parser.add_argument(\"--du\", type=int, default=0, help=\"DU value (default: 0)\")\n",
    "# parser.add_argument(\"--lb\", type=int, default=60, help=\"lookback value (default: 60)\")\n",
    "# parser.add_argument(\"--lf\", type=int, default=1, help=\"lookforward value (default: 5)\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# CU, DU = args.cu, args.du\n",
    "# LOOK_BACK, LOOK_FORWARD = args.lb, args.lf\n",
    "\n",
    "CU = 0\n",
    "DU = 0\n",
    "\n",
    "LOOK_BACK, LOOK_FORWARD = 60, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = pd.read_csv(f'dataset_srscu{CU}_srsdu{DU}_pca.csv')\n",
    "# dataset = dataset[:int(0.01*len(dataset))]\n",
    "\n",
    "dataset.index = dataset['Timestamp']\n",
    "dataset = dataset.drop(columns=['Timestamp'])\n",
    "\n",
    "# dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(df, look_back, look_forward, CU, DU):\n",
    "    \"\"\"\n",
    "    Create LSTM-ready sequences from a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "        look_back (int): Number of timesteps to look back.\n",
    "        look_forward (int): Number of timesteps to predict forward.\n",
    "\n",
    "    Returns:\n",
    "        X (np.ndarray): Shape (samples, look_back, features)\n",
    "        y_df (pd.DataFrame): Shape (samples, features), from last step of forecast horizon\n",
    "        column_names (list): List of column names used\n",
    "    \"\"\"\n",
    "    print(df.columns)\n",
    "    data = df.drop(columns=[f\"srscu{CU}_stepStress\", f\"srscu{CU}_stressType\", f\"srsdu{DU}_stepStress\", f\"srsdu{DU}_stressType\"]).values\n",
    "    targets = df[[f\"srscu{CU}_stepStress\", f\"srscu{CU}_stressType\", f\"srsdu{DU}_stepStress\", f\"srsdu{DU}_stressType\"]].values\n",
    "\n",
    "    pca_features_input, pca_features_output, input_targets, output_targets  = [], [], [], []\n",
    "\n",
    "    for i in range(len(data) - look_back - look_forward + 1):\n",
    "        pca_features_input.append(data[i:(i + look_back)])\n",
    "        pca_features_output.append(data[i + look_back + look_forward - 1])  # last step of forecast\n",
    "\n",
    "        input_targets.append(targets[i:(i + look_back)])\n",
    "        output_targets.append(targets[i + look_back + look_forward - 1])  # last step of forecast\n",
    "\n",
    "    return pca_features_input, pca_features_output, input_targets, output_targets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# **Define LSTM Model**\n",
    "class LSTMForecaster(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTMForecaster, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
    "        c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
    "        out, _ = self.lstm(x, (h_0, c_0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Reduced-1', 'Reduced-2', 'Reduced-3', 'Reduced-4', 'Reduced-5',\n",
      "       'Reduced-6', 'Reduced-7', 'Reduced-8', 'Reduced-9', 'srscu0_stepStress',\n",
      "       'srscu0_stressType', 'srsdu0_stepStress', 'srsdu0_stressType'],\n",
      "      dtype='object')\n",
      "Input shape: (1439, 60, 9)\n",
      "Output shape: (1439, 9)\n"
     ]
    }
   ],
   "source": [
    "pca_features_input , pca_features_output, input_targets, output_targets = create_sequences(dataset, LOOK_BACK, LOOK_FORWARD, CU, DU)\n",
    "\n",
    "print(f\"Input shape: {np.array(pca_features_input).shape}\")\n",
    "print(f\"Output shape: {np.array(pca_features_output).shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 1 ---\n",
      "\n",
      "Training with optimizer: RMSprop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.2395, Val Loss = 0.2016\n",
      "Epoch 2: Train Loss = 0.2161, Val Loss = 0.1929\n",
      "Epoch 3: Train Loss = 0.2106, Val Loss = 0.1889\n",
      "Epoch 4: Train Loss = 0.2063, Val Loss = 0.1860\n",
      "Epoch 5: Train Loss = 0.2025, Val Loss = 0.1839\n",
      "Best Val Loss for RMSprop in Fold 1: 0.1839 at epoch 5\n",
      "\n",
      "--- Fold 2 ---\n",
      "\n",
      "Training with optimizer: RMSprop\n",
      "Epoch 1: Train Loss = 0.2319, Val Loss = 0.2417\n",
      "Epoch 2: Train Loss = 0.2110, Val Loss = 0.2208\n",
      "Epoch 3: Train Loss = 0.2024, Val Loss = 0.2156\n",
      "Epoch 4: Train Loss = 0.1985, Val Loss = 0.2123\n",
      "Epoch 5: Train Loss = 0.1940, Val Loss = 0.2074\n",
      "Best Val Loss for RMSprop in Fold 2: 0.2074 at epoch 5\n",
      "\n",
      "--- Fold 3 ---\n",
      "\n",
      "Training with optimizer: RMSprop\n",
      "Epoch 1: Train Loss = 0.2370, Val Loss = 0.2206\n",
      "Epoch 2: Train Loss = 0.2137, Val Loss = 0.2084\n",
      "Epoch 3: Train Loss = 0.2066, Val Loss = 0.2051\n",
      "Epoch 4: Train Loss = 0.2031, Val Loss = 0.2001\n",
      "Epoch 5: Train Loss = 0.1991, Val Loss = 0.1990\n",
      "Best Val Loss for RMSprop in Fold 3: 0.1990 at epoch 5\n",
      "\n",
      "--- Fold 4 ---\n",
      "\n",
      "Training with optimizer: RMSprop\n",
      "Epoch 1: Train Loss = 0.2359, Val Loss = 0.2207\n",
      "Epoch 2: Train Loss = 0.2126, Val Loss = 0.2149\n",
      "Epoch 3: Train Loss = 0.2054, Val Loss = 0.2104\n",
      "Epoch 4: Train Loss = 0.1999, Val Loss = 0.2071\n",
      "Epoch 5: Train Loss = 0.1940, Val Loss = 0.2064\n",
      "Best Val Loss for RMSprop in Fold 4: 0.2064 at epoch 5\n",
      "\n",
      "--- Fold 5 ---\n",
      "\n",
      "Training with optimizer: RMSprop\n",
      "Epoch 1: Train Loss = 0.2358, Val Loss = 0.2249\n",
      "Epoch 2: Train Loss = 0.2112, Val Loss = 0.2161\n",
      "Epoch 3: Train Loss = 0.2038, Val Loss = 0.2106\n",
      "Epoch 4: Train Loss = 0.2002, Val Loss = 0.2080\n",
      "Epoch 5: Train Loss = 0.1956, Val Loss = 0.2044\n",
      "Best Val Loss for RMSprop in Fold 5: 0.2044 at epoch 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "from torch.utils.data import Subset, DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Convert to tensors for PyTorch\n",
    "LSTM_input_tensor = torch.tensor(np.array(pca_features_input), dtype=torch.float32)  # (num_samples, 60, M)\n",
    "LSTM_output_tensor = torch.tensor(np.array(pca_features_output), dtype=torch.float32)  # (num_samples, M)\n",
    "\n",
    "save_dir=\"lstm_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "M = LSTM_input_tensor.shape[2]  # number of features per timestep, here it's 9\n",
    "\n",
    "k_folds = 5\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "dataset = TensorDataset(LSTM_input_tensor, LSTM_output_tensor)\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "optimizers_to_try = {\n",
    "    \"Adam\": lambda params: torch.optim.Adam(params, lr=learning_rate),\n",
    "    \"SGD\": lambda params: torch.optim.SGD(params, lr=learning_rate, momentum=0.9),\n",
    "    \"RMSprop\": lambda params: torch.optim.RMSprop(params, lr=learning_rate)\n",
    "}\n",
    "\n",
    "optimizers_to_try = {\n",
    "    \"RMSprop\": lambda params: torch.optim.RMSprop(params, lr=learning_rate)\n",
    "}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f\"\\n--- Fold {fold+1} ---\")\n",
    "\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for opt_name, opt_func in optimizers_to_try.items():\n",
    "        print(f\"\\nTraining with optimizer: {opt_name}\")\n",
    "\n",
    "\n",
    "        model = LSTMForecaster(input_dim=M, hidden_dim=64, num_layers=2, output_dim=M).to(device)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        optimizer = opt_func(model.parameters())\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True, min_lr=1e-6)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        best_epoch = -1\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(batch_x)\n",
    "                loss = criterion(output, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "            avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for val_x, val_y in val_loader:\n",
    "                    val_x, val_y = val_x.to(device), val_y.to(device)\n",
    "                    val_output = model(val_x)\n",
    "                    loss_val = criterion(val_output, val_y)\n",
    "                    val_loss += loss_val.item() * val_x.size(0)\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
    "\n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "            model_path = os.path.join(save_dir, f\"lstm_fold{fold+1}_{opt_name}.pt\")\n",
    "            # Save best model for this optimizer/fold\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_epoch = epoch + 1\n",
    "                torch.save(model, model_path)\n",
    "\n",
    "        print(f\"Best Val Loss for {opt_name} in Fold {fold+1}: {best_val_loss:.4f} at epoch {best_epoch}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
