{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c30602fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96fdaeb",
   "metadata": {},
   "source": [
    "# Topology Defination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6b5ac6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topology is as follows: \n",
      "{'srscu0': ['srsdu0'], 'srscu1': ['srsdu1'], 'srscu2': ['srsdu2'], 'srscu3': ['srsdu3']}\n",
      "\n",
      "Application Features: {'dl_nof_ok', 'ri', 'ul_nof_ok', 'dl_nof_nok', 'srs_ta_ns', 'ul_mcs', 'pucch_ta_ns', 'pucch_snr_db', 'ul_brate', 'dl_mcs', 'dl_brate', 'pusch_snr_db', 'cqi', 'bsr', 'pusch_ta_ns', 'ul_nof_nok', 'dl_bs', 'ta_ns'}\n"
     ]
    }
   ],
   "source": [
    "NoOfCUs = 4\n",
    "NoOfDUs = 4\n",
    "\n",
    "# Creating Topology\n",
    "topology = {}\n",
    "\n",
    "# Form the graph where srscu0 connects to srsdu0, srscu1 to srsdu1, and so on\n",
    "for i in range(min(NoOfCUs, NoOfDUs)):  # Prevent index errors\n",
    "    topology[f\"srscu{i}\"] = [f\"srsdu{i}\"]\n",
    "\n",
    "UEsOfDUs = {}\n",
    "\n",
    "for i in range(NoOfDUs):\n",
    "    UEsOfDUs[f\"srsdu{i}\"] = []\n",
    "\n",
    "\n",
    "# Display the graph\n",
    "print(f'Topology is as follows: \\n{topology}', end=\"\\n\\n\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = pd.read_csv('small_sample.csv')\n",
    "# dataset = dataset[:int(0.01*len(dataset))]\n",
    "\n",
    "dataset.index = dataset['Timestamp']\n",
    "dataset = dataset.drop(columns=['Timestamp'])\n",
    "\n",
    "# for column in dataset.columns:\n",
    "#     if 'PCI' in column:\n",
    "#         print(column)\n",
    "\n",
    "# Dictionary to store for each PCI:\n",
    "# a list of RNTIs and a list of metric types\n",
    "pci_info_map = defaultdict(lambda: {'rntis': set(), 'metrics': set()})\n",
    "\n",
    "\n",
    "# Process each column\n",
    "for column in dataset.columns:\n",
    "    match = re.match(r'PCI-(\\d+)_RNTI-(\\d+)_([a-zA-Z0-9_]+)', column)\n",
    "    if match:\n",
    "        pci = match.group(1)\n",
    "        rnti = match.group(2)\n",
    "        metric = match.group(3)\n",
    "        pci_info_map[pci]['rntis'].add(rnti)\n",
    "        pci_info_map[pci]['metrics'].add(metric)\n",
    "\n",
    "# # Print results\n",
    "# for pci, info in pci_info_map.items():\n",
    "#     print(f\"PCI-{pci}:\")\n",
    "#     print(f\"  RNTIs: {sorted(info['rntis'])}\")\n",
    "#     print(f\"  Metrics: {sorted(info['metrics'])}\\n\\t   Length: {len(info['metrics'])}\")\n",
    "\n",
    "\n",
    "application_features = pci_info_map['1'][ 'metrics']\n",
    "print(f\"Application Features: {application_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ebef35dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = [ col for col in dataset.columns.tolist() if 'node' in col ]\n",
    "features_container_wise = {}\n",
    "application_features_for_each_DU = {}\n",
    "\n",
    "for CU, connected_DUs in topology.items():\n",
    "    features_container_wise[CU] = []\n",
    "    for DU in connected_DUs:\n",
    "        features_container_wise[DU] = []\n",
    "    # Add node features for CU\n",
    "    features_container_wise[CU].extend([col for col in dataset.columns if CU in col])\n",
    "    # Add DU features for each connected DU\n",
    "    for DU in connected_DUs:\n",
    "        # Extract features for the DU\n",
    "        DU_features = [col for col in dataset.columns if DU in col]\n",
    "        features_container_wise[DU].extend(DU_features)\n",
    "\n",
    "\n",
    "\n",
    "for CU, connected_DUs in topology.items():\n",
    "    for DU in connected_DUs:\n",
    "        du_index = str(int(DU.replace('srsdu', '')) + 1)  # PCI is assumed 0-based, matching DU index\n",
    "        rntis = pci_info_map[du_index]['rntis']\n",
    "        metrics = pci_info_map[du_index]['metrics']\n",
    "        for metric in metrics:\n",
    "            for rnti in rntis:\n",
    "                feature = f'PCI-{du_index}_RNTI-{rnti}_{metric}'\n",
    "                if feature in dataset.columns:\n",
    "                    if DU not in application_features_for_each_DU:\n",
    "                        application_features_for_each_DU[DU] = []\n",
    "                    application_features_for_each_DU[DU].append(feature)\n",
    "\n",
    "# # Print the features for each DU\n",
    "# print(\"Application features for each DU:\")\n",
    "# for DU, features in application_features_for_each_DU.items():\n",
    "#     print(f\"{DU}: {features}\\n\\t{len(features)} features\")\n",
    "\n",
    "\n",
    "# print(\"application_features_for_each_DU\")\n",
    "# for DU, features in application_features_for_each_DU.items():\n",
    "#     print(f\"{DU}: {features}\\n\\t{len(features)} features\")\n",
    "\n",
    "\n",
    "# print(\"Features container wise:\")\n",
    "# for container, features in features_container_wise.items():\n",
    "#     print(f\"{container}: {features}\\n\\t{len(features)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0f699fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_by_CU = {}\n",
    "\n",
    "for CU, connected_DUs in topology.items():\n",
    "    for DU in connected_DUs:\n",
    "        features = []\n",
    "        features = features_container_wise[CU] + features_container_wise[DU] + node_features\n",
    "        features = features + application_features_for_each_DU[DU]\n",
    "        features = sorted(set(features))\n",
    "        features_by_CU[(CU, DU)] = features\n",
    "    if(len(features) == len(set(features))):\n",
    "        pass\n",
    "    else:\n",
    "        print(f\"Duplicate features found for {CU} and {DU}\")\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(f'features_{CU}_{DU}.json', 'w') as f:\n",
    "        json.dump(features, f, indent=2)\n",
    "    \n",
    "    # # read the JSON file to verify\n",
    "    # with open(f'features_{CU}_{DU}.json', 'r') as f:\n",
    "    #     loaded_features = json.load(f)\n",
    "    #     print(f\"Loaded features for {CU} and {DU}: {loaded_features}\\n\\tLength: {len(loaded_features)} features\")\n",
    "\n",
    "\n",
    "# print(features_by_CU)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
