{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sampled_data(raw_data, sample_fraction=0.01):\n",
    "    # Extract features and target\n",
    "    target_col = 'target'\n",
    "    target = raw_data[target_col]\n",
    "\n",
    "    # Handle missing target values: either fill with mode or drop rows with NaN in target\n",
    "    target.fillna(target.mode()[0], inplace=True)  # Filling NaN with the mode of the target\n",
    "    \n",
    "    # Initialize StratifiedShuffleSplit to split the data\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=sample_fraction, random_state=42)\n",
    "\n",
    "    # Sample sample_fraction% of data maintaining class distribution\n",
    "    for train_idx, test_idx in sss.split(raw_data, target):\n",
    "        sampled_data = raw_data.iloc[test_idx]\n",
    "\n",
    "    return sampled_data\n",
    "\n",
    "def create_sequences(data, seq_length, horizon, combined_samples_targets):\n",
    "    X, X_targets, y, y_targets = [], [], [], []\n",
    "    feature_names = [col for col in data.columns if col != 'target']  # List of feature column names\n",
    "    \n",
    "    for i in range(len(data) - seq_length - horizon + 1):\n",
    "        # X should contain all columns except 'target' (make sure it's a DataFrame)\n",
    "        X_seq = data[feature_names].iloc[i:i + seq_length]\n",
    "\n",
    "        # X_targets should contain only the 'target' column\n",
    "        X_targets_seq = data[combined_samples_targets].iloc[i:i + seq_length]\n",
    "        \n",
    "        # y should contain the entire row for each sequence, except 'target'\n",
    "        y_seq = data[feature_names].iloc[i + seq_length + horizon - 1]\n",
    "        \n",
    "        # y_targets should contain only the 'target' column\n",
    "        y_target = data[combined_samples_targets].iloc[i + seq_length + horizon - 1]\n",
    "        \n",
    "        # Append sequences to the respective lists\n",
    "        X.append(X_seq)\n",
    "        X_targets.append(X_targets_seq)\n",
    "        y.append(y_seq)\n",
    "        y_targets.append(y_target)\n",
    "\n",
    "    return np.array(X), np.array(X_targets), np.array(y), np.array(y_targets)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# **Define LSTM Model**\n",
    "class LSTMForecaster(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTMForecaster, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
    "        c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
    "        out, _ = self.lstm(x, (h_0, c_0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "# **Custom Dataset**\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, num_features, num_classes):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Adjust the first layer to match the input dimensions\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + num_classes, 512),  # Adjust this to match input dimensions\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_features),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z, labels):\n",
    "        # Concatenate latent vector and labels\n",
    "        inputs = torch.cat([z, labels], dim=1)\n",
    "        return self.model(inputs)\n",
    "\n",
    "# Define the Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(num_features + num_classes, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        # Concatenate features and labels\n",
    "        inputs = torch.cat([x, labels], dim=1)\n",
    "        return self.model(inputs)\n",
    "# Define the GAN\n",
    "class GAN(nn.Module):\n",
    "    def __init__(self, generator, discriminator):\n",
    "        super(GAN, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "    \n",
    "    def forward(self, z, labels):\n",
    "        generated_data = self.generator(z, labels)\n",
    "        validity = self.discriminator(generated_data, labels)\n",
    "        return generated_data, validity\n",
    "# Function to train the GAN\n",
    "def train_gan(gan, dataloader, num_epochs, latent_dim, num_classes, device):\n",
    "    generator = gan.generator\n",
    "    discriminator = gan.discriminator\n",
    "    \n",
    "    # Loss function\n",
    "    adversarial_loss = nn.BCELoss()\n",
    "    \n",
    "    # Optimizers\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (real_data, labels) in enumerate(dataloader):\n",
    "            real_data = real_data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Create labels for real and fake data\n",
    "            valid_labels = torch.ones(real_data.size(0), 1).to(device)\n",
    "            fake_labels = torch.zeros(real_data.size(0), 1).to(device)\n",
    "            \n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            z = torch.randn(real_data.size(0), latent_dim).to(device)\n",
    "            gen_labels = torch.randint(0, num_classes, (real_data.size(0),)).to(device)\n",
    "            gen_labels_one_hot = torch.nn.functional.one_hot(gen_labels, num_classes=num_classes).float().to(device)\n",
    "            \n",
    "            generated_data = generator(z, gen_labels_one_hot)\n",
    "            g_loss = adversarial_loss(discriminator(generated_data, gen_labels_one_hot), valid_labels)\n",
    "            \n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            \n",
    "            real_loss = adversarial_loss(discriminator(real_data, labels.float()), valid_labels)\n",
    "            fake_loss = adversarial_loss(discriminator(generated_data.detach(), gen_labels_one_hot), fake_labels)\n",
    "            \n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            \n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "        \n",
    "        print(f\"[Epoch {epoch}/{num_epochs}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
    "    return generator, discriminator\n",
    "# Function to generate synthetic data using the trained GAN\n",
    "def generate_synthetic_data(generator, num_samples, latent_dim, num_classes, device):\n",
    "    z = torch.randn(num_samples, latent_dim).to(device)\n",
    "    gen_labels = torch.randint(0, num_classes, (num_samples,)).to(device)\n",
    "    gen_labels_one_hot = torch.nn.functional.one_hot(gen_labels, num_classes=num_classes).float().to(device)\n",
    "    \n",
    "    synthetic_data = generator(z, gen_labels_one_hot)\n",
    "    return synthetic_data.cpu().detach().numpy(), gen_labels.cpu().detach().numpy()\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, targets in dataloader:\n",
    "            sequences = sequences.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(sequences)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    \n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    return all_preds, all_targets\n",
    "# Function to visualize the results\n",
    "def visualize_results(preds, targets, output_dir):\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(targets, preds)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    tick_marks = np.arange(len(np.unique(targets)))\n",
    "    plt.xticks(tick_marks, np.unique(targets))\n",
    "    plt.yticks(tick_marks, np.unique(targets))\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n",
    "    plt.close()\n",
    "# Function to save classification report\n",
    "def save_classification_report(preds, targets, output_dir):\n",
    "    report = classification_report(targets, preds, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    \n",
    "    # Save the classification report as a CSV file\n",
    "    report_df.to_csv(os.path.join(output_dir, 'classification_report.csv'), index=True)\n",
    "    \n",
    "    # Print the classification report\n",
    "    print(report_df)\n",
    "# Function to save the model\n",
    "def save_model(model, output_dir, model_name):\n",
    "    torch.save(model.state_dict(), os.path.join(output_dir, f\"{model_name}.pth\"))\n",
    "    print(f\"Model saved to {os.path.join(output_dir, f'{model_name}.pth')}\")\n",
    "# Function to load the model\n",
    "def load_model(model, model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "# Function to plot feature importance\n",
    "def plot_feature_importance(model, feature_names, output_dir):\n",
    "    # Get the weights of the first layer\n",
    "    weights = model.lstm.weight_ih_l0.data.cpu().numpy()\n",
    "    \n",
    "    # Calculate feature importance\n",
    "    feature_importance = np.abs(weights).sum(axis=0)\n",
    "    \n",
    "    # Normalize feature importance\n",
    "    feature_importance /= feature_importance.sum()\n",
    "    \n",
    "    # Create a DataFrame for better visualization\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importance')\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(os.path.join(output_dir, 'feature_importance.png'))\n",
    "    plt.close()\n",
    "# Function to plot training history\n",
    "def plot_training_history(history, output_dir):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history['loss'], label='Loss')\n",
    "    plt.plot(history['accuracy'], label='Accuracy')\n",
    "    plt.title('Training History')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(os.path.join(output_dir, 'training_history.png'))\n",
    "    plt.close()\n",
    "# Function to plot PCA\n",
    "def plot_pca(data, labels, output_dir):\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(data)\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], c=labels, cmap='viridis', alpha=0.5)\n",
    "    plt.title('PCA of Data')\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    \n",
    "    # Create a legend\n",
    "    legend1 = plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "    plt.gca().add_artist(legend1)\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(os.path.join(output_dir, 'pca_plot.png'))\n",
    "    plt.close()\n",
    "# Function to plot SHAP values\n",
    "def plot_shap_values(model, data, feature_names, output_dir):\n",
    "    # Use Integrated Gradients for SHAP values\n",
    "    ig = IntegratedGradients(model)\n",
    "    \n",
    "    # Calculate SHAP values\n",
    "    shap_values = ig.attribute(data, target=0)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    shap_values = shap_values.cpu().detach().numpy()\n",
    "    \n",
    "    # Create a DataFrame for better visualization\n",
    "    shap_df = pd.DataFrame(shap_values, columns=feature_names)\n",
    "    \n",
    "    # Plot SHAP values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(shap_df.columns, shap_df.mean(axis=0))\n",
    "    plt.xlabel('SHAP Value')\n",
    "    plt.title('SHAP Values')\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(os.path.join(output_dir, 'shap_values.png'))\n",
    "    plt.close()\n",
    "# Function to create output directory\n",
    "def create_output_directory(output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    print(f\"Output directory created at: {output_dir}\")\n",
    "# Function to save synthetic data\n",
    "def save_synthetic_data(data, labels, output_dir):\n",
    "    synthetic_data_df = pd.DataFrame(data)\n",
    "    synthetic_data_df['target'] = labels\n",
    "    synthetic_data_df.to_csv(os.path.join(output_dir, 'synthetic_data.csv'), index=False)\n",
    "    print(f\"Synthetic data saved to {os.path.join(output_dir, 'synthetic_data.csv')}\")\n",
    "# Function to save the generator model\n",
    "def save_generator_model(generator, output_dir):\n",
    "    torch.save(generator.state_dict(), os.path.join(output_dir, 'generator.pth'))\n",
    "    print(f\"Generator model saved to {os.path.join(output_dir, 'generator.pth')}\")\n",
    "# Function to save the discriminator model\n",
    "def save_discriminator_model(discriminator, output_dir):\n",
    "    torch.save(discriminator.state_dict(), os.path.join(output_dir, 'discriminator.pth'))\n",
    "    print(f\"Discriminator model saved to {os.path.join(output_dir, 'discriminator.pth')}\")\n",
    "# Function to load the generator model\n",
    "def load_generator_model(generator, model_path):\n",
    "    generator.load_state_dict(torch.load(model_path))\n",
    "    generator.eval()\n",
    "    print(f\"Generator model loaded from {model_path}\")\n",
    "# Function to load the discriminator model\n",
    "def load_discriminator_model(discriminator, model_path):\n",
    "    discriminator.load_state_dict(torch.load(model_path))\n",
    "    discriminator.eval()\n",
    "    print(f\"Discriminator model loaded from {model_path}\")\n",
    "# Function to save the GAN model\n",
    "def save_gan_model(gan, output_dir):\n",
    "    torch.save(gan.state_dict(), os.path.join(output_dir, 'gan.pth'))\n",
    "    print(f\"GAN model saved to {os.path.join(output_dir, 'gan.pth')}\")\n",
    "# Function to load the GAN model\n",
    "def load_gan_model(gan, model_path):\n",
    "    gan.load_state_dict(torch.load(model_path))\n",
    "    gan.eval()\n",
    "    print(f\"GAN model loaded from {model_path}\")\n",
    "# Function to save the training history\n",
    "def save_training_history(history, output_dir):\n",
    "    with open(os.path.join(output_dir, 'training_history.csv'), 'w', newline='') as csvfile:\n",
    "        fieldnames = ['epoch', 'loss', 'accuracy']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        for epoch, (loss, accuracy) in enumerate(zip(history['loss'], history['accuracy'])):\n",
    "            writer.writerow({'epoch': epoch, 'loss': loss, 'accuracy': accuracy})\n",
    "    \n",
    "    print(f\"Training history saved to {os.path.join(output_dir, 'training_history.csv')}\")\n",
    "# Function to save the model architecture\n",
    "def save_model_architecture(model, output_dir):\n",
    "    with open(os.path.join(output_dir, 'model_architecture.txt'), 'w') as f:\n",
    "        f.write(str(model))\n",
    "    print(f\"Model architecture saved to {os.path.join(output_dir, 'model_architecture.txt')}\")\n",
    "# Function to save the training configuration\n",
    "def save_training_configuration(config, output_dir):\n",
    "    with open(os.path.join(output_dir, 'training_configuration.txt'), 'w') as f:\n",
    "        for key, value in config.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "    print(f\"Training configuration saved to {os.path.join(output_dir, 'training_configuration.txt')}\")\n",
    "# Function to save the training and validation loss\n",
    "def save_training_validation_loss(train_loss, val_loss, output_dir):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(os.path.join(output_dir, 'training_validation_loss.png'))\n",
    "    plt.close()\n",
    "# Function to save the training and validation accuracy\n",
    "def save_training_validation_accuracy(train_accuracy, val_accuracy, output_dir):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_accuracy, label='Training Accuracy')\n",
    "    plt.plot(val_accuracy, label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(os.path.join(output_dir, 'training_validation_accuracy.png'))\n",
    "    plt.close()\n",
    "# Function to save the training and validation F1 score\n",
    "def save_training_validation_f1(train_f1, val_f1, output_dir):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_f1, label='Training F1 Score')\n",
    "    plt.plot(val_f1, label='Validation F1 Score')\n",
    "    plt.title('Training and Validation F1 Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(os.path.join(output_dir, 'training_validation_f1.png'))\n",
    "    plt.close()\n",
    "# Function to save the training and validation metrics\n",
    "def save_training_validation_metrics(train_metrics, val_metrics, output_dir):\n",
    "    metrics_df = pd.DataFrame({'Epoch': range(len(train_metrics)), 'Train Metrics': train_metrics, 'Validation Metrics': val_metrics})\n",
    "    metrics_df.to_csv(os.path.join(output_dir, 'training_validation_metrics.csv'), index=False)\n",
    "    print(f\"Training and validation metrics saved to {os.path.join(output_dir, 'training_validation_metrics.csv')}\")\n",
    "# Function to save the training and validation ROC curve\n",
    "def save_training_validation_roc_curve(train_fpr, train_tpr, val_fpr, val_tpr, output_dir):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_fpr, train_tpr, label='Training ROC Curve')\n",
    "    plt.plot(val_fpr, val_tpr, label='Validation ROC Curve')\n",
    "    plt.title('Training and Validation ROC Curve')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(os.path.join(output_dir, 'training_validation_roc_curve.png'))\n",
    "    plt.close()\n",
    "# Function to save the training and validation precision-recall curve\n",
    "def save_training_validation_precision_recall_curve(train_precision, train_recall, val_precision, val_recall, output_dir):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_recall, train_precision, label='Training Precision-Recall Curve')\n",
    "    plt.plot(val_recall, val_precision, label='Validation Precision-Recall Curve')\n",
    "    plt.title('Training and Validation Precision-Recall Curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(os.path.join(output_dir, 'training_validation_precision_recall_curve.png'))\n",
    "    plt.close()\n",
    "# Function to save the training and validation confusion matrix\n",
    "def save_training_validation_confusion_matrix(train_confusion_matrix, val_confusion_matrix, output_dir):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(train_confusion_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Training Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(val_confusion_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Validation Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(os.path.join(output_dir, 'training_validation_confusion_matrix.png'))\n",
    "    plt.close()\n",
    "# Function to save the training and validation classification report\n",
    "def save_training_validation_classification_report(train_report, val_report, output_dir):\n",
    "    train_report_df = pd.DataFrame(train_report).transpose()\n",
    "    val_report_df = pd.DataFrame(val_report).transpose()\n",
    "    \n",
    "    # Save the classification reports as CSV files\n",
    "    train_report_df.to_csv(os.path.join(output_dir, 'training_classification_report.csv'), index=True)\n",
    "    val_report_df.to_csv(os.path.join(output_dir, 'validation_classification_report.csv'), index=True)\n",
    "    \n",
    "    # Print the classification reports\n",
    "    print(train_report_df)\n",
    "    print(val_report_df)\n",
    "# Function to save the training and validation metrics history\n",
    "def save_training_validation_metrics_history(train_metrics_history, val_metrics_history, output_dir):\n",
    "    metrics_df = pd.DataFrame({'Epoch': range(len(train_metrics_history)), 'Train Metrics': train_metrics_history, 'Validation Metrics': val_metrics_history})\n",
    "    metrics_df.to_csv(os.path.join(output_dir, 'training_validation_metrics_history.csv'), index=False)\n",
    "    print(f\"Training and validation metrics history saved to {os.path.join(output_dir, 'training_validation_metrics_history.csv')}\")\n",
    "# Function to save the training and validation metrics history plot\n",
    "def save_training_validation_metrics_history_plot(train_metrics_history, val_metrics_history, output_dir):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_metrics_history, label='Training Metrics History')\n",
    "    plt.plot(val_metrics_history, label='Validation Metrics History')\n",
    "    plt.title('Training and Validation Metrics History')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Metrics')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(os.path.join(output_dir, 'training_validation_metrics_history_plot.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_dim=10\n",
    "lookforward=5\n",
    "lookback=60\n",
    "PCA_DIMENSION = 10\n",
    "\n",
    "# Model parameters\n",
    "input_dim = PCA_DIMENSION\n",
    "hidden_dim = 32\n",
    "num_layers = 2\n",
    "output_dim = PCA_DIMENSION\n",
    "epochs = 500\n",
    "learning_rate = 0.005\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                host    srscu0    srscu1    srscu2    srscu3    srsdu0  \\\n",
      "2025-04-01  0.496714  1.399355 -0.675178 -1.907808 -0.863494 -0.423760   \n",
      "2025-04-02 -0.138264  0.924634 -0.144519 -0.860385 -0.031203 -0.453414   \n",
      "2025-04-03  0.647689  0.059630 -0.792420 -0.413606  0.018017 -1.795643   \n",
      "2025-04-04  1.523030 -0.646937 -0.307962  1.887688  0.472630 -0.330090   \n",
      "2025-04-05 -0.234153  0.698223 -1.893615  0.556553 -1.366858  0.732829   \n",
      "\n",
      "              srsdu1    srsdu2    srsdu3  srscu0_stressType  ...  \\\n",
      "2025-04-01 -1.114081  0.785185 -0.033025                  1  ...   \n",
      "2025-04-02 -0.630931 -1.777681 -0.503650                  2  ...   \n",
      "2025-04-03 -0.942060  0.714746 -0.172375                  3  ...   \n",
      "2025-04-04 -0.547996 -0.233724  0.714732                  4  ...   \n",
      "2025-04-05 -0.214150  0.707458  1.277857                  4  ...   \n",
      "\n",
      "            application_feature_10  application_feature_11  \\\n",
      "2025-04-01               -0.404063               -0.809919   \n",
      "2025-04-02                1.445440               -0.133404   \n",
      "2025-04-03               -0.362763                2.591477   \n",
      "2025-04-04               -0.502796                0.708762   \n",
      "2025-04-05                1.236869               -1.700552   \n",
      "\n",
      "            application_feature_12  application_feature_13  \\\n",
      "2025-04-01                0.551614                0.773489   \n",
      "2025-04-02                0.017327               -0.588120   \n",
      "2025-04-03                1.544753               -0.176099   \n",
      "2025-04-04                0.024213                1.312524   \n",
      "2025-04-05                0.419020                0.793620   \n",
      "\n",
      "            application_feature_14  application_feature_15  \\\n",
      "2025-04-01               -0.851345                1.528463   \n",
      "2025-04-02                0.865907                0.219909   \n",
      "2025-04-03                0.783653                0.420825   \n",
      "2025-04-04               -0.022010               -0.522626   \n",
      "2025-04-05               -0.558240                1.538692   \n",
      "\n",
      "            application_feature_16  application_feature_17  \\\n",
      "2025-04-01               -0.239123               -0.029531   \n",
      "2025-04-02                0.689044                1.502454   \n",
      "2025-04-03               -1.932096                0.214025   \n",
      "2025-04-04               -1.161827               -0.504450   \n",
      "2025-04-05                0.504608               -0.811072   \n",
      "\n",
      "            application_feature_18  application_feature_19  \n",
      "2025-04-01                0.651638                0.256467  \n",
      "2025-04-02               -0.691303               -0.821816  \n",
      "2025-04-03                0.000687                1.378026  \n",
      "2025-04-04                1.355564               -0.384567  \n",
      "2025-04-05               -0.487849               -0.754938  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "# # Load dataset\n",
    "# dataset = pd.read_csv('small_sample.csv')\n",
    "# # dataset = dataset[:int(0.01*len(dataset))]\n",
    "\n",
    "# dataset.index = dataset['Timestamp']\n",
    "# dataset = dataset.drop(columns=['Timestamp'])\n",
    "# print(dataset.head())\n",
    "\n",
    "# # **Data Preprocessing**\n",
    "# # Handle missing values\n",
    "# dataset = dataset.apply(lambda x: x.fillna(0) if x.isna().all() else x)\n",
    "# threshold = 0.6 * len(dataset)\n",
    "# for col in dataset.columns:\n",
    "#     if dataset[col].isna().sum() > threshold:\n",
    "#         mode_value = dataset[col].mode().iloc[0] if not dataset[col].mode().empty else 0\n",
    "#         dataset.fillna({col: mode_value}, inplace=True)\n",
    "# #            dataset[col].fillna(mode_value, inplace=True)\n",
    "# # dataset = dataset.dropna(subset=['target'])\n",
    "# numeric_cols = dataset.select_dtypes(include=[np.number]).columns\n",
    "# dataset[numeric_cols] = dataset[numeric_cols].fillna(dataset[numeric_cols].mean())\n",
    "\n",
    "\n",
    "\n",
    "# dataset.shape\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create dummy dataset with 4 features and 2 targets\n",
    "num_samples = 1000\n",
    "data = {\n",
    "    'host': np.random.randn(num_samples),\n",
    "    'srscu0': np.random.randn(num_samples),\n",
    "    'srscu1': np.random.randn(num_samples),\n",
    "    'srscu2': np.random.randn(num_samples),\n",
    "    'srscu3': np.random.randn(num_samples),\n",
    "    'srsdu0': np.random.randn(num_samples),\n",
    "    'srsdu1': np.random.randn(num_samples),\n",
    "    'srsdu2': np.random.randn(num_samples),\n",
    "    'srsdu3': np.random.randn(num_samples),\n",
    "    'srscu0_stressType': np.random.randint(0, 5, num_samples),  # Binary target\n",
    "    'srscu1_stressType': np.random.randint(0, 5, num_samples),  # Binary target\n",
    "    'srscu2_stressType': np.random.randint(0, 5, num_samples),  # Binary target\n",
    "    'srscu3_stressType': np.random.randint(0, 5, num_samples),  # Binary target\n",
    "    'srsdu0_stressType': np.random.randint(0, 5, num_samples),  # Binary target\n",
    "    'srsdu1_stressType': np.random.randint(0, 5, num_samples),  # Binary target\n",
    "    'srsdu2_stressType': np.random.randint(0, 5, num_samples),  # Binary target\n",
    "    'srsdu3_stressType': np.random.randint(0, 5, num_samples),  # Binary target\n",
    "}\n",
    "\n",
    "for i in range(20):\n",
    "    data[f'application_feature_{i}'] = np.random.randn(num_samples)  # Additional features\n",
    "\n",
    "\n",
    "# Create a DataFrame\n",
    "dataset = pd.DataFrame(data)\n",
    "\n",
    "# Create a timestamp index (assuming the start date is '2025-04-01')\n",
    "dataset.index = pd.date_range(start='2025-04-01', periods=num_samples, freq='D')\n",
    "\n",
    "\n",
    "# Show the first few rows of the dataset\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'srscu0': ['srsdu0'], 'srscu1': ['srsdu1'], 'srscu2': ['srsdu2'], 'srscu3': ['srsdu3']}\n",
      "{('srscu0', 'srsdu0'): {'features': ['application_feature_12', 'application_feature_13', 'application_feature_8', 'srsdu0', 'application_feature_10', 'host', 'application_feature_0', 'application_feature_16', 'application_feature_14', 'application_feature_4', 'application_feature_18', 'application_feature_9', 'application_feature_11', 'srscu0', 'application_feature_17', 'application_feature_7', 'application_feature_6', 'application_feature_1', 'application_feature_2', 'application_feature_5', 'application_feature_3', 'application_feature_15', 'application_feature_19'], 'targets': ['srscu0_stressType', 'srsdu0_stressType']}, ('srscu1', 'srsdu1'): {'features': ['application_feature_12', 'application_feature_13', 'application_feature_8', 'application_feature_10', 'host', 'application_feature_0', 'application_feature_16', 'application_feature_14', 'application_feature_4', 'application_feature_18', 'srsdu1', 'application_feature_9', 'application_feature_11', 'srscu1', 'application_feature_17', 'application_feature_7', 'application_feature_6', 'application_feature_1', 'application_feature_2', 'application_feature_5', 'application_feature_3', 'application_feature_15', 'application_feature_19'], 'targets': ['srscu1_stressType', 'srsdu1_stressType']}, ('srscu2', 'srsdu2'): {'features': ['application_feature_12', 'application_feature_13', 'application_feature_8', 'application_feature_10', 'host', 'application_feature_0', 'application_feature_16', 'application_feature_14', 'application_feature_4', 'application_feature_18', 'srsdu2', 'application_feature_9', 'application_feature_11', 'application_feature_17', 'application_feature_7', 'srscu2', 'application_feature_6', 'application_feature_1', 'application_feature_2', 'application_feature_5', 'application_feature_3', 'application_feature_15', 'application_feature_19'], 'targets': ['srscu2_stressType', 'srsdu2_stressType']}, ('srscu3', 'srsdu3'): {'features': ['srscu3', 'application_feature_12', 'application_feature_13', 'application_feature_8', 'application_feature_10', 'host', 'application_feature_0', 'application_feature_16', 'application_feature_14', 'application_feature_4', 'application_feature_18', 'application_feature_9', 'application_feature_11', 'application_feature_17', 'application_feature_7', 'application_feature_6', 'application_feature_1', 'application_feature_3', 'application_feature_2', 'application_feature_5', 'srsdu3', 'application_feature_15', 'application_feature_19'], 'targets': ['srsdu3_stressType', 'srscu3_stressType']}}\n",
      "Host and CU: srscu0, DU: srsdu0 - Combined Features:\n",
      "Number of Features: 23\n",
      "Number of Targets: 2\n",
      "['application_feature_12', 'application_feature_13', 'application_feature_8', 'srsdu0', 'application_feature_10', 'host', 'application_feature_0', 'application_feature_16', 'application_feature_14', 'application_feature_4']\n",
      "----------------------------------------\n",
      "Host and CU: srscu1, DU: srsdu1 - Combined Features:\n",
      "Number of Features: 23\n",
      "Number of Targets: 2\n",
      "['application_feature_12', 'application_feature_13', 'application_feature_8', 'application_feature_10', 'host', 'application_feature_0', 'application_feature_16', 'application_feature_14', 'application_feature_4', 'application_feature_18']\n",
      "----------------------------------------\n",
      "Host and CU: srscu2, DU: srsdu2 - Combined Features:\n",
      "Number of Features: 23\n",
      "Number of Targets: 2\n",
      "['application_feature_12', 'application_feature_13', 'application_feature_8', 'application_feature_10', 'host', 'application_feature_0', 'application_feature_16', 'application_feature_14', 'application_feature_4', 'application_feature_18']\n",
      "----------------------------------------\n",
      "Host and CU: srscu3, DU: srsdu3 - Combined Features:\n",
      "Number of Features: 23\n",
      "Number of Targets: 2\n",
      "['srscu3', 'application_feature_12', 'application_feature_13', 'application_feature_8', 'application_feature_10', 'host', 'application_feature_0', 'application_feature_16', 'application_feature_14', 'application_feature_4']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "NoOfCUs = 4\n",
    "NoOfDUs = 4\n",
    "\n",
    "# Creating Topology\n",
    "topology = {}\n",
    "\n",
    "# Form the graph where srscu0 connects to srsdu0, srscu1 to srsdu1, and so on\n",
    "for i in range(min(NoOfCUs, NoOfDUs)):  # Prevent index errors\n",
    "    topology[f\"srscu{i}\"] = [f\"srsdu{i}\"]\n",
    "\n",
    "# Display the graph\n",
    "print(topology)\n",
    "\n",
    "\n",
    "common_features = dataset.columns.tolist()\n",
    "container_specific_features = {}\n",
    "\n",
    "# Loop through and remove columns containing specific substrings\n",
    "for i in range(NoOfDUs+1):\n",
    "    common_features = [col for col in common_features if f\"srscu{i}\" not in col and f\"srsdu{i}\" not in col]\n",
    "\n",
    "# Store container-specific dataframes instead of lists\n",
    "for i in range(NoOfCUs+1):\n",
    "    container_specific_features[f'srscu{i}'] = dataset[[col for col in dataset.columns.tolist() if f\"srscu{i}\" in col]]\n",
    "\n",
    "for i in range(NoOfDUs+1):\n",
    "    container_specific_features[f'srsdu{i}'] = dataset[[col for col in dataset.columns.tolist() if f\"srsdu{i}\" in col]]\n",
    "\n",
    "# # Print the remaining features\n",
    "# print(len(common_features), common_features)\n",
    "\n",
    "# print(\"Before:\")\n",
    "\n",
    "# # Print container-specific features (as dataframes now)\n",
    "# for i in range(NoOfCUs):\n",
    "#     print(f\"srscu{i}:\")\n",
    "#     print(container_specific_features[f'srscu{i}'].shape)\n",
    "#     print(container_specific_features[f'srscu{i}'].head())\n",
    "\n",
    "# for i in range(NoOfDUs):\n",
    "#     print(f\"srsdu{i}:\")\n",
    "#     print(container_specific_features[f'srsdu{i}'].shape)\n",
    "#     print(container_specific_features[f'srsdu{i}'].head())\n",
    "\n",
    "# Filter out columns containing 'stepStress' from the container-specific dataframes\n",
    "for i in range(NoOfCUs):\n",
    "    container_specific_features[f'srscu{i}'] = container_specific_features[f'srscu{i}'].loc[:, ~container_specific_features[f'srscu{i}'].columns.str.contains('stepStress')]\n",
    "\n",
    "for i in range(NoOfDUs):\n",
    "    container_specific_features[f'srsdu{i}'] = container_specific_features[f'srsdu{i}'].loc[:, ~container_specific_features[f'srsdu{i}'].columns.str.contains('stepStress')]\n",
    "\n",
    "# print(\"After:\")\n",
    "\n",
    "# # Print container-specific features (after filtering)\n",
    "# for i in range(NoOfCUs):\n",
    "#     print(f\"srscu{i}:\")\n",
    "#     print(container_specific_features[f'srscu{i}'].shape)\n",
    "#     print(container_specific_features[f'srscu{i}'].head())\n",
    "\n",
    "# for i in range(NoOfDUs):\n",
    "#     print(f\"srsdu{i}:\")\n",
    "#     print(container_specific_features[f'srsdu{i}'].shape)\n",
    "#     print(container_specific_features[f'srsdu{i}'].head())\n",
    "\n",
    "\n",
    "# Iterate through the topology and combine features\n",
    "combined_samples = {}\n",
    "\n",
    "for CU in topology.keys():\n",
    "    # The CU container-specific features\n",
    "    CU_features = container_specific_features[CU]\n",
    "    \n",
    "    # The connected DUs (from topology)\n",
    "    connected_DUs = topology[CU]\n",
    "    \n",
    "    # Add CU-specific features to the combined list\n",
    "    CU_features_list = CU_features.columns.tolist()\n",
    "    \n",
    "    # Extract the CU stress type column (if exists)\n",
    "    CU_stressType = f'{CU}_stressType' if f'{CU}_stressType' in CU_features.columns else None\n",
    "    \n",
    "    # Add DU-specific features to the combined list for each connected DU\n",
    "    for DU in connected_DUs:\n",
    "        # Ensure DU exists in container_specific_features\n",
    "        if DU in container_specific_features:\n",
    "            DU_features = container_specific_features[DU]\n",
    "            DU_features_list = DU_features.columns.tolist()\n",
    "\n",
    "            # Combine CU and DU features (remove the stress type columns from features)\n",
    "            combined_features = common_features.copy()  # Start with the common features\n",
    "            \n",
    "            # Modify these lines:\n",
    "            combined_features.extend(CU_features_list)  # Keep all CU features\n",
    "            combined_features.extend(DU_features_list)  # Keep all DU features\n",
    "\n",
    "            \n",
    "\n",
    "            # Extract targets and remove them from features\n",
    "            targets = [col for col in combined_features if '_stressType' in col]\n",
    "\n",
    "            # To keep stressType columns temporarily:\n",
    "            combined_samples[(CU, DU)] = {\n",
    "                'features': list(set(combined_features) - set(targets)),  # Include targets in features temporarily\n",
    "                'targets': list(set(targets))\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            print(f\"Error: {DU} not found in container_specific_features!\")\n",
    "            continue  # Skip this DU if not found in container_specific_features\n",
    "    \n",
    "print(combined_samples)\n",
    "# Print the results for each CU-DU pair and its combined features\n",
    "for (CU, DU), sample in combined_samples.items():\n",
    "    print(f\"Host and CU: {CU}, DU: {DU} - Combined Features:\")\n",
    "    print(f\"Number of Features: {len(sample['features'])}\")\n",
    "    print(f\"Number of Targets: {len(sample['targets'])}\")\n",
    "    print(sample['features'][:10])  # Print first 10 features as a preview\n",
    "    print(\"----\" * 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC_ROC = []\n",
    "# PRECISION = []\n",
    "# RECALL = []\n",
    "# F1_SCORE = []\n",
    "\n",
    "# for (CU, DU), sample in combined_samples.items():\n",
    "#     print(f\"Training on data of Host, {CU} and {DU}\")\n",
    "\n",
    "\n",
    "#     # Load data with duplicate handling\n",
    "#     raw_data = dataset[sample['features'] + sample['targets']].copy()\n",
    "#     raw_data = raw_data.loc[:, ~raw_data.columns.duplicated()]  # KEY FIX\n",
    "\n",
    "\n",
    "#     # Filter using targets\n",
    "#     for target_col in sample['targets']:\n",
    "#         raw_data = raw_data[raw_data[target_col].isin([0, 1, 2, 3])]\n",
    "\n",
    "\n",
    "#     # **Data Preprocessing**\n",
    "#     # Handle missing values\n",
    "#     raw_data = raw_data.apply(lambda x: x.fillna(0) if x.isna().all() else x)\n",
    "\n",
    "\n",
    "#     threshold = 0.6 * len(raw_data)\n",
    "#     raw_data = raw_data.loc[:, ~raw_data.columns.duplicated()]  # Remove duplicates\n",
    "\n",
    "#     for col in raw_data.columns:\n",
    "#         nan_count = raw_data[col].isna().sum()\n",
    "#         if int(nan_count) > threshold:  # Explicit scalar conversion\n",
    "#             mode_value = raw_data[col].mode().iloc[0] if not raw_data[col].mode().empty else 0\n",
    "#             raw_data[col].fillna(mode_value, inplace=True)\n",
    "\n",
    "#     numeric_cols = raw_data.select_dtypes(include=[np.number]).columns\n",
    "#     raw_data[numeric_cols] = raw_data[numeric_cols].fillna(raw_data[numeric_cols].mean())\n",
    "\n",
    "\n",
    "#     # **Convert target columns to binary (0 or 1)**\n",
    "#     # Instead of using a loop over rows, we can do it in a vectorized way\n",
    "#     for target_col in sample['targets']:\n",
    "#         raw_data[target_col] = raw_data[target_col].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "\n",
    "#     # Create unified target column\n",
    "#     raw_data['target'] = 0\n",
    "#     for idx in raw_data.index:\n",
    "#         if any(raw_data.loc[idx, sample['targets']] == 1):\n",
    "#             raw_data.at[idx, 'target'] = 1\n",
    "\n",
    "    \n",
    "#     X = raw_data.drop(columns=sample['targets']+['target'])\n",
    "#     Y = raw_data['target']\n",
    "\n",
    "#     # To avoid division by zero:\n",
    "#     X = (X - X.mean()) / (X.std() + 1e-8)\n",
    "\n",
    "#     train_idx = int(0.8 * len(X))\n",
    "\n",
    "#     # Concatenate X and Y into raw_data (pd.concat is used to join the features and targets)\n",
    "#     raw_data = pd.concat([X, Y], axis=1)\n",
    "\n",
    "\n",
    "#     raw_data_training = raw_data[:train_idx]\n",
    "#     raw_data_testing = raw_data[train_idx:]\n",
    "\n",
    "\n",
    "#     # Convert all columns to float16\n",
    "#     raw_data_training = raw_data_training.astype(np.float16)\n",
    "#     raw_data_testing = raw_data_testing.astype(np.float16)\n",
    "\n",
    "\n",
    "#     # Convert to PyTorch tensors\n",
    "#     features = torch.FloatTensor(raw_data_training.drop(columns=['target']).values).to(device)\n",
    "#     labels = torch.LongTensor(raw_data_training['target'].values).to(device)\n",
    "\n",
    "#     # Create dataset and dataloader\n",
    "#     trainingDataset = TensorDataset(features, labels)\n",
    "#     trainingDataloader = DataLoader(trainingDataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "#     # Hyperparameters\n",
    "#     latent_dim = raw_data_training.shape[1] - 1\n",
    "#     # latent_dim = 100\n",
    "#     num_features = raw_data_training.shape[1] - 1\n",
    "#     num_classes = 2\n",
    "#     lr = 0.0002\n",
    "#     num_epochs = 100\n",
    "\n",
    "#     print(f'\\n\\nJVGAN parameters: Latent Dimension = {latent_dim}, num_features = {num_features}, lr={lr}', end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "#     # # Check if the models exist\n",
    "#     # generator_model_path = 'generator.pth'\n",
    "#     # discriminator_model_path = 'discriminator.pth'\n",
    "\n",
    "#     # # Initialize the models if they don't exist, otherwise load the saved models\n",
    "#     # if os.path.exists(generator_model_path) and os.path.exists(discriminator_model_path):\n",
    "#     #     # Load pre-trained models\n",
    "#     #     generator = Generator(latent_dim, num_features, num_classes).to(device)\n",
    "#     #     discriminator = Discriminator(num_features, num_classes).to(device)\n",
    "        \n",
    "#     #     # Load the state dicts for both generator and discriminator\n",
    "#     #     generator.load_state_dict(torch.load(generator_model_path))\n",
    "#     #     discriminator.load_state_dict(torch.load(discriminator_model_path))\n",
    "        \n",
    "#     #     print(\"Loaded pre-trained generator and discriminator models.\")\n",
    "#     # else:\n",
    "#     #     # Initialize models if they don't exist\n",
    "#     #     generator = Generator(latent_dim, num_features, num_classes).to(device)\n",
    "#     #     discriminator = Discriminator(num_features, num_classes).to(device)\n",
    "        \n",
    "#     #     print(\"Initialized new generator and discriminator models.\")\n",
    "\n",
    "#     # Initialize models\n",
    "#     generator = Generator(latent_dim, num_features, num_classes).to(device)\n",
    "#     discriminator = Discriminator(num_features, num_classes).to(device)\n",
    "\n",
    "#     # Optimizers\n",
    "#     g_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "#     d_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "#     # Loss function\n",
    "#     criterion = nn.BCELoss()\n",
    "\n",
    "#     # Training loop\n",
    "#     for epoch in range(num_epochs):\n",
    "#         for i, (real_data, real_labels) in enumerate(trainingDataloader):\n",
    "#             batch_size = real_data.size(0)\n",
    "            \n",
    "#             # Train Discriminator\n",
    "#             d_optimizer.zero_grad()\n",
    "            \n",
    "#             # Real data\n",
    "#             real_labels_onehot = nn.functional.one_hot(real_labels, num_classes).float().to(device)\n",
    "#             real_validity = discriminator(real_data, real_labels_onehot)\n",
    "#             d_real_loss = criterion(real_validity, torch.ones_like(real_validity).to(device))\n",
    "            \n",
    "#             # Fake data\n",
    "#             z = torch.randn(batch_size, latent_dim).to(device)\n",
    "#             fake_labels = torch.randint(0, num_classes, (batch_size,)).to(device)\n",
    "#             fake_labels_onehot = nn.functional.one_hot(fake_labels, num_classes).float().to(device)\n",
    "#             fake_data = generator(z, fake_labels_onehot)\n",
    "#             fake_validity = discriminator(fake_data.detach(), fake_labels_onehot)\n",
    "#             d_fake_loss = criterion(fake_validity, torch.zeros_like(fake_validity).to(device))\n",
    "            \n",
    "#             d_loss = d_real_loss + d_fake_loss\n",
    "#             d_loss.backward()\n",
    "#             d_optimizer.step()\n",
    "            \n",
    "#             # Train Generator\n",
    "#             g_optimizer.zero_grad()\n",
    "            \n",
    "#             z = torch.randn(batch_size, latent_dim).to(device)\n",
    "#             fake_labels = torch.randint(0, num_classes, (batch_size,)).to(device)\n",
    "#             fake_labels_onehot = nn.functional.one_hot(fake_labels, num_classes).float().to(device)\n",
    "#             fake_data = generator(z, fake_labels_onehot)\n",
    "#             fake_validity = discriminator(fake_data, fake_labels_onehot)\n",
    "#             g_loss = criterion(fake_validity, torch.ones_like(fake_validity).to(device))\n",
    "            \n",
    "#             g_loss.backward()\n",
    "#             g_optimizer.step()\n",
    "            \n",
    "#             if i % 100 == 0:\n",
    "#                 print(f\"Epoch [{epoch}/{num_epochs}] Batch [{i}/{len(trainingDataloader)}] \"\n",
    "#                     f\"D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "#     # Testing\n",
    "#     # Convert to PyTorch tensors\n",
    "#     test_z = torch.FloatTensor(raw_data_testing.values[:, :-1]).to(device)\n",
    "#     test_labels = torch.LongTensor(raw_data_testing['target'].values).to(device)\n",
    "#     test_labels_onehot = nn.functional.one_hot(test_labels, num_classes).float().to(device)\n",
    "#     test_data = generator(test_z, test_labels_onehot)\n",
    "\n",
    "#     torch.save(generator.state_dict(), 'generator.pth')\n",
    "#     torch.save(discriminator.state_dict(), 'discriminator.pth')\n",
    "\n",
    "#     # Evaluate the generated data for anomaly detection\n",
    "#     def evaluate_anomaly_detection(generator, real_data, labels, num_samples=1000):\n",
    "#         # Generate synthetic data\n",
    "#         z = torch.randn(num_samples, latent_dim).to(device)\n",
    "#         synthetic_labels = torch.randint(0, num_classes, (num_samples,)).to(device)\n",
    "#         synthetic_labels_onehot = nn.functional.one_hot(synthetic_labels, num_classes).float().to(device)\n",
    "#         synthetic_data = generator(z, synthetic_labels_onehot)\n",
    "        \n",
    "#         # Combine real and synthetic data\n",
    "#         all_data = torch.cat([real_data, synthetic_data], dim=0)\n",
    "#         all_labels = torch.cat([labels, synthetic_labels], dim=0)\n",
    "        \n",
    "#         # Use discriminator to classify real vs synthetic\n",
    "#         with torch.no_grad():\n",
    "#             predictions = discriminator(all_data, nn.functional.one_hot(all_labels, num_classes).float().to(device))\n",
    "        \n",
    "#         # Convert predictions to binary (0 for synthetic, 1 for real)\n",
    "#         predictions = (predictions > 0.5).float()\n",
    "        \n",
    "#         # Calculate anomaly detection metrics\n",
    "#         real_labels = torch.ones(real_data.size(0)).to(device)\n",
    "#         synthetic_labels = torch.zeros(synthetic_data.size(0)).to(device)\n",
    "#         true_labels = torch.cat([real_labels, synthetic_labels], dim=0)\n",
    "        \n",
    "#         auc_roc = roc_auc_score(true_labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "#         precision, recall, f1, _ = precision_recall_fscore_support(true_labels.cpu().numpy(), predictions.cpu().numpy(), average='binary')\n",
    "        \n",
    "#         return auc_roc, precision, recall, f1\n",
    "\n",
    "\n",
    "#     # Evaluate anomaly detection performance\n",
    "#     auc_roc, precision, recall, f1 = evaluate_anomaly_detection(generator, features, labels)\n",
    "    \n",
    "#     print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "#     print(f\"Precision: {precision:.4f}\")\n",
    "#     print(f\"Recall: {recall:.4f}\")\n",
    "#     print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "\n",
    "#     AUC_ROC.append(auc_roc)\n",
    "#     PRECISION.append(precision)\n",
    "#     RECALL.append(recall)\n",
    "#     F1_SCORE.append(f1)\n",
    "\n",
    "# print(f\"AUC-ROC: {AUC_ROC}\")\n",
    "# print(f\"Precision: {PRECISION}\")\n",
    "# print(f\"Recall: {RECALL}\")\n",
    "# print(f\"F1-Score: {F1_SCORE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            application_feature_0  application_feature_1  \\\n",
      "2025-04-01               1.372963               0.502080   \n",
      "2025-04-02              -0.139897               0.775624   \n",
      "2025-04-07              -0.891497              -0.171421   \n",
      "2025-04-08              -0.006526               0.807843   \n",
      "2025-04-09               0.165892               0.026686   \n",
      "\n",
      "            application_feature_10  application_feature_11  \\\n",
      "2025-04-01               -0.404063               -0.809919   \n",
      "2025-04-02                1.445440               -0.133404   \n",
      "2025-04-07                0.680433               -0.598605   \n",
      "2025-04-08                0.460853               -1.269932   \n",
      "2025-04-09               -0.892247                0.482648   \n",
      "\n",
      "            application_feature_12  application_feature_13  \\\n",
      "2025-04-01                0.551614                0.773489   \n",
      "2025-04-02                0.017327               -0.588120   \n",
      "2025-04-07               -0.330304                0.134943   \n",
      "2025-04-08               -0.433721                0.419370   \n",
      "2025-04-09                1.426578                0.148748   \n",
      "\n",
      "            application_feature_14  application_feature_15  \\\n",
      "2025-04-01               -0.851345                1.528463   \n",
      "2025-04-02                0.865907                0.219909   \n",
      "2025-04-07               -2.394466                0.602044   \n",
      "2025-04-08                0.139707               -0.492889   \n",
      "2025-04-09               -1.246995               -0.144445   \n",
      "\n",
      "            application_feature_16  application_feature_17  ...  \\\n",
      "2025-04-01               -0.239123               -0.029531  ...   \n",
      "2025-04-02                0.689044                1.502454  ...   \n",
      "2025-04-07               -0.949885               -0.820035  ...   \n",
      "2025-04-08               -0.378457                0.300408  ...   \n",
      "2025-04-09                0.356460               -0.237478  ...   \n",
      "\n",
      "            application_feature_5  application_feature_6  \\\n",
      "2025-04-01              -0.326723               0.071286   \n",
      "2025-04-02               0.737958              -0.373596   \n",
      "2025-04-07              -0.571011              -0.120571   \n",
      "2025-04-08              -0.449971               0.211492   \n",
      "2025-04-09              -0.040478              -0.126648   \n",
      "\n",
      "            application_feature_7  application_feature_8  \\\n",
      "2025-04-01              -0.758095              -0.560427   \n",
      "2025-04-02               0.624246               2.766422   \n",
      "2025-04-07               0.064652               0.649848   \n",
      "2025-04-08               0.904474              -1.397754   \n",
      "2025-04-09               1.170597               0.593032   \n",
      "\n",
      "            application_feature_9      host    srscu0  srscu0_stressType  \\\n",
      "2025-04-01              -0.735403  0.496714  1.399355                  1   \n",
      "2025-04-02               0.348270 -0.138264  0.924634                  2   \n",
      "2025-04-07              -0.064242  1.579213  0.895193                  1   \n",
      "2025-04-08               0.774491  0.767435  0.635172                  0   \n",
      "2025-04-09              -0.020661 -0.469474  1.049553                  2   \n",
      "\n",
      "              srsdu0  srsdu0_stressType  \n",
      "2025-04-01 -0.423760                  0  \n",
      "2025-04-02 -0.453414                  3  \n",
      "2025-04-07  1.048483                  1  \n",
      "2025-04-08  0.487775                  2  \n",
      "2025-04-09 -0.734233                  3  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "(638, 23)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['srscu0_stressType', 'srsdu0_stressType'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 43\u001b[0m\n\u001b[1;32m     38\u001b[0m X_features, X_targets, y_features, y_targets \u001b[38;5;241m=\u001b[39m create_sequences(features_pca, lookback, lookforward, combined_samples[(CU, DU)][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# print(f\"X_features shape: {X_features.shape}\")\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# print(f\"y_features shape: {y_features.shape}\")\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# print(f\"y_targets shape: {y_targets.shape}\")\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m X_original_features, X_targets, y_original_features, y_targets \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined_samples\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDU\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtargets\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# print(f\"X_original_features shape: {X_original_features.shape}\")\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# print(f\"y_original_features shape: {y_original_features.shape}\")\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# print(f\"y_targets shape: {y_targets.shape}\")\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Convert to PyTorch tensors\u001b[39;00m\n\u001b[1;32m     49\u001b[0m X_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(X_features)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[53], line 27\u001b[0m, in \u001b[0;36mcreate_sequences\u001b[0;34m(data, seq_length, horizon, combined_samples_targets)\u001b[0m\n\u001b[1;32m     24\u001b[0m X_seq \u001b[38;5;241m=\u001b[39m data[feature_names]\u001b[38;5;241m.\u001b[39miloc[i:i \u001b[38;5;241m+\u001b[39m seq_length]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# X_targets should contain only the 'target' column\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m X_targets_seq \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcombined_samples_targets\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39miloc[i:i \u001b[38;5;241m+\u001b[39m seq_length]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# y should contain the entire row for each sequence, except 'target'\u001b[39;00m\n\u001b[1;32m     30\u001b[0m y_seq \u001b[38;5;241m=\u001b[39m data[feature_names]\u001b[38;5;241m.\u001b[39miloc[i \u001b[38;5;241m+\u001b[39m seq_length \u001b[38;5;241m+\u001b[39m horizon \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['srscu0_stressType', 'srsdu0_stressType'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "CU=\"srscu0\"\n",
    "DU=\"srsdu0\"\n",
    "\n",
    "raw_data = combined_samples[(CU, DU)]['features']\n",
    "raw_data = dataset[raw_data + combined_samples[(CU, DU)]['targets']].copy()\n",
    "raw_data = raw_data.loc[:, ~raw_data.columns.duplicated()]  # KEY FIX\n",
    "# Filter using targets\n",
    "for target_col in combined_samples[(CU, DU)]['targets']:\n",
    "    raw_data = raw_data[raw_data[target_col].isin([0, 1, 2, 3])]\n",
    "\n",
    "sorted_feature_names = sorted(raw_data.columns.tolist())\n",
    "raw_data = raw_data[sorted_feature_names]\n",
    "\n",
    "print(raw_data.head())\n",
    "\n",
    "# Scale features and apply PCA\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "features_scaled = scaler.fit_transform(raw_data.drop(columns=combined_samples[(CU, DU)]['targets']))\n",
    "features_scaled = pd.DataFrame(features_scaled, columns=raw_data.drop(columns=combined_samples[(CU, DU)]['targets']).columns.tolist())\n",
    "\n",
    "pca = PCA(n_components=PCA_DIMENSION)\n",
    "print(features_scaled.shape)\n",
    "# Apply PCA\n",
    "features_pca = pca.fit_transform(features_scaled)\n",
    "# Convert to PyTorch tensors\n",
    "features = torch.FloatTensor(features_pca).to(device)\n",
    "labels = torch.LongTensor(raw_data[combined_samples[(CU, DU)]['targets']].values).to(device)\n",
    "\n",
    "features_pca = pd.DataFrame(features_pca, columns=[f'pca_{i}' for i in range(PCA_DIMENSION)])\n",
    "\n",
    "#combine the target column with the pca features\n",
    "for target in combined_samples[(CU, DU)]['targets']:\n",
    "    features_pca[target] = raw_data[target].values\n",
    "\n",
    "features_pca = features_pca.dropna()\n",
    "features_pca = features_pca.astype(np.float16)\n",
    "\n",
    "X_features, X_targets, y_features, y_targets = create_sequences(features_pca, lookback, lookforward, combined_samples[(CU, DU)]['targets'])\n",
    "\n",
    "# print(f\"X_features shape: {X_features.shape}\")\n",
    "# print(f\"y_features shape: {y_features.shape}\")\n",
    "# print(f\"y_targets shape: {y_targets.shape}\")\n",
    "X_original_features, X_targets, y_original_features, y_targets = create_sequences(features_scaled, lookback, lookforward, combined_samples[(CU, DU)]['targets'])\n",
    "# print(f\"X_original_features shape: {X_original_features.shape}\")\n",
    "# print(f\"y_original_features shape: {y_original_features.shape}\")\n",
    "# print(f\"y_targets shape: {y_targets.shape}\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_features = torch.FloatTensor(X_features).to(device)\n",
    "X_targets = torch.FloatTensor(X_targets).to(device)\n",
    "y_features = torch.FloatTensor(y_features).to(device)\n",
    "y_targets = torch.FloatTensor(y_targets).to(device)\n",
    "\n",
    "X_original_features = torch.FloatTensor(X_original_features).to(device)\n",
    "X_original_features = X_original_features.view(X_original_features.size(0), -1)\n",
    "y_original_features = torch.FloatTensor(y_original_features).to(device)\n",
    "y_original_features = y_original_features.view(y_original_features.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in dataset: 17\n",
      "                host    srscu0    srscu1    srscu2    srscu3    srsdu0  \\\n",
      "2025-04-01  0.496714 -1.415371  0.357787 -0.828995 -1.594428  0.926178   \n",
      "2025-04-02 -0.138264 -0.420645  0.560785 -0.560181 -0.599375  1.909417   \n",
      "2025-04-03  0.647689 -0.342715  1.083051  0.747294  0.005244 -1.398568   \n",
      "2025-04-04  1.523030 -0.802277  1.053802  0.610370  0.046981  0.562969   \n",
      "2025-04-05 -0.234153 -0.161286 -1.377669 -0.020902 -0.450065 -0.650643   \n",
      "\n",
      "              srsdu1    srsdu2    srsdu3  srscu0_stressType  \\\n",
      "2025-04-01  0.756989 -0.522723  0.938284                  1   \n",
      "2025-04-02 -0.922165  1.049009 -0.516045                  3   \n",
      "2025-04-03  0.869606 -0.704344  0.096121                  1   \n",
      "2025-04-04  1.355638 -1.408461 -0.462275                  4   \n",
      "2025-04-05  0.413435 -1.556629 -0.434496                  4   \n",
      "\n",
      "            srscu1_stressType  srscu2_stressType  srscu3_stressType  \\\n",
      "2025-04-01                  4                  4                  4   \n",
      "2025-04-02                  2                  2                  3   \n",
      "2025-04-03                  1                  3                  4   \n",
      "2025-04-04                  3                  1                  0   \n",
      "2025-04-05                  2                  1                  0   \n",
      "\n",
      "            srsdu0_stressType  srsdu1_stressType  srsdu2_stressType  \\\n",
      "2025-04-01                  0                  1                  3   \n",
      "2025-04-02                  4                  2                  3   \n",
      "2025-04-03                  0                  4                  0   \n",
      "2025-04-04                  3                  3                  0   \n",
      "2025-04-05                  1                  1                  4   \n",
      "\n",
      "            srsdu3_stressType  \n",
      "2025-04-01                  1  \n",
      "2025-04-02                  4  \n",
      "2025-04-03                  0  \n",
      "2025-04-04                  2  \n",
      "2025-04-05                  0  \n"
     ]
    }
   ],
   "source": [
    "feature_names = dataset.columns\n",
    "timestamps = dataset.index\n",
    "\n",
    "\n",
    "# # Filter out samples where the target is not in {0, 1, 2, 3}\n",
    "# dataset = dataset[dataset['target'].isin([0, 1, 2, 3])]\n",
    "\n",
    "\n",
    "# # Separate features and target\n",
    "# target_col = 'target'\n",
    "# target = dataset[target_col]\n",
    "# original_features = dataset.drop(columns=['Timestamp'], errors='ignore')\n",
    "\n",
    "print(f\"Number of columns in dataset: {len(dataset.columns)}\")\n",
    "print(dataset.head())\n",
    "\n",
    "# # Binarize the target column\n",
    "# target = dataset[target_col].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "# # print(f\"Number of columns in dataset: {len(original_features.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#combine the target column with the pca features\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(features_pca)):\n\u001b[0;32m---> 17\u001b[0m     features_pca\u001b[38;5;241m.\u001b[39mat[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m[i]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# print(features_pca.head())\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target' is not defined"
     ]
    }
   ],
   "source": [
    "# Scale features and apply PCA\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "features_scaled = scaler.fit_transform(original_features)\n",
    "pca = PCA(n_components=PCA_DIMENSION)\n",
    "features_pca = pca.fit_transform(features_scaled)\n",
    "\n",
    "# print(f\"Number of columns in dataset: {len(features.columns)}\")\n",
    "# print(f\"Number of rows in dataset: {len(features)}\")\n",
    "\n",
    "# print(f\"Number of columns in reduced dataset: {len(features_pca[0])}\")\n",
    "# print(f\"Number of rows in reduced dataset: {len(features_pca)}\")\n",
    "\n",
    "features_pca = pd.DataFrame(features_pca, columns=[f'pca_{i}' for i in range(PCA_DIMENSION)])\n",
    "\n",
    "#combine the target column with the pca features\n",
    "for i in range(len(features_pca)):\n",
    "    features_pca.at[i, 'target'] = target[i]\n",
    "\n",
    "# print(features_pca.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Prepare Sequences**\n",
    "# Combine the target column with the features\n",
    "original_features['target'] = target\n",
    "\n",
    "X_features, X_targets, y_features, y_targets = create_sequences(features_pca, lookback, lookforward)\n",
    "\n",
    "# print(f\"X_features shape: {X_features.shape}\")\n",
    "# print(f\"y_features shape: {y_features.shape}\")\n",
    "# print(f\"y_targets shape: {y_targets.shape}\")\n",
    "\n",
    "X_original_features, X_targets, y_original_features, y_targets = create_sequences(original_features, lookback, lookforward)\n",
    "\n",
    "\n",
    "# print(f\"X_original_features shape: {X_original_features.shape}\")\n",
    "# print(f\"y_original_features shape: {y_original_features.shape}\")\n",
    "# print(f\"y_targets shape: {y_targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in target: [1]\n",
      "\n",
      "Overall class distribution: {1: 435}\n"
     ]
    }
   ],
   "source": [
    "# **Stratified K-Fold Cross-Validation**\n",
    "n_splits = 2\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize metric accumulators\n",
    "accuracy_scores_actual = []\n",
    "f1_scores_actual = []\n",
    "classification_reports_actual = []\n",
    "accuracy_scores_forecasted = []\n",
    "f1_scores_forecasted = []\n",
    "classification_reports_forecasted = []\n",
    "total_conf_matrix_actual = None\n",
    "total_conf_matrix_forecasted = None\n",
    "RMSE_PCA = []\n",
    "\n",
    "print(\"Unique values in target:\", target.unique())\n",
    "print(\"\\nOverall class distribution:\", dict(zip(*np.unique(y_targets, return_counts=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim, sequence_length, hidden_dim, output_dim):\n",
    "        super(SequentialGenerator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # LSTM to process the sequence data\n",
    "        self.lstm = nn.LSTM(input_size=latent_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True)\n",
    "        \n",
    "        # Linear layer to map LSTM output to the desired output dimension\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, z, previous_sequence):\n",
    "        # z shape: (batch_size, latent_dim)\n",
    "        # previous_sequence shape: (batch_size, sequence_length)\n",
    "        \n",
    "        # Repeat z across the sequence length dimension\n",
    "        z_repeated = z.unsqueeze(1).repeat(1, self.sequence_length, 1)\n",
    "        \n",
    "        # LSTM to generate new sequence\n",
    "        lstm_out, _ = self.lstm(z_repeated)\n",
    "        \n",
    "        # Output layer to generate the next time step\n",
    "        generated_output = self.fc(lstm_out[:, -1, :])\n",
    "        \n",
    "        return generated_output\n",
    "\n",
    "class SequentialDiscriminator(nn.Module):\n",
    "    def __init__(self, sequence_length, hidden_dim, input_size):  # Changed parameter name\n",
    "        super(SequentialDiscriminator, self).__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Match input_size to actual feature dimension (100)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        lstm_out, _ = self.lstm(sequence)  # lstm_out shape: [batch_size, hidden_size]\n",
    "        return self.sigmoid(self.fc(lstm_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 397)) while a minimum of 1 is required by MinMaxScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 75\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# print(f\"JVGAN Features:\\n{JVGAN_features.head()}\")\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Scale features and apply PCA\u001b[39;00m\n\u001b[1;32m     74\u001b[0m JVGAN_scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler(feature_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 75\u001b[0m JVGAN_features_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mJVGAN_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Train JVGAN\u001b[39;00m\n\u001b[1;32m     78\u001b[0m real_sequences \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(JVGAN_features_scaled, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    904\u001b[0m             (\n\u001b[1;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    914\u001b[0m         )\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:447\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 447\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:487\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    484\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m    486\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 487\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_pass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_array_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupported_float_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m data_min \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m    496\u001b[0m data_max \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:1130\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1133\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m   1134\u001b[0m         )\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1137\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 397)) while a minimum of 1 is required by MinMaxScaler."
     ]
    }
   ],
   "source": [
    "# **Cross-Validation Loop**\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X_features, y_targets), start=1):\n",
    "    # Split data into train and test folds\n",
    "    X_train_fold, X_test_fold = X_features[train_idx], X_features[test_idx]\n",
    "    y_train_fold, y_test_fold = y_features[train_idx], y_features[test_idx]\n",
    "\n",
    "    X_original_train_fold, X_original_test_fold = X_original_features[train_idx], X_original_features[test_idx]\n",
    "    y_original_train_fold, y_original_test_fold =y_original_features[train_idx], y_original_features[test_idx]\n",
    "    \n",
    "    y_train_target, y_test_target = y_targets[train_idx], y_targets[test_idx]\n",
    "    \n",
    "    \n",
    "    # Train LSTM model\n",
    "    train_dataset = SequenceDataset(X_train_fold, y_train_fold)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    lstm_model = LSTMForecaster(input_dim, hidden_dim, num_layers, output_dim).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(lstm_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if(not os.path.exists('models')):\n",
    "        os.makedirs('models')\n",
    "    if os.path.exists(f'models/lstm_model_fold_{fold_idx}.pt'):\n",
    "        lstm_model.load_state_dict(torch.load(f\"models/lstm_model_fold_{fold_idx}.pt\"))\n",
    "        lstm_model.eval()\n",
    "    else:\n",
    "        for epoch in range(epochs):\n",
    "            lstm_model.train()\n",
    "            epoch_loss = 0\n",
    "            for batch_sequences, batch_targets in train_loader:\n",
    "                batch_sequences = batch_sequences.to(device, dtype=torch.float32)\n",
    "                batch_targets = batch_targets.to(device, dtype=torch.float32)\n",
    "                optimizer.zero_grad()\n",
    "                predictions = lstm_model(batch_sequences)\n",
    "                d_loss = criterion(predictions, batch_targets)\n",
    "                predictions_inverse = pca.inverse_transform(predictions.cpu().detach().numpy())\n",
    "                predictions_inverse_tensor = torch.tensor(predictions_inverse, dtype=torch.float32).to(device)\n",
    "                batch_inverse_targets = pca.inverse_transform(batch_targets.cpu().detach().numpy())\n",
    "                batch_inverse_targets_tensor = torch.tensor(batch_inverse_targets, dtype=torch.float32).to(device)\n",
    "                o_loss = criterion(predictions_inverse_tensor, batch_inverse_targets_tensor)\n",
    "                final_loss = d_loss + o_loss / 30\n",
    "                final_loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += final_loss.item()\n",
    "        torch.save(lstm_model.state_dict(), f\"models/lstm_model_fold_{fold_idx}.pt\")\n",
    "    \n",
    "    # Forecasting with LSTM\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.tensor(X_test_fold, dtype=torch.float32).to(device)\n",
    "        y_pred_test = lstm_model(X_test_tensor)\n",
    "        y_pred_pca_inverse = pca.inverse_transform(y_pred_test.cpu().numpy())\n",
    "        y_true_pca_inverse = pca.inverse_transform(y_test_fold)\n",
    "        rmse_pca = np.sqrt(np.mean((y_true_pca_inverse - y_pred_pca_inverse) ** 2))\n",
    "        RMSE_PCA.append(rmse_pca)\n",
    "        y_pred_test_original = scaler.inverse_transform(y_pred_pca_inverse)\n",
    "\n",
    "\n",
    "    JVGAN_features = []\n",
    "    # Filter normal data where target == 0\n",
    "    original_features['Timestamp'] =  timestamps\n",
    "\n",
    "    JVGAN_features = original_features[original_features['target'] == 0]\n",
    "    \n",
    "    # Ensure the data is sorted by timestamp if it's not already\n",
    "    JVGAN_features = JVGAN_features.sort_values(by='Timestamp')\n",
    "\n",
    "    # Drop the target column as it is not needed for JVGAN training\n",
    "    JVGAN_features = JVGAN_features.drop(columns=['target'])\n",
    "\n",
    "    # anomaly detection using JVGAN\n",
    "    JVGAN_features = pd.DataFrame(JVGAN_features)\n",
    "    # print(f\"JVGAN Features:\\n{JVGAN_features.head()}\")\n",
    "    \n",
    "    # Scale features and apply PCA\n",
    "    JVGAN_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    JVGAN_features_scaled = scaler.fit_transform(JVGAN_features)\n",
    "\n",
    "    # Train JVGAN\n",
    "    real_sequences = torch.tensor(JVGAN_features_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Hyperparameters\n",
    "    JVGAN_latent_dim = 100  # Latent dimension for noise vector\n",
    "    JVGAN_sequence_length = lookback  # Number of previous time steps to condition on\n",
    "    JVGAN_LSTM_hidden_dim = 128  # Hidden dimension for LSTM layers\n",
    "    JVGAN_output_dim = JVGAN_features_scaled.shape[1]\n",
    "\n",
    "\n",
    "    # Initialize models\n",
    "    generator = SequentialGenerator(JVGAN_latent_dim, JVGAN_sequence_length, JVGAN_LSTM_hidden_dim, JVGAN_output_dim).to(device)\n",
    "    # When creating discriminator:\n",
    "    discriminator = SequentialDiscriminator(\n",
    "        sequence_length=JVGAN_sequence_length,\n",
    "        hidden_dim=JVGAN_LSTM_hidden_dim,\n",
    "        input_size=JVGAN_features_scaled.shape[1]  # Should be 100 for your data\n",
    "    ).to(device)    \n",
    "\n",
    "    # Loss and optimizers\n",
    "    criterion = nn.BCELoss()  # Binary cross-entropy for GAN\n",
    "    optimizer_g = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "    num_epochs = 200\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(0, len(real_sequences) - JVGAN_sequence_length, batch_size):\n",
    "            batch = real_sequences[i:i+batch_size]\n",
    "            \n",
    "            # Train Discriminator\n",
    "            optimizer_d.zero_grad()\n",
    "            \n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "            \n",
    "            real_output = discriminator(batch)\n",
    "            d_loss_real = criterion(real_output, real_labels)\n",
    "            \n",
    "            z = torch.randn(batch_size, JVGAN_latent_dim).to(device)\n",
    "            fake_sequence = generator(z, batch)\n",
    "            fake_output = discriminator(fake_sequence.detach())\n",
    "            d_loss_fake = criterion(fake_output, fake_labels)\n",
    "            \n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            d_loss.backward()\n",
    "            optimizer_d.step()\n",
    "            \n",
    "            # Train Generator\n",
    "            optimizer_g.zero_grad()\n",
    "            \n",
    "            fake_output = discriminator(fake_sequence)\n",
    "            g_loss = criterion(fake_output, real_labels)\n",
    "            \n",
    "            g_loss.backward()\n",
    "            optimizer_g.step()\n",
    "        if(epoch % 10 == 0):\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "    # **Classification on Actual Future Features**\n",
    "    # Anomaly Detection using Discriminator\n",
    "    # with torch.no_grad():\n",
    "    #     test_scores = discriminator(X_test_tensor).numpy().flatten()\n",
    "    #     anomaly_preds = (test_scores < 0.5).astype(int)  # Lower scores indicate anomalies\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # print(f\"X_test_fold shape: {X_original_test_fold.shape}\")\n",
    "        # X_test_tensor is 3D: (batch_size, seq_length, input_dim)\n",
    "        JVGAN_X_test_tensor = torch.tensor(X_original_test_fold, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Select the last timestep\n",
    "        JVGAN_X_last_timestep = JVGAN_X_test_tensor[:, -1, :]  # Shape: (batch_size, input_dim)\n",
    "\n",
    "        # Pass the last timestep to the discriminator\n",
    "        # print(f\"X_last_timestep shape: {JVGAN_X_last_timestep.shape}\")\n",
    "        test_scores = discriminator(JVGAN_X_last_timestep).cpu().numpy().flatten()\n",
    "        anomaly_preds = (test_scores < 0.5).astype(int)  # Lower scores indicate anomalies\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    lstm_model = LSTMForecaster(input_dim, hidden_dim, num_layers, output_dim).to(device)\n",
    "    lstm_model.load_state_dict(torch.load(f\"models/lstm_model_fold_{fold_idx}.pt\", map_location=device, weights_only=False))\n",
    "    lstm_model.eval()\n",
    "\n",
    "    # **Classification on Actual Future Features**\n",
    "    print(f\"\\nClassification Report for Actual Future Features (Fold {fold_idx}):\")\n",
    "    print(classification_report(y_test_target, anomaly_preds))\n",
    "    conf_matrix_actual = confusion_matrix(y_test_target, anomaly_preds)\n",
    "    print(f\"Confusion Matrix for Actual Features (Fold {fold_idx}):\")\n",
    "    print(conf_matrix_actual)\n",
    "    accuracy_actual = accuracy_score(y_test_target, anomaly_preds)\n",
    "    f1_actual = f1_score(y_test_target, anomaly_preds, average='weighted')\n",
    "    accuracy_scores_actual.append(accuracy_actual)\n",
    "    f1_scores_actual.append(f1_actual)\n",
    "    classification_reports_actual.append(classification_report(y_test_target, anomaly_preds, output_dict=True))\n",
    "\n",
    "    # **Classification on Forecasted Features**\n",
    "    y_jvgan_pred_forecasted = discriminator(torch.tensor(y_pred_test_original, dtype=torch.float32).to(device)).cpu().detach().numpy().flatten()\n",
    "    y_jvgan_pred_forecasted = (y_jvgan_pred_forecasted < 0.5).astype(int)  # Lower scores indicate anomalies\n",
    "    print(f\"\\nClassification Report for Forecasted Features (Fold {fold_idx}):\")\n",
    "    print(classification_report(y_test_target, y_jvgan_pred_forecasted))\n",
    "    conf_matrix_forecasted = confusion_matrix(y_test_target, y_jvgan_pred_forecasted)\n",
    "    print(f\"Confusion Matrix for Forecasted Features (Fold {fold_idx}):\")\n",
    "    print(conf_matrix_forecasted)\n",
    "    accuracy_forecasted = accuracy_score(y_test_target, y_jvgan_pred_forecasted)\n",
    "    f1_forecasted = f1_score(y_test_target, y_jvgan_pred_forecasted, average='weighted')\n",
    "    accuracy_scores_forecasted.append(accuracy_forecasted)\n",
    "    f1_scores_forecasted.append(f1_forecasted)\n",
    "    classification_reports_forecasted.append(classification_report(y_test_target, y_jvgan_pred_forecasted, output_dict=True))\n",
    "            \n",
    "    for i in range(len(anomaly_preds)):\n",
    "        if anomaly_preds[i] != 0:\n",
    "            # here add the code for the integrated gradients\n",
    "            # Initialize the IntegratedGradients object\n",
    "            ig = IntegratedGradients(discriminator) # discriminator is the model used for anomaly detection\n",
    "            # Get the input tensor\n",
    "            input_tensor = torch.tensor(y_pred_test_original, dtype=torch.float32).to(device)\n",
    "            # Get the baseline tensor\n",
    "            baseline_tensor = torch.zeros_like(input_tensor)\n",
    "            # Get the attributions\n",
    "            attributions, delta = ig.attribute(input_tensor, baseline_tensor, target=0, return_convergence_delta=True)\n",
    "            \n",
    "            \n",
    "            # Get the attributions as numpy array\n",
    "            attributions = attributions.cpu().detach().numpy()\n",
    "            # Get the delta as numpy array\n",
    "            delta = delta.cpu().detach().numpy()\n",
    "\n",
    "            if not os.path.exists('RCA.csv'):\n",
    "                with open('RCA.csv', mode='w', newline='') as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    # create a dictionary with feature_names and write attributions sequentially\n",
    "                    attributions_dict = dict(zip(feature_names, attributions[i]))\n",
    "                    attributions_dict['predicted_target'] = anomaly_preds[i]\n",
    "                    writer.writerow(attributions_dict.keys())  # Write the column names (keys)\n",
    "\n",
    "            with open('RCA.csv', mode='a', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                # create a dictionary with feature_names and write attributions sequentially\n",
    "                attributions_dict = dict(zip(feature_names, attributions[i]))\n",
    "                attributions_dict['predicted_target'] = anomaly_preds[i]\n",
    "                writer.writerow(attributions_dict.values())\n",
    "\n",
    "    # Accumulate confusion matrices\n",
    "    if total_conf_matrix_actual is None:\n",
    "        total_conf_matrix_actual = conf_matrix_actual\n",
    "    else:\n",
    "        total_conf_matrix_actual += conf_matrix_actual\n",
    "\n",
    "    if total_conf_matrix_forecasted is None:\n",
    "        total_conf_matrix_forecasted = conf_matrix_forecasted\n",
    "    else:\n",
    "        total_conf_matrix_forecasted += conf_matrix_forecasted\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Compute Average Metrics**\n",
    "avg_accuracy_actual = np.mean(accuracy_scores_actual)\n",
    "avg_f1_actual = np.mean(f1_scores_actual)\n",
    "avg_accuracy_forecasted = np.mean(accuracy_scores_forecasted)\n",
    "avg_f1_forecasted = np.mean(f1_scores_forecasted)\n",
    "avg_rmse_pca = np.mean(RMSE_PCA)\n",
    "avg_conf_matrix_actual = total_conf_matrix_actual / n_splits\n",
    "avg_conf_matrix_forecasted = total_conf_matrix_forecasted / n_splits\n",
    "\n",
    "# Average classification report for actual features\n",
    "avg_report_actual = {}\n",
    "for key in classification_reports_actual[0].keys():\n",
    "    if key not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "        avg_report_actual[key] = {metric: np.mean([r[key][metric] for r in classification_reports_actual])\n",
    "                                    for metric in ['precision', 'recall', 'f1-score']}\n",
    "avg_report_actual['macro avg'] = {metric: np.mean([r['macro avg'][metric] for r in classification_reports_actual])\n",
    "                                    for metric in ['precision', 'recall', 'f1-score']}\n",
    "avg_report_actual['weighted avg'] = {metric: np.mean([r['weighted avg'][metric] for r in classification_reports_actual])\n",
    "                                    for metric in ['precision', 'recall', 'f1-score']}\n",
    "\n",
    "# Average classification report for forecasted features\n",
    "avg_report_forecasted = {}\n",
    "for key in classification_reports_forecasted[0].keys():\n",
    "    if key not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "        avg_report_forecasted[key] = {metric: np.mean([r[key][metric] for r in classification_reports_forecasted])\n",
    "                                        for metric in ['precision', 'recall', 'f1-score']}\n",
    "avg_report_forecasted['macro avg'] = {metric: np.mean([r['macro avg'][metric] for r in classification_reports_forecasted])\n",
    "                                        for metric in ['precision', 'recall', 'f1-score']}\n",
    "avg_report_forecasted['weighted avg'] = {metric: np.mean([r['weighted avg'][metric] for r in classification_reports_forecasted])\n",
    "                                        for metric in ['precision', 'recall', 'f1-score']}\n",
    "\n",
    "# **Display Results**\n",
    "print(\"\\n\\nAverage Metrics for Classification on Actual Future Features:\")\n",
    "print(f\"Average Accuracy: {avg_accuracy_actual}\")\n",
    "print(f\"Average F1-Score: {avg_f1_actual}\")\n",
    "print(\"Average Classification Report:\")\n",
    "for key, metrics in avg_report_actual.items():\n",
    "    if key not in ['macro avg', 'weighted avg']:\n",
    "        print(f\"Class {key}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"  {metric}: {value}\")\n",
    "    else:\n",
    "        print(f\"{key}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"  {metric}: {value}\")\n",
    "print(\"Average Confusion Matrix (Actual Features):\")\n",
    "print(avg_conf_matrix_actual)\n",
    "\n",
    "print(\"\\n\\nAverage Metrics for Classification on Forecasted Features:\")\n",
    "print(f\"Average Accuracy: {avg_accuracy_forecasted}\")\n",
    "print(f\"Average F1-Score: {avg_f1_forecasted}\")\n",
    "print(\"Average Classification Report:\")\n",
    "for key, metrics in avg_report_forecasted.items():\n",
    "    if key not in ['macro avg', 'weighted avg']:\n",
    "        print(f\"Class {key}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"  {metric}: {value}\")\n",
    "    else:\n",
    "        print(f\"{key}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"  {metric}: {value}\")\n",
    "print(f\"\\nAverage RMSE (PCA Inverse): {avg_rmse_pca}\")\n",
    "print(\"Average Confusion Matrix (Forecasted Features):\")\n",
    "print(avg_conf_matrix_forecasted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
