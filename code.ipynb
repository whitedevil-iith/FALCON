{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sampled_data(raw_data, sample_fraction=0.01):\n",
    "    # Extract features and target\n",
    "    target_col = 'target'\n",
    "    target = raw_data[target_col]\n",
    "\n",
    "    # Handle missing target values: either fill with mode or drop rows with NaN in target\n",
    "    target.fillna(target.mode()[0], inplace=True)  # Filling NaN with the mode of the target\n",
    "    \n",
    "    # Initialize StratifiedShuffleSplit to split the data\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=sample_fraction, random_state=42)\n",
    "\n",
    "    # Sample sample_fraction% of data maintaining class distribution\n",
    "    for train_idx, test_idx in sss.split(raw_data, target):\n",
    "        sampled_data = raw_data.iloc[test_idx]\n",
    "\n",
    "    return sampled_data\n",
    "\n",
    "def create_sequences(data, seq_length, horizon):\n",
    "    X, X_targets, y, y_targets = [], [], [], []\n",
    "    feature_names = [col for col in data.columns if col != 'target']  # List of feature column names\n",
    "    \n",
    "    for i in range(len(data) - seq_length - horizon + 1):\n",
    "        # X should contain all columns except 'target' (make sure it's a DataFrame)\n",
    "        X_seq = data[feature_names].iloc[i:i + seq_length]\n",
    "\n",
    "        # X_targets should contain only the 'target' column\n",
    "        X_targets_seq = data['target'].iloc[i:i + seq_length]\n",
    "\n",
    "        # y should contain the entire row for each sequence, except 'target'\n",
    "        y_seq = data[feature_names].iloc[i + seq_length + horizon - 1]\n",
    "        \n",
    "        # y_targets should contain only the 'target' column\n",
    "        y_target = data['target'].iloc[i + seq_length + horizon - 1]\n",
    "        \n",
    "        # Append sequences to the respective lists\n",
    "        X.append(X_seq)\n",
    "        X_targets.append(X_targets_seq)\n",
    "        y.append(y_seq)\n",
    "        y_targets.append(y_target)\n",
    "\n",
    "    return np.array(X), np.array(X_targets), np.array(y), np.array(y_targets)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# **Define LSTM Model**\n",
    "class LSTMForecaster(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTMForecaster, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
    "        c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
    "        out, _ = self.lstm(x, (h_0, c_0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "# **Custom Dataset**\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_dim=10\n",
    "lookforward=5\n",
    "lookback=60\n",
    "PCA_DIMENSION = pca_dim\n",
    "\n",
    "# Model parameters\n",
    "input_dim = PCA_DIMENSION\n",
    "hidden_dim = 32\n",
    "num_layers = 2\n",
    "output_dim = PCA_DIMENSION\n",
    "epochs = 500\n",
    "learning_rate = 0.005\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "raw_data = pd.read_csv('old_prometheus_combine_1.csv')\n",
    "\n",
    "# Define target features to aggregate into a single target column\n",
    "target_features = [\n",
    "    'srsdu1_stressType', 'srsdu2_stressType', 'srsdu0_stressType', 'srsdu3_stressType',\n",
    "    'srscu0_stressType', 'srscu2_stressType', 'srscu3_stressType', 'srscu1_stressType'\n",
    "]\n",
    "\n",
    "# Create a unified target column based on the most frequent non-zero stress type\n",
    "for idx, sample in raw_data.iterrows():\n",
    "    targets = [sample[target] for target in target_features]\n",
    "    non_zero_targets = [value for value in targets if value != 0]\n",
    "    raw_data.at[idx, 'target'] = max(set(non_zero_targets), key=non_zero_targets.count) if non_zero_targets else 0\n",
    "\n",
    "\n",
    "for idx, sample in raw_data.iterrows():\n",
    "    if('key' in raw_data.columns):\n",
    "        for itr in ['key']:  # Loop through the list of columns you want to modify\n",
    "            val = str(sample[itr])\n",
    "            \n",
    "            # Remove the substring 'bsr' from the value\n",
    "            val = val.replace('bsr', '')\n",
    "            \n",
    "            # Convert the modified value to an integer, if possible\n",
    "            try:\n",
    "                raw_data.at[idx, itr] = int(val)\n",
    "            except ValueError:\n",
    "                # Handle the case where the value cannot be converted to an integer\n",
    "                raw_data.at[idx, itr] = 0  # Or set it to some default value\n",
    "\n",
    "# Drop original target feature columns\n",
    "raw_data = raw_data.drop(columns=target_features)\n",
    "\n",
    "# raw_data = load_sampled_data(raw_data, sample_fraction=0.01)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# raw_data = np.random.random((1000, 100))\n",
    "# raw_data = pd.DataFrame(raw_data, columns=[f'feature_{i}' for i in range(100)])\n",
    "# raw_data['target'] = np.random.randint(0, 4, size=(1000,))\n",
    "# raw_data['Timestamp'] = pd.date_range(start='1/1/2020', periods=1000, freq='h')\n",
    "# raw_data['Timestamp'] = raw_data['Timestamp'].astype(str)\n",
    "# raw_data['Timestamp'] = pd.to_datetime(raw_data['Timestamp'])\n",
    "# raw_data = raw_data.set_index('Timestamp')\n",
    "\n",
    "\n",
    "\n",
    "# print(raw_data.head(), raw_data['target'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# raw_data_2 = pd.read_csv('processed_data_0.csv')\n",
    "# raw_data_combined = pd.concat([raw_data, raw_data_2], ignore_index=True)\n",
    "    \n",
    "# print(f\"Number of columns in dataset: {len(raw_data_combined.columns)}\")\n",
    "# print(f\"Number of rows in dataset: {len(raw_data_combined)}\")\n",
    "\n",
    "\n",
    "dataset = raw_data.copy()\n",
    "\n",
    "\n",
    "feature_names = dataset.columns\n",
    "timestamps = dataset.index\n",
    "dataset = dataset.drop(columns=['Timestamp'], errors='ignore')\n",
    "\n",
    "# **Data Preprocessing**\n",
    "# Handle missing values\n",
    "dataset = dataset.apply(lambda x: x.fillna(0) if x.isna().all() else x)\n",
    "threshold = 0.6 * len(dataset)\n",
    "for col in dataset.columns:\n",
    "    if dataset[col].isna().sum() > threshold:\n",
    "        mode_value = dataset[col].mode().iloc[0] if not dataset[col].mode().empty else 0\n",
    "        dataset.fillna({col: mode_value}, inplace=True)\n",
    "#            dataset[col].fillna(mode_value, inplace=True)\n",
    "# dataset = dataset.dropna(subset=['target'])\n",
    "numeric_cols = dataset.select_dtypes(include=[np.number]).columns\n",
    "dataset[numeric_cols] = dataset[numeric_cols].fillna(dataset[numeric_cols].mean())\n",
    "\n",
    "# Filter out samples where the target is not in {0, 1, 2, 3}\n",
    "dataset = dataset[dataset['target'].isin([0, 1, 2, 3])]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Separate features and target\n",
    "target_col = 'target'\n",
    "target = dataset[target_col]\n",
    "original_features = dataset.drop(columns=['Timestamp', target_col], errors='ignore')\n",
    "\n",
    "\n",
    "# Binarize the target column\n",
    "target = dataset[target_col].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "# print(f\"Number of columns in dataset: {len(original_features.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features and apply PCA\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "features_scaled = scaler.fit_transform(original_features)\n",
    "pca = PCA(n_components=PCA_DIMENSION)\n",
    "features_pca = pca.fit_transform(features_scaled)\n",
    "\n",
    "# print(f\"Number of columns in dataset: {len(features.columns)}\")\n",
    "# print(f\"Number of rows in dataset: {len(features)}\")\n",
    "\n",
    "# print(f\"Number of columns in reduced dataset: {len(features_pca[0])}\")\n",
    "# print(f\"Number of rows in reduced dataset: {len(features_pca)}\")\n",
    "\n",
    "features_pca = pd.DataFrame(features_pca, columns=[f'pca_{i}' for i in range(PCA_DIMENSION)])\n",
    "\n",
    "#combine the target column with the pca features\n",
    "for i in range(len(features_pca)):\n",
    "    features_pca.at[i, 'target'] = target[i]\n",
    "\n",
    "# print(features_pca.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Prepare Sequences**\n",
    "# Combine the target column with the features\n",
    "original_features['target'] = target\n",
    "\n",
    "X_features, X_targets, y_features, y_targets = create_sequences(features_pca, lookback, lookforward)\n",
    "\n",
    "# print(f\"X_features shape: {X_features.shape}\")\n",
    "# print(f\"y_features shape: {y_features.shape}\")\n",
    "# print(f\"y_targets shape: {y_targets.shape}\")\n",
    "\n",
    "X_original_features, X_targets, y_original_features, y_targets = create_sequences(original_features, lookback, lookforward)\n",
    "\n",
    "\n",
    "# print(f\"X_original_features shape: {X_original_features.shape}\")\n",
    "# print(f\"y_original_features shape: {y_original_features.shape}\")\n",
    "# print(f\"y_targets shape: {y_targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in target: [1]\n",
      "\n",
      "Overall class distribution: {1: 435}\n"
     ]
    }
   ],
   "source": [
    "# **Stratified K-Fold Cross-Validation**\n",
    "n_splits = 2\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize metric accumulators\n",
    "accuracy_scores_actual = []\n",
    "f1_scores_actual = []\n",
    "classification_reports_actual = []\n",
    "accuracy_scores_forecasted = []\n",
    "f1_scores_forecasted = []\n",
    "classification_reports_forecasted = []\n",
    "total_conf_matrix_actual = None\n",
    "total_conf_matrix_forecasted = None\n",
    "RMSE_PCA = []\n",
    "\n",
    "print(\"Unique values in target:\", target.unique())\n",
    "print(\"\\nOverall class distribution:\", dict(zip(*np.unique(y_targets, return_counts=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim, sequence_length, hidden_dim, output_dim):\n",
    "        super(SequentialGenerator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # LSTM to process the sequence data\n",
    "        self.lstm = nn.LSTM(input_size=latent_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True)\n",
    "        \n",
    "        # Linear layer to map LSTM output to the desired output dimension\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, z, previous_sequence):\n",
    "        # z shape: (batch_size, latent_dim)\n",
    "        # previous_sequence shape: (batch_size, sequence_length)\n",
    "        \n",
    "        # Repeat z across the sequence length dimension\n",
    "        z_repeated = z.unsqueeze(1).repeat(1, self.sequence_length, 1)\n",
    "        \n",
    "        # LSTM to generate new sequence\n",
    "        lstm_out, _ = self.lstm(z_repeated)\n",
    "        \n",
    "        # Output layer to generate the next time step\n",
    "        generated_output = self.fc(lstm_out[:, -1, :])\n",
    "        \n",
    "        return generated_output\n",
    "\n",
    "class SequentialDiscriminator(nn.Module):\n",
    "    def __init__(self, sequence_length, hidden_dim, input_size):  # Changed parameter name\n",
    "        super(SequentialDiscriminator, self).__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Match input_size to actual feature dimension (100)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        lstm_out, _ = self.lstm(sequence)  # lstm_out shape: [batch_size, hidden_size]\n",
    "        return self.sigmoid(self.fc(lstm_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, num_features, num_classes):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Adjust the first layer to match the input dimensions\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + num_classes, 512),  # Adjust this to match input dimensions\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_features),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z, labels):\n",
    "        # Concatenate latent vector and labels\n",
    "        inputs = torch.cat([z, labels], dim=1)\n",
    "        return self.model(inputs)\n",
    "\n",
    "# Define the Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(num_features + num_classes, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        # Concatenate features and labels\n",
    "        inputs = torch.cat([x, labels], dim=1)\n",
    "        return self.model(inputs)\n",
    "\n",
    "NoOfCUs = 4\n",
    "NoOfDUs = 4\n",
    "\n",
    "# Creating Topology\n",
    "topology = {}\n",
    "\n",
    "# Form the graph where srscu0 connects to srsdu0, srscu1 to srsdu1, and so on\n",
    "for i in range(min(NoOfCUs, NoOfDUs)):  # Prevent index errors\n",
    "    topology[f\"srscu{i}\"] = [f\"srsdu{i}\"]\n",
    "\n",
    "# Display the graph\n",
    "print(topology)\n",
    "\n",
    "\n",
    "common_features = dataset.columns.tolist()\n",
    "container_specific_features = {}\n",
    "\n",
    "# Loop through and remove columns containing specific substrings\n",
    "for i in range(NoOfDUs+1):\n",
    "    common_features = [col for col in common_features if f\"srscu{i}\" not in col and f\"srsdu{i}\" not in col]\n",
    "\n",
    "# Store container-specific dataframes instead of lists\n",
    "for i in range(NoOfCUs+1):\n",
    "    container_specific_features[f'srscu{i}'] = dataset[[col for col in dataset.columns.tolist() if f\"srscu{i}\" in col]]\n",
    "\n",
    "for i in range(NoOfDUs+1):\n",
    "    container_specific_features[f'srsdu{i}'] = dataset[[col for col in dataset.columns.tolist() if f\"srsdu{i}\" in col]]\n",
    "\n",
    "# # Print the remaining features\n",
    "# print(len(common_features), common_features)\n",
    "\n",
    "# print(\"Before:\")\n",
    "\n",
    "# # Print container-specific features (as dataframes now)\n",
    "# for i in range(NoOfCUs):\n",
    "#     print(f\"srscu{i}:\")\n",
    "#     print(container_specific_features[f'srscu{i}'].shape)\n",
    "#     print(container_specific_features[f'srscu{i}'].head())\n",
    "\n",
    "# for i in range(NoOfDUs):\n",
    "#     print(f\"srsdu{i}:\")\n",
    "#     print(container_specific_features[f'srsdu{i}'].shape)\n",
    "#     print(container_specific_features[f'srsdu{i}'].head())\n",
    "\n",
    "# Filter out columns containing 'stepStress' from the container-specific dataframes\n",
    "for i in range(NoOfCUs):\n",
    "    container_specific_features[f'srscu{i}'] = container_specific_features[f'srscu{i}'].loc[:, ~container_specific_features[f'srscu{i}'].columns.str.contains('stepStress')]\n",
    "\n",
    "for i in range(NoOfDUs):\n",
    "    container_specific_features[f'srsdu{i}'] = container_specific_features[f'srsdu{i}'].loc[:, ~container_specific_features[f'srsdu{i}'].columns.str.contains('stepStress')]\n",
    "\n",
    "# print(\"After:\")\n",
    "\n",
    "# # Print container-specific features (after filtering)\n",
    "# for i in range(NoOfCUs):\n",
    "#     print(f\"srscu{i}:\")\n",
    "#     print(container_specific_features[f'srscu{i}'].shape)\n",
    "#     print(container_specific_features[f'srscu{i}'].head())\n",
    "\n",
    "# for i in range(NoOfDUs):\n",
    "#     print(f\"srsdu{i}:\")\n",
    "#     print(container_specific_features[f'srsdu{i}'].shape)\n",
    "#     print(container_specific_features[f'srsdu{i}'].head())\n",
    "\n",
    "\n",
    "# Iterate through the topology and combine features\n",
    "combined_samples = {}\n",
    "\n",
    "for CU in topology.keys():\n",
    "    # The CU container-specific features\n",
    "    CU_features = container_specific_features[CU]\n",
    "    \n",
    "    # The connected DUs (from topology)\n",
    "    connected_DUs = topology[CU]\n",
    "    \n",
    "    # Add CU-specific features to the combined list\n",
    "    CU_features_list = CU_features.columns.tolist()\n",
    "    \n",
    "    # Extract the CU stress type column (if exists)\n",
    "    CU_stressType = f'{CU}_stressType' if f'{CU}_stressType' in CU_features.columns else None\n",
    "    \n",
    "    # Add DU-specific features to the combined list for each connected DU\n",
    "    for DU in connected_DUs:\n",
    "        # Ensure DU exists in container_specific_features\n",
    "        if DU in container_specific_features:\n",
    "            DU_features = container_specific_features[DU]\n",
    "            DU_features_list = DU_features.columns.tolist()\n",
    "\n",
    "            # Combine CU and DU features (remove the stress type columns from features)\n",
    "            combined_features = common_features.copy()  # Start with the common features\n",
    "            \n",
    "            # Modify these lines:\n",
    "            combined_features.extend(CU_features_list)  # Keep all CU features\n",
    "            combined_features.extend(DU_features_list)  # Keep all DU features\n",
    "\n",
    "            \n",
    "\n",
    "            # Extract targets and remove them from features\n",
    "            targets = [col for col in combined_features if '_stressType' in col]\n",
    "\n",
    "            # To keep stressType columns temporarily:\n",
    "            combined_samples[(CU, DU)] = {\n",
    "                'features': list(set(combined_features) - set(targets)),  # Include targets in features temporarily\n",
    "                'targets': list(set(targets))\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            print(f\"Error: {DU} not found in container_specific_features!\")\n",
    "            continue  # Skip this DU if not found in container_specific_features\n",
    "    \n",
    "# print(combined_samples)\n",
    "# # Print the results for each CU-DU pair and its combined features\n",
    "# for (CU, DU), sample in combined_samples.items():\n",
    "#     print(f\"Host and CU: {CU}, DU: {DU} - Combined Features:\")\n",
    "#     print(f\"Number of Features: {len(sample['features'])}\")\n",
    "#     print(f\"Number of Targets: {len(sample['targets'])}\")\n",
    "#     print(sample['features'][:10])  # Print first 10 features as a preview\n",
    "#     print(\"----\" * 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 397)) while a minimum of 1 is required by MinMaxScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 75\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# print(f\"JVGAN Features:\\n{JVGAN_features.head()}\")\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Scale features and apply PCA\u001b[39;00m\n\u001b[1;32m     74\u001b[0m JVGAN_scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler(feature_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 75\u001b[0m JVGAN_features_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mJVGAN_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Train JVGAN\u001b[39;00m\n\u001b[1;32m     78\u001b[0m real_sequences \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(JVGAN_features_scaled, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    904\u001b[0m             (\n\u001b[1;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    914\u001b[0m         )\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:447\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 447\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:487\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    484\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m    486\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 487\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_pass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_array_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupported_float_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m data_min \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m    496\u001b[0m data_max \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:1130\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1133\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m   1134\u001b[0m         )\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1137\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 397)) while a minimum of 1 is required by MinMaxScaler."
     ]
    }
   ],
   "source": [
    "# **Cross-Validation Loop**\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X_features, y_targets), start=1):\n",
    "    # Split data into train and test folds\n",
    "    X_train_fold, X_test_fold = X_features[train_idx], X_features[test_idx]\n",
    "    y_train_fold, y_test_fold = y_features[train_idx], y_features[test_idx]\n",
    "\n",
    "    X_original_train_fold, X_original_test_fold = X_original_features[train_idx], X_original_features[test_idx]\n",
    "    y_original_train_fold, y_original_test_fold =y_original_features[train_idx], y_original_features[test_idx]\n",
    "    \n",
    "    y_train_target, y_test_target = y_targets[train_idx], y_targets[test_idx]\n",
    "    \n",
    "    \n",
    "    # Train LSTM model\n",
    "    train_dataset = SequenceDataset(X_train_fold, y_train_fold)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    lstm_model = LSTMForecaster(input_dim, hidden_dim, num_layers, output_dim).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(lstm_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if(not os.path.exists('models')):\n",
    "        os.makedirs('models')\n",
    "    if os.path.exists(f'models/lstm_model_fold_{fold_idx}.pt'):\n",
    "        lstm_model.load_state_dict(torch.load(f\"models/lstm_model_fold_{fold_idx}.pt\"))\n",
    "        lstm_model.eval()\n",
    "    else:\n",
    "        for epoch in range(epochs):\n",
    "            lstm_model.train()\n",
    "            epoch_loss = 0\n",
    "            for batch_sequences, batch_targets in train_loader:\n",
    "                batch_sequences = batch_sequences.to(device, dtype=torch.float32)\n",
    "                batch_targets = batch_targets.to(device, dtype=torch.float32)\n",
    "                optimizer.zero_grad()\n",
    "                predictions = lstm_model(batch_sequences)\n",
    "                d_loss = criterion(predictions, batch_targets)\n",
    "                predictions_inverse = pca.inverse_transform(predictions.cpu().detach().numpy())\n",
    "                predictions_inverse_tensor = torch.tensor(predictions_inverse, dtype=torch.float32).to(device)\n",
    "                batch_inverse_targets = pca.inverse_transform(batch_targets.cpu().detach().numpy())\n",
    "                batch_inverse_targets_tensor = torch.tensor(batch_inverse_targets, dtype=torch.float32).to(device)\n",
    "                o_loss = criterion(predictions_inverse_tensor, batch_inverse_targets_tensor)\n",
    "                final_loss = d_loss + o_loss / 30\n",
    "                final_loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += final_loss.item()\n",
    "        torch.save(lstm_model.state_dict(), f\"models/lstm_model_fold_{fold_idx}.pt\")\n",
    "    \n",
    "    # Forecasting with LSTM\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.tensor(X_test_fold, dtype=torch.float32).to(device)\n",
    "        y_pred_test = lstm_model(X_test_tensor)\n",
    "        y_pred_pca_inverse = pca.inverse_transform(y_pred_test.cpu().numpy())\n",
    "        y_true_pca_inverse = pca.inverse_transform(y_test_fold)\n",
    "        rmse_pca = np.sqrt(np.mean((y_true_pca_inverse - y_pred_pca_inverse) ** 2))\n",
    "        RMSE_PCA.append(rmse_pca)\n",
    "        y_pred_test_original = scaler.inverse_transform(y_pred_pca_inverse)\n",
    "\n",
    "\n",
    "    JVGAN_features = []\n",
    "    # Filter normal data where target == 0\n",
    "    original_features['Timestamp'] =  timestamps\n",
    "\n",
    "    JVGAN_features = original_features[original_features['target'] == 0]\n",
    "    \n",
    "    # Ensure the data is sorted by timestamp if it's not already\n",
    "    JVGAN_features = JVGAN_features.sort_values(by='Timestamp')\n",
    "\n",
    "    # Drop the target column as it is not needed for JVGAN training\n",
    "    JVGAN_features = JVGAN_features.drop(columns=['target'])\n",
    "\n",
    "    # anomaly detection using JVGAN\n",
    "    JVGAN_features = pd.DataFrame(JVGAN_features)\n",
    "    # print(f\"JVGAN Features:\\n{JVGAN_features.head()}\")\n",
    "    \n",
    "    # Scale features and apply PCA\n",
    "    JVGAN_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    JVGAN_features_scaled = scaler.fit_transform(JVGAN_features)\n",
    "\n",
    "    # Train JVGAN\n",
    "    real_sequences = torch.tensor(JVGAN_features_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Hyperparameters\n",
    "    JVGAN_latent_dim = 100  # Latent dimension for noise vector\n",
    "    JVGAN_sequence_length = lookback  # Number of previous time steps to condition on\n",
    "    JVGAN_LSTM_hidden_dim = 128  # Hidden dimension for LSTM layers\n",
    "    JVGAN_output_dim = JVGAN_features_scaled.shape[1]\n",
    "\n",
    "\n",
    "    # Initialize models\n",
    "    generator = SequentialGenerator(JVGAN_latent_dim, JVGAN_sequence_length, JVGAN_LSTM_hidden_dim, JVGAN_output_dim).to(device)\n",
    "    # When creating discriminator:\n",
    "    discriminator = SequentialDiscriminator(\n",
    "        sequence_length=JVGAN_sequence_length,\n",
    "        hidden_dim=JVGAN_LSTM_hidden_dim,\n",
    "        input_size=JVGAN_features_scaled.shape[1]  # Should be 100 for your data\n",
    "    ).to(device)    \n",
    "\n",
    "    # Loss and optimizers\n",
    "    criterion = nn.BCELoss()  # Binary cross-entropy for GAN\n",
    "    optimizer_g = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "    num_epochs = 200\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(0, len(real_sequences) - JVGAN_sequence_length, batch_size):\n",
    "            batch = real_sequences[i:i+batch_size]\n",
    "            \n",
    "            # Train Discriminator\n",
    "            optimizer_d.zero_grad()\n",
    "            \n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "            \n",
    "            real_output = discriminator(batch)\n",
    "            d_loss_real = criterion(real_output, real_labels)\n",
    "            \n",
    "            z = torch.randn(batch_size, JVGAN_latent_dim).to(device)\n",
    "            fake_sequence = generator(z, batch)\n",
    "            fake_output = discriminator(fake_sequence.detach())\n",
    "            d_loss_fake = criterion(fake_output, fake_labels)\n",
    "            \n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            d_loss.backward()\n",
    "            optimizer_d.step()\n",
    "            \n",
    "            # Train Generator\n",
    "            optimizer_g.zero_grad()\n",
    "            \n",
    "            fake_output = discriminator(fake_sequence)\n",
    "            g_loss = criterion(fake_output, real_labels)\n",
    "            \n",
    "            g_loss.backward()\n",
    "            optimizer_g.step()\n",
    "        if(epoch % 10 == 0):\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "    # **Classification on Actual Future Features**\n",
    "    # Anomaly Detection using Discriminator\n",
    "    # with torch.no_grad():\n",
    "    #     test_scores = discriminator(X_test_tensor).numpy().flatten()\n",
    "    #     anomaly_preds = (test_scores < 0.5).astype(int)  # Lower scores indicate anomalies\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # print(f\"X_test_fold shape: {X_original_test_fold.shape}\")\n",
    "        # X_test_tensor is 3D: (batch_size, seq_length, input_dim)\n",
    "        JVGAN_X_test_tensor = torch.tensor(X_original_test_fold, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Select the last timestep\n",
    "        JVGAN_X_last_timestep = JVGAN_X_test_tensor[:, -1, :]  # Shape: (batch_size, input_dim)\n",
    "\n",
    "        # Pass the last timestep to the discriminator\n",
    "        # print(f\"X_last_timestep shape: {JVGAN_X_last_timestep.shape}\")\n",
    "        test_scores = discriminator(JVGAN_X_last_timestep).cpu().numpy().flatten()\n",
    "        anomaly_preds = (test_scores < 0.5).astype(int)  # Lower scores indicate anomalies\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    lstm_model = LSTMForecaster(input_dim, hidden_dim, num_layers, output_dim).to(device)\n",
    "    lstm_model.load_state_dict(torch.load(f\"models/lstm_model_fold_{fold_idx}.pt\", map_location=device, weights_only=False))\n",
    "    lstm_model.eval()\n",
    "\n",
    "    # **Classification on Actual Future Features**\n",
    "    print(f\"\\nClassification Report for Actual Future Features (Fold {fold_idx}):\")\n",
    "    print(classification_report(y_test_target, anomaly_preds))\n",
    "    conf_matrix_actual = confusion_matrix(y_test_target, anomaly_preds)\n",
    "    print(f\"Confusion Matrix for Actual Features (Fold {fold_idx}):\")\n",
    "    print(conf_matrix_actual)\n",
    "    accuracy_actual = accuracy_score(y_test_target, anomaly_preds)\n",
    "    f1_actual = f1_score(y_test_target, anomaly_preds, average='weighted')\n",
    "    accuracy_scores_actual.append(accuracy_actual)\n",
    "    f1_scores_actual.append(f1_actual)\n",
    "    classification_reports_actual.append(classification_report(y_test_target, anomaly_preds, output_dict=True))\n",
    "\n",
    "    # **Classification on Forecasted Features**\n",
    "    y_jvgan_pred_forecasted = discriminator(torch.tensor(y_pred_test_original, dtype=torch.float32).to(device)).cpu().detach().numpy().flatten()\n",
    "    y_jvgan_pred_forecasted = (y_jvgan_pred_forecasted < 0.5).astype(int)  # Lower scores indicate anomalies\n",
    "    print(f\"\\nClassification Report for Forecasted Features (Fold {fold_idx}):\")\n",
    "    print(classification_report(y_test_target, y_jvgan_pred_forecasted))\n",
    "    conf_matrix_forecasted = confusion_matrix(y_test_target, y_jvgan_pred_forecasted)\n",
    "    print(f\"Confusion Matrix for Forecasted Features (Fold {fold_idx}):\")\n",
    "    print(conf_matrix_forecasted)\n",
    "    accuracy_forecasted = accuracy_score(y_test_target, y_jvgan_pred_forecasted)\n",
    "    f1_forecasted = f1_score(y_test_target, y_jvgan_pred_forecasted, average='weighted')\n",
    "    accuracy_scores_forecasted.append(accuracy_forecasted)\n",
    "    f1_scores_forecasted.append(f1_forecasted)\n",
    "    classification_reports_forecasted.append(classification_report(y_test_target, y_jvgan_pred_forecasted, output_dict=True))\n",
    "            \n",
    "    for i in range(len(anomaly_preds)):\n",
    "        if anomaly_preds[i] != 0:\n",
    "            # here add the code for the integrated gradients\n",
    "            # Initialize the IntegratedGradients object\n",
    "            ig = IntegratedGradients(discriminator) # discriminator is the model used for anomaly detection\n",
    "            # Get the input tensor\n",
    "            input_tensor = torch.tensor(y_pred_test_original, dtype=torch.float32).to(device)\n",
    "            # Get the baseline tensor\n",
    "            baseline_tensor = torch.zeros_like(input_tensor)\n",
    "            # Get the attributions\n",
    "            attributions, delta = ig.attribute(input_tensor, baseline_tensor, target=0, return_convergence_delta=True)\n",
    "            \n",
    "            \n",
    "            # Get the attributions as numpy array\n",
    "            attributions = attributions.cpu().detach().numpy()\n",
    "            # Get the delta as numpy array\n",
    "            delta = delta.cpu().detach().numpy()\n",
    "\n",
    "            if not os.path.exists('RCA.csv'):\n",
    "                with open('RCA.csv', mode='w', newline='') as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    # create a dictionary with feature_names and write attributions sequentially\n",
    "                    attributions_dict = dict(zip(feature_names, attributions[i]))\n",
    "                    attributions_dict['predicted_target'] = anomaly_preds[i]\n",
    "                    writer.writerow(attributions_dict.keys())  # Write the column names (keys)\n",
    "\n",
    "            with open('RCA.csv', mode='a', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                # create a dictionary with feature_names and write attributions sequentially\n",
    "                attributions_dict = dict(zip(feature_names, attributions[i]))\n",
    "                attributions_dict['predicted_target'] = anomaly_preds[i]\n",
    "                writer.writerow(attributions_dict.values())\n",
    "\n",
    "    # Accumulate confusion matrices\n",
    "    if total_conf_matrix_actual is None:\n",
    "        total_conf_matrix_actual = conf_matrix_actual\n",
    "    else:\n",
    "        total_conf_matrix_actual += conf_matrix_actual\n",
    "\n",
    "    if total_conf_matrix_forecasted is None:\n",
    "        total_conf_matrix_forecasted = conf_matrix_forecasted\n",
    "    else:\n",
    "        total_conf_matrix_forecasted += conf_matrix_forecasted\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Compute Average Metrics**\n",
    "avg_accuracy_actual = np.mean(accuracy_scores_actual)\n",
    "avg_f1_actual = np.mean(f1_scores_actual)\n",
    "avg_accuracy_forecasted = np.mean(accuracy_scores_forecasted)\n",
    "avg_f1_forecasted = np.mean(f1_scores_forecasted)\n",
    "avg_rmse_pca = np.mean(RMSE_PCA)\n",
    "avg_conf_matrix_actual = total_conf_matrix_actual / n_splits\n",
    "avg_conf_matrix_forecasted = total_conf_matrix_forecasted / n_splits\n",
    "\n",
    "# Average classification report for actual features\n",
    "avg_report_actual = {}\n",
    "for key in classification_reports_actual[0].keys():\n",
    "    if key not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "        avg_report_actual[key] = {metric: np.mean([r[key][metric] for r in classification_reports_actual])\n",
    "                                    for metric in ['precision', 'recall', 'f1-score']}\n",
    "avg_report_actual['macro avg'] = {metric: np.mean([r['macro avg'][metric] for r in classification_reports_actual])\n",
    "                                    for metric in ['precision', 'recall', 'f1-score']}\n",
    "avg_report_actual['weighted avg'] = {metric: np.mean([r['weighted avg'][metric] for r in classification_reports_actual])\n",
    "                                    for metric in ['precision', 'recall', 'f1-score']}\n",
    "\n",
    "# Average classification report for forecasted features\n",
    "avg_report_forecasted = {}\n",
    "for key in classification_reports_forecasted[0].keys():\n",
    "    if key not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "        avg_report_forecasted[key] = {metric: np.mean([r[key][metric] for r in classification_reports_forecasted])\n",
    "                                        for metric in ['precision', 'recall', 'f1-score']}\n",
    "avg_report_forecasted['macro avg'] = {metric: np.mean([r['macro avg'][metric] for r in classification_reports_forecasted])\n",
    "                                        for metric in ['precision', 'recall', 'f1-score']}\n",
    "avg_report_forecasted['weighted avg'] = {metric: np.mean([r['weighted avg'][metric] for r in classification_reports_forecasted])\n",
    "                                        for metric in ['precision', 'recall', 'f1-score']}\n",
    "\n",
    "# **Display Results**\n",
    "print(\"\\n\\nAverage Metrics for Classification on Actual Future Features:\")\n",
    "print(f\"Average Accuracy: {avg_accuracy_actual}\")\n",
    "print(f\"Average F1-Score: {avg_f1_actual}\")\n",
    "print(\"Average Classification Report:\")\n",
    "for key, metrics in avg_report_actual.items():\n",
    "    if key not in ['macro avg', 'weighted avg']:\n",
    "        print(f\"Class {key}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"  {metric}: {value}\")\n",
    "    else:\n",
    "        print(f\"{key}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"  {metric}: {value}\")\n",
    "print(\"Average Confusion Matrix (Actual Features):\")\n",
    "print(avg_conf_matrix_actual)\n",
    "\n",
    "print(\"\\n\\nAverage Metrics for Classification on Forecasted Features:\")\n",
    "print(f\"Average Accuracy: {avg_accuracy_forecasted}\")\n",
    "print(f\"Average F1-Score: {avg_f1_forecasted}\")\n",
    "print(\"Average Classification Report:\")\n",
    "for key, metrics in avg_report_forecasted.items():\n",
    "    if key not in ['macro avg', 'weighted avg']:\n",
    "        print(f\"Class {key}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"  {metric}: {value}\")\n",
    "    else:\n",
    "        print(f\"{key}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"  {metric}: {value}\")\n",
    "print(f\"\\nAverage RMSE (PCA Inverse): {avg_rmse_pca}\")\n",
    "print(\"Average Confusion Matrix (Forecasted Features):\")\n",
    "print(avg_conf_matrix_forecasted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
